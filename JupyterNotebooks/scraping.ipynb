{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K. Cao 6/24/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import time \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser to get config credentials and other private information\n",
    "parser = configparser.ConfigParser()\n",
    "parser.read_file(open('../credentials.config'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium variables\n",
    "selenium_driver_path = parser.get('SELENIUM', 'path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_prompt(driver):\n",
    "    '''\n",
    "    Avoids modal pop up by clicking away from pop up.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        driver: Selenium driver object for scraping.\n",
    "    RETURNS:\n",
    "        None\n",
    "    '''\n",
    "    try:\n",
    "        elem = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"LoginModal\"]/div/div/div[2]/button')))\n",
    "        action = ActionChains(driver)\n",
    "        action.move_to_element(elem).move_by_offset(250, 0).click().perform()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(driver, test, element, property, by_type='class'):\n",
    "    '''\n",
    "    Finds page element by class or by xpath.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        driver: Selenium driver object for scraping.\n",
    "        test: If test, then print Exception, else set value as None\n",
    "        element: Page element.\n",
    "        property: What job property scraped.\n",
    "        by_type: By 'class' name or by 'xpath'.\n",
    "    '''\n",
    "    try:\n",
    "        if by_type == 'class':\n",
    "            return driver.find_element_by_class_name(element).text\n",
    "        elif by_type == 'xpath':\n",
    "            return driver.find_element_by_xpath(element).text\n",
    "    except NoSuchElementException:\n",
    "            if test:\n",
    "                print(f'NoSuchElementException for {property}. Defaulting to None')\n",
    "            else:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_values_dict(driver, test=False):\n",
    "    '''\n",
    "    Scrapes all data for each job post on a page.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        driver: Selenium driver object for scraping.\n",
    "        element: Page element.\n",
    "        test: If test, then print Exception, else set value as None\n",
    "\n",
    "    RETURNS:\n",
    "        Dictionary object of all job and company properties.\n",
    "    '''\n",
    "    try:\n",
    "        # Expand 'Show More' option for job description\n",
    "        driver.find_element_by_class_name('css-t3xrds').click()\n",
    "    except:\n",
    "        pass\n",
    "    job_info = {}\n",
    "    params = {\n",
    "        'CompanyName': ['css-87uc0g', 'Company Name', 'class'],\n",
    "        # 'CompanyName': ['//*[@class=\"css-8wag7x\"]', 'Company Name', 'xpath'],\n",
    "        'JobTitle': ['//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[2]', 'Job Name', 'xpath'],\n",
    "        'JobLocation': ['css-56kyx5', 'Job Location', 'class'],\n",
    "        # 'JobLocation': ['//*[@class=\"location mt-xxsm\"]', 'Job Location', 'xpath'],\n",
    "        'EasyApply': ['//*[@id=\"MainCol\"]/div[1]/ul/li[3]/div/div/a/div[1]/div[5]/div', 'Easy Apply', 'xpath'],\n",
    "        'JobDescription': ['.//div[@class=\"jobDescriptionContent desc\"]', 'Job Description', 'xpath'],\n",
    "        'JobSalary': ['css-1xe2xww', 'Salary', 'class'],\n",
    "        # 'JobSalary': ['//*[@class=\"salary-estimate\"]', 'Salary', 'xpath'],\n",
    "        'CompanyRating': ['css-1m5m32b', 'Company Rating', 'class'],\n",
    "        # 'CompanyRating': ['//*[@class=\"job-search-rnnx2x\"]', 'Company Rating', 'xpath'],\n",
    "        'CompanySize': ['//div[@id=\"EmpBasicInfo\"]/div[1]/div/div[1]/span[2]', 'Company Size', 'xpath'],\n",
    "        'CompanyType': ['//div[@id=\"EmpBasicInfo\"]/div[1]/div/div[3]/span[2]', 'Company Type', 'xpath'],\n",
    "        'CompanySector': ['//div[@id=\"EmpBasicInfo\"]/div[1]/div/div[5]/span[2]', 'Company Sector', 'xpath'],\n",
    "        'CompanyYearFounded': ['//div[@id=\"EmpBasicInfo\"]/div[1]/div/div[2]/span[2]', 'Year Founded', 'xpath'],\n",
    "        'CompanyIndustry': ['//div[@id=\"EmpBasicInfo\"]/div[1]/div/div[4]/span[2]', 'Company Industry', 'xpath'],\n",
    "        'CompanyRevenue': ['//div[@id=\"EmpBasicInfo\"]/div[1]/div/div[6]/span[2]', 'Company Revenue', 'xpath']\n",
    "    }\n",
    "\n",
    "    for param in params.items():\n",
    "        val = get_value(driver, test, param[1][0], param[1][1], param[1][2])\n",
    "        job_info[param[0]] = job_info.get(param[0], val)\n",
    "        \n",
    "    return job_info\n",
    "\n",
    "\n",
    "# //*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_count(driver, test=False):\n",
    "    '''\n",
    "    Uses pagination footer to determine the number of pages of job postings available.\n",
    "    \n",
    "    ARGUMENTS:  \n",
    "        driver: Selenium driver object for scraping.\n",
    "        test: If test, then print Exception, else set value as None\n",
    "        \n",
    "    RETURNS:\n",
    "        None\n",
    "    '''\n",
    "    try:\n",
    "        # Return the last page number. 'Page # of ##'\n",
    "        return get_value(driver, test, 'paginationFooter', 'class').split(' ')[-1]\n",
    "    except:\n",
    "        print('NoSuchElementException')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_data(test: bool, url: str = 'https://www.glassdoor.com/Job/united-states-data-engineer-jobs-SRCH_IL.0,13_IN1_KO14,27.htm?fromAge=1'):\n",
    "    '''\n",
    "    Collects data on all jobs given a starting URL for Glassdoor. Assumes iteration is from first page of URL on towards the last page.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        test: Boolean. If test, print statements for debugging. Else catch exceptions only.\n",
    "        url: Glassdoor URL to scrape data from. Must be of type string. Defaults to 'Data Engineer' jobs posted in the last 24 hours.\n",
    "\n",
    "    RETURNS:\n",
    "        all_data: All data on jobs from given Glassdoor URL\n",
    "    '''\n",
    "    # Set selenium driver path\n",
    "    all_jobs = []\n",
    "    with webdriver.Edge('../SeleniumDrivers/msedgedriver.exe') as driver:\n",
    "        # Open Glasdoor page\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        # Wait for page to load\n",
    "        time.sleep(2)\n",
    "        for p in range(int(page_count(driver, True))):\n",
    "            print(f'Page Number: {p + 1}')\n",
    "            # Get all jobs on page\n",
    "            job_posts = driver.find_elements_by_class_name('react-job-listing')\n",
    "            # Iterate through job posts\n",
    "            for i, job in enumerate(job_posts):\n",
    "                print(f'Job Number: {i + 1}')\n",
    "                test = False\n",
    "                time.sleep(1)\n",
    "                job.click()\n",
    "                exit_prompt(driver)\n",
    "                try:\n",
    "                    job_info = get_all_values_dict(driver, True)\n",
    "                    all_jobs.append(job_info)\n",
    "                    if test:  # If test, print outputs \n",
    "                        print(json.dumps(job_info))\n",
    "                        print()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    time.sleep(4)\n",
    "                if job == job_posts[-1]:\n",
    "                    print('Last Job')\n",
    "            page_element = f'//*[@id=\"MainCol\"]/div[2]/div/div[1]/button[{p + 3}]'\n",
    "            try:\n",
    "                driver.find_element_by_xpath(page_element).click()\n",
    "                exit_prompt(driver)\n",
    "                time.sleep(2)\n",
    "            except NoSuchElementException:\n",
    "                print('No Page Element Found')\n",
    "                break\n",
    "    return all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Number: 1\n",
      "Job Number: 1\n",
      "Job Number: 2\n",
      "Job Number: 3\n",
      "Job Number: 4\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 5\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 6\n",
      "NoSuchElementException for Company Rating. Defaulting to None\n",
      "NoSuchElementException for Company Size. Defaulting to None\n",
      "NoSuchElementException for Company Type. Defaulting to None\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Year Founded. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 7\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 8\n",
      "Job Number: 9\n",
      "Job Number: 10\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 11\n",
      "Job Number: 12\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "NoSuchElementException for Company Rating. Defaulting to None\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 13\n",
      "Job Number: 14\n",
      "Job Number: 15\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 16\n",
      "Job Number: 17\n",
      "Job Number: 18\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 19\n",
      "Job Number: 20\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 21\n",
      "Job Number: 22\n",
      "NoSuchElementException for Company Rating. Defaulting to None\n",
      "Job Number: 23\n",
      "Job Number: 24\n",
      "Job Number: 25\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 26\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 27\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 28\n",
      "Job Number: 29\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 30\n",
      "Last Job\n",
      "Page Number: 2\n",
      "Job Number: 1\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 2\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 3\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 4\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 5\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 6\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 7\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 8\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 9\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 10\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 11\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 12\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 13\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 14\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 15\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 16\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 17\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 18\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 19\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 20\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 21\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 22\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 23\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 24\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 25\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 26\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 27\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 28\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 29\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 30\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "NoSuchElementException for Company Rating. Defaulting to None\n",
      "NoSuchElementException for Company Size. Defaulting to None\n",
      "NoSuchElementException for Company Type. Defaulting to None\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Year Founded. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Last Job\n",
      "Page Number: 3\n",
      "Job Number: 1\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 2\n",
      "Job Number: 3\n",
      "Job Number: 4\n",
      "NoSuchElementException for Company Sector. Defaulting to None\n",
      "NoSuchElementException for Company Industry. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 5\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 6\n",
      "Job Number: 7\n",
      "Job Number: 8\n",
      "Job Number: 9\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 10\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 11\n",
      "Job Number: 12\n",
      "Job Number: 13\n",
      "Job Number: 14\n",
      "Job Number: 15\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 16\n",
      "Job Number: 17\n",
      "Job Number: 18\n",
      "Job Number: 19\n",
      "Job Number: 20\n",
      "Job Number: 21\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 22\n",
      "Job Number: 23\n",
      "Job Number: 24\n",
      "Job Number: 25\n",
      "Job Number: 26\n",
      "Job Number: 27\n",
      "Job Number: 28\n",
      "Job Number: 29\n",
      "Job Number: 30\n",
      "Last Job\n",
      "Page Number: 4\n",
      "Job Number: 1\n",
      "Job Number: 2\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 3\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 4\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 5\n",
      "Job Number: 6\n",
      "Job Number: 7\n",
      "Job Number: 8\n",
      "Job Number: 9\n",
      "Job Number: 10\n",
      "Job Number: 11\n",
      "Job Number: 12\n",
      "Job Number: 13\n",
      "Job Number: 14\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 15\n",
      "Job Number: 16\n",
      "Job Number: 17\n",
      "Job Number: 18\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 19\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 20\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 21\n",
      "Job Number: 22\n",
      "Job Number: 23\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 24\n",
      "Job Number: 25\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 26\n",
      "Job Number: 27\n",
      "Job Number: 28\n",
      "Job Number: 29\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 30\n",
      "Last Job\n",
      "Page Number: 5\n",
      "Job Number: 1\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Company Revenue. Defaulting to None\n",
      "Job Number: 2\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 3\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 4\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 5\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 6\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 7\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "Job Number: 8\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Job Number: 9\n",
      "NoSuchElementException for Easy Apply. Defaulting to None\n",
      "NoSuchElementException for Salary. Defaulting to None\n",
      "Last Job\n"
     ]
    }
   ],
   "source": [
    "# all_data = collect_all_data(test=False, url='https://www.glassdoor.com/Job/seattle-data-engineer-jobs-SRCH_IL.0,7_IC1150505_KO8,21.htm?fromAge=30')\n",
    "all_data = collect_all_data(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CompanyName': 'Booz Allen Hamilton\\n4.2',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Arlington, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'The Opportunity:\\nEver-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions—from fraud detection to cancer research to national intelligence.\\nAs a big data engineer at Booz Allen, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful.\\nHere, you’ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.\\nWork with us to use data for good.\\nJoin us. The world can’t wait.\\nYou Have:\\n10+ years of experience with application development in a professional work environment\\n5+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale\\n3+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data\\n3+ years of experience with building scalable ETL and ELT workflows for reporting and analytics\\nExperience creating solutions within a collaborative, cross-functional team environment\\nAbility to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms\\nBachelor’s degree\\nTop Secret clearance\\nNice If You Have:\\nExperience with Postres\\nExperience with Data modeling\\nExperience with data migration\\nExperience with Python, SQL, Scala, or Java\\nExperience with UNIX and Linux, including basic commands and Shell scripting\\nExperience with a public Cloud, including AWS, Microsoft Azure, or Google Cloud\\nExperience working on real-time data and streaming applications\\nExperience with data warehousing using AWS Redshift, MySQL, or Snowflake\\nExperience with Agile engineering practices\\nClearance:\\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Top Secret clearance is required.\\nCreate Your Career:\\nGrow With Us\\nYour growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.\\nA Place Where You Belong\\nDiverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.\\nSupport Your Well-Being\\nOur comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.\\nYour Candidate Journey\\nAt Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.\\nCompensation\\nAt Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.\\nSalary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,100.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.\\nWork Model\\nOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.\\nIf this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.\\nIf this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.\\nEEO Commitment\\nWe’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.\\n#LI-AH1, DRE1, ID13-C',\n",
       "  'JobSalary': 'Employer Provided Salary:$73K - $166K',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Management & Consulting',\n",
       "  'CompanyYearFounded': '1914',\n",
       "  'CompanyIndustry': 'Business Consulting',\n",
       "  'CompanyRevenue': '$5 to $10 billion (USD)'},\n",
       " {'CompanyName': 'FM Global\\n3.7',\n",
       "  'JobTitle': 'Data Engineer III-Data infrastructure',\n",
       "  'JobLocation': 'Johnston, RI',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"More information about this job:\\nOverview:\\n\\nFM Global is a leading property insurer of the world's largest businesses, providing more than one-third of FORTUNE 1000-size companies with engineering-based risk management and property insurance solutions. FM Global helps clients maintain continuity in their business operations by drawing upon state-of-the-art loss-prevention engineering and research; risk management skills and support services; tailored risk transfer capabilities; and superior financial strength. To do so, we rely on a dynamic, culturally diverse group of employees, working in more than 100 countries, in a variety of challenging roles.\\n\\nResponsibilities:\\n\\nThe Data Engineer III is responsible for data ingestion, integration, and storage. This role is also responsible for creating and managing data infrastructure, data pipeline design, implementation, and data verification.\\n\\nAlong with the team, the Data Engineer III is responsible for ensuring the highest standards of data quality, security and compliance. Additionally, the Data Engineer III will implement methods to improve data reliability and quality, combine raw information from different sources to create consistent data sets.\\n\\nThis team is responsible for ingestion of data from source systems, integration of data across business entities (etl/elt), delivery of data via views for use by Analytics Reporting Tech, Business Intelligence and Advanced Analytics (Data Scientists). Backend data storage and prep for teams who are mining the data for predictive and prescriptive analytics and operational reporting.\\n\\nThe Data Engineer III is the third level position in the Data Engineer job family. Those holding this position are typically assigned to work on integrated project teams for medium to large projects and be the lead for small to medium projects. The Data Engineer III must also be able to work independently.\\n\\nData Acquisition\\nPossess and continually grow knowledge of structured and unstructured data sources within each product journey (Underwriting and Risk; Client Service, Sales and Marketing; Claims; Account and Location Engineering) as well as emerging data sources (purchased data sets; external data; etc.).\\nPartner with Product Owners, developers, Solution Architects, Enterprise Architects, Business Analysts, Data Engineers, Data Analysts, Data Scientists, and others to understand data and reporting needs.\\nDevelop solutions using data modeling techniques and using technologies such as Azure Synapse Analytics, SQL Server database; SSIS; C#, Power BI Premium, SQL Server & Analysis Services, and others as required.\\nValidate solutions are accurate through detailed and disciplined testing methodologies.\\nEnsure tables and views are designed for data integrity, efficiency and performance, and are easy to comprehend.\\n\\nMove and Store Data\\nDesign and maintenance of data flow, infrastructure pipelines, ETL/ELT, structured and unstructured data movement and storage solutions\\nDesign data models and data flows into and out of Data Analytics databases\\nUnderstand and design data relationships between business and data subject areas\\nFollow standards for naming conventions, code documentation and code review.\\n\\nSupport Data Exploration and Transformation Needs\\nConduct data cleansing and support other team members with data cleansing tasks if needed\\nConduct data profiling to identify data anomalies and resolve issues\\nComplete data preparation tasks\\nIdentify, design, and implement internal process improvements such as but not limited to automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability, etc.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure and SQL.\\n\\nSupport users and production jobs\\nManage and address operational data issues by establishing workarounds and/or bringing in multi-functional teams to solve the issues in a timely manner\\nSupport developers, data analysts and data scientists who need to work with data in the appropriate data base(s)\\nAnalyze and assess reported data quality issues, conduct root cause analysis\\nConsult dba(s) and other technology infrastructure team members on configuration and maintenance of platform\\nMonitor system performance and look for opportunities for optimization\\nMonitor data storage capacity\\nAddress production issues quickly, with appropriate validation and deployment steps\\nProvide concise and professional communication to users, management and teammates\\n\\nParticipate in effective execution of team priorities\\nAble to solve complex problems with on-time delivery\\nIdentify work tasks and capture them in the team backlog\\nOrganize known tasks, prioritize work as neededAble to resolve colliding priorities and advances as needed\\nProvides production support\\nNetwork with product teams to keep abreast of database changes as well as business process changes which result in data interpretation changes\\nFamiliarity with 3rd Normal Form and Dimensional (Inmon/Kimball) database theory\\n\\nQualifications:\\n\\nPrefer 4 year/Bachelor’s Degree or Master's Degree in Computer Science, Information Technology, Computer Engineering, or equivalent experience\\n5+ years relatable work experience\\nETL/ELT design required.\\nSQL database and massively parallel processing (mpp) platforms required\\nExperience designing/building/maintaining data warehouse database(s) for analytical use\\nSkilled with database clustering\\nDAX coding language helpful but not required.\\nPower BI Premium helpful, SSAS considered.\\nFamiliarity with Azure cloud applications\\n\\nThe hiring range for this position dependent on final grade and title is $97,500 to $140,100 USD. The final salary offer will vary based on geographic location, individual education, skills, and experience. The position is eligible to participate in FM Global’s comprehensive Total Rewards program that includes an incentive plan, generous health and well-being programs, a 401(k) and pension plan, career development opportunities, tuition reimbursement, flexible work, time off allowances and much more.\\n\\nFM Global is an Equal Opportunity Employer and is committed to attracting, developing, and retaining a diverse workforce.\\n\\n#LI-TA1\",\n",
       "  'JobSalary': '$83K - $111K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Insurance',\n",
       "  'CompanyYearFounded': '1835',\n",
       "  'CompanyIndustry': 'Insurance Carriers',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Iron Service Global Inc\\n3.3',\n",
       "  'JobTitle': 'Sr. Azure Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Role: Sr. Azure Data Engineer\\nJob Location: Remote\\nType: Long Term Contract\\nJob Summary:\\n10+ years of experience\\nRequired Skills:\\nAzure Databricks, Azure Data Factory, Apache Spark, pySpark, Scala, SparkSQL, Hive, Azure fundamental knowledge, GitHub, Maven\\nHigh-level tasks:\\nAnalyze the existing code and discover the use case, sources and targets (databases/tables). The existing code could be in Spark/Hive QL/Scala/Python/Shell Script/SSIS/SAS/SQL Stored Procedure/etc.\\nDevelop the Ingestion pipeline using Azure Data Factory.\\nDevelop data transformation code using pySpark in Databricks notebooks.\\nOrchestrate the Ingestion and transformation pipeline using Azure data factory.\\nCheck in/Check out the ADF/notebooks/python/any code using Github.\\nDeploy and test Azure Data Factory and Databricks code.\\nProd deployment support.\\nDocument the prod hand oversteps.\\nJob Types: Full-time, Contract\\nSalary: $60.00 - $65.00 per hour\\nSchedule:\\n8 hour shift\\nDay shift\\nMonday to Friday\\nExperience:\\nAzure: 4 years (Preferred)\\nSQL: 10 years (Preferred)\\nAdobe Spark: 3 years (Preferred)\\nHive: 3 years (Preferred)\\nData Engineering: 10 years (Preferred)\\nWork Location: Remote',\n",
       "  'JobSalary': 'Employer Provided Salary:$60.00 - $65.00 Per Hour',\n",
       "  'CompanyRating': '3.3',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1987',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Sourcemantra\\n3.7',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Job Title: Data Engineer\\nLocation: 100 % Remote\\nPosition: Contract.\\nGeneric Job Description\\n8+ years of hands-on data modeling and data engineering experience\\nMust have hands on experience with PySpark, Python, ETL, Terradata, AWS\\nExperience in design and development of custom ETL pipelines using SQL and scripting languages\\nProficiency in advanced SQL, performance tuning\\nExperienced in analytic and problem-solving skills.\\nAbility to perform detailed analysis of business problems and technical environments.\\nStrong oral and written communication skills.\\nPositive relationship and collaborating skills.\\nIf Abinitio is there great if not equivalent is good.\\nJob Type: Full-time\\nSalary: $70.00 per hour\\nSchedule:\\n8 hour shift\\nWork Location: Remote',\n",
       "  'JobSalary': 'Employer Provided Salary:$70.00 Per Hour',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Accounting & Tax',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': 'Financial Services',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Solugen Inc\\n4.6',\n",
       "  'JobTitle': 'Data Engineer I',\n",
       "  'JobLocation': 'Houston, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Solugen\\nHouston, TX 77035\\n\\nPosition\\nData Engineer I\\n\\nMission\\nSolugen’s mission is to decarbonize chemical manufacturing. As a first step, Solugen is building a 10,000 tpy first commercial plant with the potential to offset up to 37,000 tpy of carbon dioxide equivalents. This role directly supports Solugen’s mission by ensuring R&D assets are being fully utilized and data is being efficiently visualized and processed.\\n\\nThis role will support utilization of the R&D assets by maintaining Solugen’s LIMS system and automation suites. While also serving as a system administrator, this role will monitor the usage and upkeep of the platform. This role will serve as an SME for routine troubleshooting and upkeep of the Benchling LIMS system by updating templates, building dashboards, and maintaining data parity under the guidance of the Lead Automation Engineer.\\n\\nThis role will bring about a new level of visualization through the process of creating dashboards in the platform that can surface and plot data, facilitating advanced analysis and machine learning algorithms.\\n\\n-\\nReporting directly to the Lead Data Engineer/Director of Data Architecture, this role will be accountable to maintain sample parity, ease of use for scientists, and structure data in our data warehouse to facilitate new research paths and deliver on department OKRs.\\n\\nThe position requires technical knowledge in lab data management and processing (LIMS/ELN), understanding of enterprise software solutions for R&D, routine science and engineering processes as well as interpersonal skills to manage complex solutions to evolving technologies and science.\\n\\nThe ideal person for this role has hands-on experience with both daily lab work and digital infrastructure that supports timely documentation and execution. This role has development opportunities within IT and R&D project management.\\n\\nKey Responsibilities\\nFacilitate adoption by maintaining and developing templates, data structures, and software implementations for R&D review technologies.\\nOptimize lab automation suites to ensure end-to-end data parity.\\nUtilize existing and emergent software solutions to reduce scientist overhead and support data engineering operations.\\nDocument trainings and use and operations specifications for the LIMS/ELN platform, automation systems, and cross-functional data connectivity as R&D scales and as business needs demand.\\nMaintain and update documentation monthly to capture the links being established in the platform to maintain best practices.\\nOn a routine basis perform IQ, OQ, and PQ activities to maintain a stable environment.\\n\\nKey Competencies\\nJob Specific\\nAttention to detail and organization.\\nStrong interpersonal ability to help build trust, and drive adoption of new software across multiple departments and functions.\\nExperience with bench-side scientific software and process solutions.\\nExpert on Benchling LIMS/ELN software or comparable background.\\nBroad understanding of life sciences, and design of experiments with a scientific focus.\\nExperience with programming languages like Python and SQL.\\nExcellent analytical, quantitative and qualitative problem-solving skills.\\nComfortable balancing multiple projects at one time.\\nExcellent written and verbal communication skills.\\n\\nCompany Specific\\nAsk a lot of questions, be curious, and have insights into your work.\\nBe nimble, embrace the speed and ambiguity of startup culture.\\nA passion for green chemistry and fighting climate change.\\nAbility to self-manage and set your own priorities and schedule.\\nBe comfortable working in multiple roles.\\nEmphasize safety above all else.\\nRespect for others.\\nCreative problem solving.\\nWillingness to learn and research new engineering/science concepts.\\n\\nQualifications\\nBS in Bioengineering, biology, chemistry, computer engineering, or related field.\\n2+ years’ experience in life sciences LIMS development and wet lab experience in Life science automation and high throughput screening and/or equivalent chemical engineering background.\\nExperience with programming languages like python and SQL. Automation API development experience a plus.\\n\\nSolugen is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, gender, sexual orientation, national origin, disability status, protected veteran status or any other legally protected status.',\n",
       "  'JobSalary': '$75K - $104K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.6',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Biotech & Pharmaceuticals',\n",
       "  'CompanySector': '$25 to $100 million (USD)',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Pharmaceutical & Biotechnology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'e business internation',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Greensboro, NC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Implement, operate, and support data pipelines within Azure Data Factory.\\n· Ability to move data from source to destination on a variety of platforms including ADP, NetSuite, SQL Databases, Azure Data lakes, Azure Synapse Datawarehouse, and more\\n· Optimize and maintain existing data pipelines\\n· Develop new data pipelines from different sources\\n· Validate data across systems to verify completeness and accuracy of data movement\\n· Develop and maintain code to pull and push data via API calls\\nTechnical Skill Set\\n· Azure Data Factory\\n· SQL Server Development including Advance SQL (TSQL and PL/SQL)\\n· Azure API Management Platform\\n· C#/.Net\\n· JSON\\n· Web APIs (REST)\\n· Web Service Development (SOAP)\\n· GitHub Repository\\nEducational Requirements\\n· High school diploma or equivalent (Required)\\n· Associate or Bachelor's degree from an accredited institution in a related field (Preferred)\\nJob Type: Full-time\\nSalary: $75,181.34 - $100,840.44 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nLife insurance\\nPaid time off\\nVision insurance\\nCompensation package:\\nBonus pay\\nExperience level:\\n3 years\\nSchedule:\\nMonday to Friday\\nApplication Question(s):\\nvisa type?\\ndo yo have experience in Azure Data Factory\\n· SQL Server Development including Advance SQL (TSQL and PL/SQL)\\ndo you have Technical Skill in C#/.Net\\n· JSON\\n· Web APIs (REST)\\n· Web Service Development (SOAP)\\nWork Location: On the road\",\n",
       "  'JobSalary': 'Employer Provided Salary:$75K - $101K',\n",
       "  'CompanyRating': None,\n",
       "  'CompanySize': None,\n",
       "  'CompanyType': None,\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': None,\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Reveille Technologies\\n3.3',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Position: Data Engineer\\nLocation: Dallas, TX - Hybrid\\nExperience:4 to 7 (We are looking Mid Level Candidate)\\nTax Term:W2\\nMust Haves:\\n3-5 years of exp\\nInformatica (either powercenter or cloud is okay)\\nSQL\\nTableau\\nJob Type: Contract\\nSalary: $75,853.60 - $165,235.45 per year\\nExperience level:\\n4 years\\n5 years\\n6 years\\nExperience:\\nInformatica: 3 years (Preferred)\\nSQL: 3 years (Preferred)\\nTableau: 3 years (Preferred)\\nWork Location: On the road',\n",
       "  'JobSalary': 'Employer Provided Salary:$76K - $165K',\n",
       "  'CompanyRating': '3.3',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Information Technology Support Services',\n",
       "  'CompanySector': '$5 to $25 million (USD)',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'E-Business International INC\\n3.6',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Greensboro, NC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Stake Center Locating has an opportunity in Greensboro, NC for a Data Engineer/ETL Developer to join the Data team. The role is primarily focused on moving data across platforms via ETL using Azure Data Factory and APIs. This is a mid-level position, and training will be provided (both through On-The-Job training, formal and informal mentoring, and formal vendor training with opportunities to gain industry-recognized certifications). This position is an in-office position located in our Greensboro, NC headquarters.\\nResponsibilities\\n· Implement, operate, and support data pipelines within Azure Data Factory.\\n· Ability to move data from source to destination on a variety of platforms including ADP, NetSuite, SQL Databases, Azure Data lakes, Azure Synapse Datawarehouse, and more\\n· Optimize and maintain existing data pipelines\\n· Develop new data pipelines from different sources\\n· Validate data across systems to verify completeness and accuracy of data movement\\n· Develop and maintain code to pull and push data via API calls\\nTechnical Skill Set\\n· Azure Data Factory\\n· SQL Server Development including Advance SQL (TSQL and PL/SQL)\\n· Azure API Management Platform\\n· C#/.Net\\n· JSON\\n· Web APIs (REST)\\n· Web Service Development (SOAP)\\n· GitHub Repository\\nEducational Requirements\\n· High school diploma or equivalent (Required)\\n· Associate or Bachelor's degree from an accredited institution in a related field (Preferred)\\nRequirements:\\nQualifications\\n· Ability to implement solutions to issues with complex requirements\\n· Ability to expand skills and knowledge beyond current levels\\n· Ability to organize and execute a daily work plan\\nJob Type: Full-time\\nSalary: Up to $100,000.00 per year\\nBenefits:\\nFlexible schedule\\nHealth insurance\\nLife insurance\\nPaid time off\\nCompensation package:\\nYearly pay\\nExperience level:\\n4 years\\nSchedule:\\n8 hour shift\\nApplication Question(s):\\nAre you an Citizen Or Green Card holder? (only looking for Citizen and GC )\\nExperience:\\nJSON: 1 year (Required)\\nSQL: 1 year (Required)\\nSOAP: 1 year (Required)\\nWork Location: On the road\",\n",
       "  'JobSalary': 'Employer Provided Salary:$100K',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1992',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Brillio\\n3.7',\n",
       "  'JobTitle': 'AWS Data Engineer – R01525162',\n",
       "  'JobLocation': 'Austin, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'AWS Data Engineer - R01525162\\nAbout Brillio:\\nBrillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022\\n\\nConsultant\\nPrimary Skills\\n\\nSpecialization\\n\\nJob requirements\\nJob Description: We are looking for an experienced AWS Data Engineer to join our team and help us build and maintain our data infrastructure on AWS. As an AWS Data Engineer, you will work closely with data Architect, Business and Data analysts, BI Developer, Customer Business and engineering team, and other stakeholders to design, implement, and manage data pipelines and systems that support our business needs.\\n\\nKey Job Responsibilities:\\nDesign, implement, and maintain data pipelines and data processing systems, ETL Infrastructure on AWS using technologies like Apache Spark, AWS Glue, AWS Lambda, and AWS S3 and strong Python and Pandas hands on experience.\\nCollaborate with data Architect and analysts/BI Developer to understand their data requirements and design data models that meet their needs.\\nWork with DevOps engineers to ensure that data pipelines are reliable, scalable, and secure.\\nMonitor and troubleshoot data pipelines to ensure that data is flowing correctly and on time.\\nDevelop automation scripts to streamline deployment, testing, and maintenance of data pipelines and systems.\\nDocument technical designs, standard operating procedures, and best practices for data engineering on AWS.\\nKeep up to date with emerging technologies and best practices in data engineering and recommend new tools and technologies as appropriate.\\nExcellent communication and collaboration skills.\\nNeed strong commitment to project, Problem solving, Team Player\\n\\nPreferred qualifications:\\nBachelor’s or master’s degree in computer science, Information Technology, or a related field.\\n3+ years of experience in data engineering with a focus on AWS technologies.\\nStrong knowledge of AWS services such as S3, EC2, Glue, Lambda, and Athena.\\nExperience designing and building data pipelines using Apache Spark.\\nExperience working with SQL and NoSQL databases.\\nFamiliarity with ETL processes and tools such as Talend or Informatica.\\nStrong scripting skills in Python or Java.\\n#LI-CH1\\nKnow what it’s like to work and grow at Brillio: Click here',\n",
       "  'JobSalary': '$79K - $110K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2014',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'REPS & Co.\\n4.6',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Founded in 2017, REPS & Co. is a leader in the entertainment industry specializing in ticketing for live events. We provide tickets to many events and shows across the nation including music, sports and theatrical performances.\\nWe pride ourselves in offering the best experience for the best price to our customers. Our technology is what allows us to outperform our competitors and deliver an unforgettable experience to fans.\\nWe are looking for a TALENTED and heavily data focused Data Engineer with 5+ years of professional experience.\\nThe Data engineer is responsible for dealing massive amounts of data, optimization, expansion of our data with great experience on data pipeline architecture and transformation.\\nIf you feel you can manage data in all its forms possible, we are excited to invite you to our growing team!\\nResponsibilities:\\nWork on data architecture and use a systematic approach to plan, create, and maintain data.\\nCollect and obtain data from appropriate sources after formulating a set of dataset processes.\\nCreate models and identify patterns.\\nData automation.\\nImprove data reliability, efficiency and quality\\nUse of large data sets\\nPrepare data for predictive and prescriptive modeling\\nBuild the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources.\\nBuild analytics tools that utilize data pipelines to provide actionable insights into operational efficiency and other key business performance metrics.\\nWork with stakeholders including the executive, product, data and design teams to assist with data-related technical issues and support their data infrastructure needs.\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product.\\nWork with data and analytics experts to strive for greater functionality.\\nQualifications:\\nMassive experience with SQL\\nAdvanced experience working with relational databases, query authoring (SQL).\\nFamiliarity with a variety of databases.\\nDatabase design\\nExperienced building and optimizing “big data” data pipelines, architecture and data sets.\\nExperience performing root cause analysis on internal and external data.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nExcellent experience of manipulating, processing and extracting value from large disconnected datasets\\nOn hands experience and knowledge of message queuing, stream processing and highly scalable “big data” data stores\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nData Scripting experience.\\nSQS\\nProfessional experience using the following software/tools:\\nMassive experience with relational SQL databases\\nRDS\\nPostgres\\nMySQL\\nCassandra\\nStrong experience in data pipeline and workflow tools:\\nDBT\\nLambda\\nAirbyte\\nAirflow etc.\\nExperience with BigData cloud services:\\nAmazon Web Services (S3 data oriented, EC2)\\nBigQuery\\nEMR\\nRDS\\nRedshift\\nGlue\\nAthena\\nExperience with stream-processing systems:\\nDruid\\nStorm\\nSpark-Streaming, etc.\\nExperience with data object-oriented languages:\\nPython\\nJava\\nJavascript etc\\nFeel free to apply using our internal application link: https://reps2.hire.trakstar.com/jobs/fk0xjly?source=Indeed\\nEither way, we are excited to learn more about you!\\nBenefits\\nMedical, Dental and Vision insurance\\nPTO\\nSick Leave\\nPaid Holidays\\nVolunteer Time Off\\n401k with Match\\nESOP\\nParental Leave\\n100% Employer Paid Life Insurance & Long Term Disability\\nEAP Program\\nBonus\\nGym Membership Reimbursement\\nTicket Benefit\\nJob Type: Full-time\\nExperience level:\\n5 years\\nApplication Question(s):\\nDo you need Visa Sponsorship to work in the US?\\nWork Location: Remote',\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.6',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2017',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'BMS CAT\\n2.9',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Haltom City, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Founded in 1948, Blackmon Mooring / BMS CAT is a global leader in disaster restoration. From initial clean-up to complete rebuilds, we are there every step of the way. We are a team built on high energy, high performing individuals who work together to maintain Blackmon Mooring / BMSCAT as an industry leader.\\n\\nPosition Summary:\\nThe Data Engineer is responsible for developing and maintaining data processing software like databases. Their duties include coordinating with company Executives and other professionals to create unique data infrastructure, running tests on their designs to isolate errors and updating systems to accommodate changes in company needs. The Data Engineer will be working within the IT team and will report directly to the Head of IT.\\n\\nPrimary Responsibilities:\\nIn addition to creating and maintaining an optimal pipeline architecture, typical duties and responsibilities for the Data Engineer position include:\\nAssembling large, complex sets of data that meet non-functional and functional business requirements\\nBuild and optimize data sets, 'big data' data pipelines and architectures\\nIdentifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes\\nBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition\\nPerform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions\\nMicrosoft Certified Solutions Expert (MCSE), include a wide range of topics with specific sub-certifications including MCSE: Data Management and Analytics.\\n\\nQualifications:\\nFive years of industry experience\\nA degree in information technology or computer science\\nAnalytical Skills\\nStrong Communication skills\\nAbility to multitask\\nProject management skills\\n\\nCompensation:\\nAs the Data Engineer, you will receive a base salary and will participate in the Corporate Discretionary quarterly bonus plan. Your compensation will vary by location and experience level. This can be discussed during the first interview. We offer fully paid employee medical insurance, optional dental and optical insurance, 401 (k) plan and more.\\n\\nIf this sounds like you, please apply TODAY!\",\n",
       "  'JobSalary': '$72K - $105K (Glassdoor est.)',\n",
       "  'CompanyRating': '2.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Construction, Repair & Maintenance Services',\n",
       "  'CompanyYearFounded': '1948',\n",
       "  'CompanyIndustry': 'General Repair & Maintenance',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Lighthouse MTG LLC',\n",
       "  'JobTitle': 'Azure Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Description:\\nSpyglass MTG (Microsoft Technology Group) is a Microsoft Gold Certified Partner. We hire people who are professional consultants in addition to being highly competent performers in their specific discipline. As a Consultant at Spyglass MTG you will be working on projects to develop Microsoft technology focused solutions for a variety of clients in industries such as Financial Services, Healthcare, Life Sciences, Manufacturing and Higher Education. Our office is located in Lincoln, RI, however our clients are typically located in the Greater Boston and New England area. You will be working in a team environment that consists of Spyglass and Client members.\\nAs an Azure Data Engineer, you will join our growing Azure Data and Analytics practice. We are looking for a Data Engineer to collect, aggregate, store, and reconcile data to Azure in support of Client business decisions. You will help design and build data pipelines, data streams, and data service APIs on Azure which drive data driven end-user information portals and insight tools. You will be a critical part of the data supply chain, ensuring that our clients can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics at terabyte size data scale. You will analyze the business, functional, and technical issues in order to successfully implement innovative Analytics solutions on Azure. You will be integral to the design, development, and delivery of all modern warehouse solutions on Azure for our clients.\\nCandidate Expectations:\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nExperience with data pipelining and data engineering.\\nExperience with one or more languages: C#, Python, SQL, R, Scala.\\nExperience with code deployment concepts.\\nExperience working with data integration techniques & self-service data preparation.\\nAssist business users on functional and data requirements to enhance data models and pipelines.\\nExperience in requirements analysis, design and prototyping.\\nExperience deploying modern data solutions leveraging components like Azure functions, Azure Data Factory, Data Flows, Azure Data Lake, Azure SQL, Azure Synapse, Streaming Analytics or equivalent on another cloud provider such as AWS or Google Cloud.\\nFamiliarity with DevOps tools like Azure DevOps, Jenkins, Maven etc.\\nBasic Qualifications:\\n5-8 years total experience with Data Engineering\\n1 year of experience with Azure SQL, Azure Synapse or other Cloud Data Services.\\n1 year developing, deploying, and testing data pipelines\\n1 year experience developing data pipelines within a low-code no-code integration tool\\n1 year of data engineering with Hadoop, Spark, Databricks, SSIS, Data Factory, Synapse, or other data integration tool\\n1 year of working experience with SQL Server, data warehousing or other data analytics project\\n2 years programming with SQL, Python, Scala OR 1 year plus education experience\\nBenefits:\\nMedical, Vision and Dental Plans\\nLife and Disability Insurance\\nOpen PTO Policy\\nHoliday PTO\\nPaid training certification\\nBonus plan\\n401k\\nFlexible working arrangements\\n& more\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, transgender status, national origin, citizenship, age, disability or protected veteran status.',\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': None,\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Optimal Inc.\\n3.6',\n",
       "  'JobTitle': 'Data Engineer-Terraform/GCP',\n",
       "  'JobLocation': 'Dearborn, MI',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Position Description:\\nSenior Data Engineers will be responsible for designing the transformation and modernization of big data solutions on GCP cloud integrating native GCP services and 3rd party data technologies Experience with large scale solutioning and operationalization of data warehouses, data lakes and analytics platforms on GCP is a must. We are looking for candidates who have a broad set of technology skills across these areas and who can demonstrate an ability to design the right solutions with appropriate combination of GCP and 3rd party technologies for deploying on GCP cloud. Key Responsibilities: Work as part of an implementation team from concept to operations, providing deep technical subject matter expertise for successfully deployment of Data Platform Implement methods for automation of all parts of the pipeline to minimize labor in development and production. Identify, develop, evaluate, and summarize Proof of Concepts to prove out solutions Test and compare competing solutions and report out a point of view on the best solution Design and build production data engineering solutions to deliver our pipeline patterns using Google Cloud Platform (GCP) services:\\nSkills Required:\\nTools and Technologies:\\nGoogle Cloud Platform: Data Flow, Cloud Run, Big Query, Pub/Sub, Cloud Logging and cloud Monitoring etc\\nInfrastructure as code: Terraform / GitHub ELT Framework: DBT Orchestrator/ Schedule:\\nAstronomer/Airflow Programming Language/ Scripting Java/ python, Shell scripting IDE: VS code/ Eclipse etc Agile Software: Rally\\nSkills Preferred:\\nData Visualization Tools ETL tool: DataStage preferred.\\nExperience Required:\\nGoogle Cloud Platform (GCP) Certification preferred.\\n5+ years of Software Engineering experience developing applications w/Java and Python 3+ years of GCP experience 5 yrs of data engineering pipielines/ building data warhouse systmes with abiility to understand ETL principles and write complex sql queries.\\n3 years of GCP experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion, Dataflow, Dataproc, etc.\\n1 Year experience of deploying google cloud services using Terraform 1 Yr or more experience in ELT framework DBT.\\nEducation Required:\\nBachelor's in computer science\",\n",
       "  'JobSalary': '$80K - $111K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Nonprofit Organization',\n",
       "  'CompanySector': 'Education',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Education & Training Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Farm Credit East\\n4.1',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Enfield, CT',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.\\n\\nPosition Summary\\nThe Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.\\n\\nDuties and Responsibilities\\nWork with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.\\nDesign data models to meet requirements\\nPerform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.\\nDesign and develop data access methods, datasets, views etc.\\nDevelops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.\\nCoordinates new data developments to ensure consistency with existing warehouse structure.\\nCollaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.\\nAssists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.\\nAssists in identification of data integrity problems and recommends solutions.\\nWork collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).\\n\\nJob Qualifications/Requirements\\nBachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.\\nExperience with MSFT SQL Server\\nMicrosoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)\\n2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.\\n1 + years of experience using Microsoft Azure product to perform ETL\\nFamiliar with Databricks Unity catalog\\n\\nFarm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .\",\n",
       "  'JobSalary': '$73K - $99K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1916',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'PRO IT\\n4.9',\n",
       "  'JobTitle': 'AWS Data Engineer',\n",
       "  'JobLocation': 'Hartford, CT',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Data Engineer\\nLocation: Hartford, CT 100 % onsite position candidates has to relocate onsite from day 1\\nDuration Fulltime (No visa transfer)\\nOverview: Data Migration Engineer having an experience migrating data from on prem to cloud.\\nMust Have: AWS (S3, Aethna, EMR, EC2), Spark, PySpark\\nGood to have: Databricks, Snowflake\\nRequirements:\\nCandidate must be experienced working in projects involving data migration on AWS\\nExperience on data migration from On-Prem databases to AWS Cloud on S3\\nUnderstands where to obtain information needed to make the appropriate decisions\\nDemonstrates ability to break down a problem to manageable pieces and implement effective, timely solutions\\nIdentifies the problem versus the symptoms\\nManages problems that require involvement of others to solve\\nReaches sound decisions quickly\\nDevelops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit\\nRoles & responsibilities:\\nActs as a single point of contact for data migration to AWS projects for customer\\nProvides innovative and cost-effective solution using AWS, Spark, python & customer suggested toolset\\nOptimizes the use of all available resources\\nDevelops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit\\nAs a leader in the Cloud Engineering you will be responsible for the overseeing development\\nLearn/adapt quickly to new Technologies as per the business need\\nDevelop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability\\nSkills:\\nThe Candidate must have 3-5 yrs of experience in AWS, PySpark & Python\\nHands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR\\nExperience on spark scripting\\nHas working knowledge on migrating relational and dimensional databases on AWS Cloud platform\\nRelevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.\\nStrong experience with relational databases and data access methods, especially SQL.\\nKnowledge of Amazon AWS architecture and design\\nJob Type: Full-time\\nPay: $83,383.40 - $120,987.64 per year\\nCompensation package:\\nYearly pay\\nSchedule:\\n8 hour shift\\nExperience:\\nData Engineer: 1 year (Preferred)\\nPy spark: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: On the road\\nSpeak with the employer\\n+91 6365651095',\n",
       "  'JobSalary': 'Employer Provided Salary:$83K - $121K',\n",
       "  'CompanyRating': '4.9',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Restaurant Brands International\\n3.6',\n",
       "  'JobTitle': 'Data Engineer I',\n",
       "  'JobLocation': 'Miami, FL',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"About Restaurant Brands International:\\nRestaurant Brands International Inc. is one of the world's largest quick service restaurant companies with more than $35 billion in annual system-wide sales and over 28,000 restaurants in more than 100 countries. RBI owns four of the world's most prominent and iconic quick service restaurant brands – TIM HORTONS®, BURGER KING®, POPEYES® and FIREHOUSE SUBS®. These independently operated brands have been serving their respective guests, franchisees and communities for decades. Through its Restaurant Brands for Good framework, RBI working towards its goal of improving sustainable outcomes related to its food, the planet, and people and communities.\\nWe are seeking a Junior Data Engineer to join our team. The ideal candidate will have experience with AWS, Data warehouses such as Snowflake, Python language, and workflow orchestration such as Airflow and/or Dagster.\\nResponsibilities:\\nDesign, build, and maintain data pipelines and ETL processes\\nDevelop and maintain data warehouse and data lake solutions\\nCollaborate with data scientists and analysts to ensure data accuracy and consistency\\nOptimize data storage and retrieval for performance and cost efficiency\\nMonitor and troubleshoot data pipelines and systems\\nRequirements:\\nBachelor's degree in computer science, Engineering, or related field. Equivalent experience on the software engineering field would be also considered.\\n1-2 years of experience in data engineering\\nStrong programming skills in Python\\nExperience with AWS. Be able to design and architect AWS Infrastructure as Code using Terraform and/or equivalents such as AWS Cloudformation, CDK, Serverless Framework. Technologies you will be using in AWS will include S3, KMS, Glue/EMR, SNS, SQS, Athena.\\nExperience working with AWS Glue and/or AWS EMR is a plus.\\nExperience working with Data Warehouse Snowflake and/or Redshift.\\nFamiliarity with workflow orchestration Airflow and/or Dagster.\\nStrong problem-solving and analytical skills.\\nExcellent communication and collaboration skills.\\n\\nIf you are passionate about data engineering and want to work with cutting-edge technologies, we encourage you to apply for this exciting opportunity.\\nPlease submit your resume and cover letter for consideration.\\n\\nRestaurant Brands International and all of its affiliated companies (collectively, RBI) are equal opportunity and affirmative action employers that do not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or veteran status, or any other characteristic protected by local, state, provincial or federal laws, rules, or regulations. RBI's policy applies to all terms and conditions of employment. Accommodation is available for applicants with disabilities upon request.\",\n",
       "  'JobSalary': '$74K - $99K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Restaurants & Food Service',\n",
       "  'CompanyYearFounded': '2014',\n",
       "  'CompanyIndustry': 'Restaurants & Cafes',\n",
       "  'CompanyRevenue': '$5 to $10 billion (USD)'},\n",
       " {'CompanyName': 'KEA\\n4.4',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Rockwall, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"We have an immediate need for a Data Engineer whose primary role will be compiling comprehensive data and information on energy and commercial real estate data, and managing the back-end framework while helping the team utilize the data to create front-end visuals. This also involves architecting the back-end system to be scalable in use with web applications. Use multiple software platforms and develop and create reports and graphics to show trends and analysis for internal and external reports. The role is based in Rockwall, 10 miles east of Dallas, Texas. We are a team-first culture that involves leaning on the expertise around you, so any experience in this regard is helpful.\\n\\nAnalyze, automate, and prepare data for filings in certain key markets\\nTrack, maintain and disseminate detailed data for major CRE markets, including: Leasing activity and comparables, Building and land sales activity and comparables, Ownership analysis\\nUtilize shape files and platforms such as Mapbox and ArcGIS and tie back to big data\\nPrepare and develop a monthly market report detailing sales transactions at the submarket level in major markets\\nAnalyze and integrate large amounts of oil & gas data into our system\\nWork with the business development team to prepare external market reports as thought leadership in the marketplace\\nPrepare and create multiple template reports out of multiple software systems\\n\\nBachelor's degree in economics, real estate, computer science, engineering, or finance\\nUsage and full understanding of shape files and a platform such as Mapbox or ArcGIS\\nUnderstanding of Python, C#, and R\\nWeb application building a plus\\n3+ years of work experience\\nPrevious usage of Tableau or Alteryx\\nStrong data and analysis skills including Excel and Microsoft Office\\nStrong communication skills, with the proven ability to coherently ideas and opinions using the data and communicate them\\n\\nFull health benefits\\n1-2 days at home per week\\nFull dental benefits\\n401(k) 4% automatic contribution\\nCompetitive compensation\\n9/80 opportunities\",\n",
       "  'JobSalary': '$67K - $94K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1978',\n",
       "  'CompanyIndustry': 'Accounting & Tax',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'EarnIn\\n4.2',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"About EarnIn\\nAs one of the first pioneers of earned wage access, our passion at EarnIn is building products that deliver real time financial flexibility for those with the unique needs of living paycheck to paycheck. Our community members access their earnings as they earn them, with options to spend, save, and grow their money without mandatory fees, interest rates, or credit checks. Since our founding, our app has been downloaded over 13M times and we have provided access to $15 billion in earnings.\\nWe're fortunate to have an incredibly experienced leadership team, combined with world-class funding partners like A16Z, Matrix Partners, DST, and a very healthy core business with a tremendous runway. We're growing fast and are excited to continue bringing world class talent onboard to help shape the next chapter of our growth journey.\\nPosition Summary\\nWe are looking for experienced, passionate and resourceful senior level data engineers to join our data engineering team. As a senior data engineer you will work cross functionally with various teams and contribute to the design and development of our data products and various pipelines.\\nThis is a remote position. The US base salary range for this full-time position is $140,000 $252,000 + equity + benefits. Our salary ranges are determined by role, level, and location.\\nWhat You'll Do:\\nWork with the leadership across EarnIn to deliver on our mission: building a financial system that works for everyday people.\\nBe a strong partner to product and other engineering teams to drive improvements to business metrics and build new features.\\nCreate an architecture improvement roadmap and drive its implementation with your team across the company.\\nLead large scale system redesigns initiatives to shape the future of EarnIn's platform.\\nEstablish processes to foster adoption of best practices across the organization.\\nRecruit other great engineers for your team and help in the recruiting efforts for other teams.\\nBe a mentor and coach junior to staff level engineers by enabling them to do their best work.\\nProvide technical leadership to developers both in your team and across the engineering organization.\\nManage cross-functional relationships with all teams to deliver high quality products.\\nContribute to engineering wide initiatives at EarnIn.\\nWhat We're Looking For:\\nDrive the design & implementation of new features - break down complex problems into their bare essentials, translate this complexity into elegant design and create high quality, clean code.\\nMake a meaningful impact in the lives of our community members.\\nLead, design, develop and deliver large-scale data systems, data processing, and data transformation projects.\\nCollaborate and mentor other engineers while providing thoughtful guidance using code, design and architecture reviews.\\nContribute to defining technical direction, planning the roadmap, escalating issues, and synthesizing feedback to ensure team success.\\nEstimate and manage team project timelines and risks.\\nCare passionately about producing high quality, efficient designs and code.\\nParticipate in hiring and onboarding for new team members.\\nLead cross-team engineering initiatives.\\nConstantly be learning about new technologies and industry standards in data engineering.\\nAt EarnIn, we believe that the best way to build a financial system that works for everyday people is by hiring a team that represents our diverse community. Our team is diverse not only in background and experience, but also in perspective. We celebrate our diversity and strive to create a culture of belonging. EarnIn does not unlawfully discriminate on the basis of race, color, religion, sex (including pregnancy, childbirth, breastfeeding or related medical conditions), gender identity, gender expression, national origin, ancestry, citizenship, age, physical or mental disability, legally protected medical condition, family care status, military or veteran status, marital status, registered domestic partner status, sexual orientation, genetic information, or any other basis protected by local, state, or federal laws. EarnIn is an E-Verify participant.\\nEarnIn does not accept unsolicited resumes from individual recruiters or third party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or HR tea\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2012',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'ABBVIE\\n3.9',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Crystal Lake, IL',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"AbbVie’s Brand and Commercial Operations Business Technology Team (BCo) is working in partnership with AbbVie’s HCP CRM team to ensure right data is available on time and smooth execution of campaigns. The HCP Data Operations workstream is positioned to support this effort via best-in-class data innovation, agility, and overarching solution delivery to enable HCP CRM for seamless execution.\\nAs the HCP Data Operations workstream works to support the business at large across activities associated with marketing campaign execution, this is a great opportunity to be a part of this team in leading the delivery of quality data and helping identify efficiency opportunities. The Data Engineer will work in a collaborative and agile environment to that aim to compliantly assist in the physicians’ education of AbbVie’s approved products. This helps the physicians make the most informed treatment decision for their patients.\\nThe Data Engineer will work collaboratively with HCP CRM business team, other BTS functions and service providers to provide best in class service experience. The work will be fast-paced, dynamic, and afford many learning opportunities both functionally and technically in overall service delivery. This individual will use a broad understanding of the HCP data ecosystem needs support data engineering across AbbVie’s HCP data eco system. The ideal candidate will be a proponent for innovation, best practices, sound design with data & information optimization in mind, strong development habits, and efficient team/project structures. You will help enable and grow our culture of data driven insights and decision making, improving operational productivity by shortening delivery times.\\n\\nResponsibilities:\\nProvide innovative solution ideas along with execution strategies to help drive optimization across the AbbVie HCP Data Ecosystem.\\nWorks with stakeholders and service providers to deliver high quality service.\\nHelps the business execute HCP campaigns on AbbVie Digital Lab model.\\nData processing & QC checks across multiple layers of data warehouses, combination of multiple internal and external data sets.\\nOnboarding, consolidating, cleansing, and structuring data for efficient use in reports, analytical applications, and machine learning models.\\nPerforming data analysis and reconciliation on datasets of varying sizes and complexities.\\nUnderstand organization's vision, goals, and strategies. Aligns department priorities appropriately, determines critical success factors, evaluates, and pursues initiatives based on fit with strategies.\\n\\nQualifications:\\nBachelor's degree with 2+ years of relative work experience in a quantitative discipline (e.g. applied math, operation research, computer science, engineering, etc.)\\nFamiliarity with BI/DW and Analytics and proven academic or professional experience.\\nAbility to function effectively in an environment with multiple and fluctuating priorities.\\nFamiliarity with navigating in both a relational (i.e. Oracle or Teradata) and non-relational (i.e. Snowflake on AWS) environment.\\nExperience with Business Intelligence tools such as QlikView, Qlik Sense, Power BI, Tableau, etc. preferred.\\nExcellent written and oral communication skills.\\nExperience working with cross functional teams, third party service providers and vendors.\\nExcellent analytical and problem-solving skills.\\n\\n\\nAbbVie is an equal opportunity employer including disability/vets. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a disabled veteran, recently separated veteran, Armed Forces service medal veteran or active duty wartime or campaign badge veteran or a person’s relationship or association with a protected veteran, including spouses and other family members, or any other protected group status. We will take affirmative action to employ and advance in employment qualified minorities, women, individuals with a disability, disabled veterans, recently separated veterans, Armed Forces service medal veterans or active-duty wartime or campaign badge veterans. The Affirmative Action Plan is available for viewing in the Human Resources office during regular business hours.\",\n",
       "  'JobSalary': '$83K - $110K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Pharmaceutical & Biotechnology',\n",
       "  'CompanyYearFounded': '2013',\n",
       "  'CompanyIndustry': 'Biotech & Pharmaceuticals',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'INTELETECH GLOBAL INC\\n3.7',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Altamonte Springs, FL',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Role: Data Engineer\\nLocation: Florida\\nType: Contract ( Only w2 )\\nVisa: Any\\nExperience level: Mid-level\\n\\nThe Role:\\nYou'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.\\nYou'll be ensuring product deliverables are executed reliably and accurately on a regular basis.\\nYou'll be managing and monitoring client interfaces to ensure timely delivery of data.\\n\\n\\nQualifications :\\nBachelor's Degree in computer science, physics, math, or a related field\\nMinimum 3+ years experience in data engineering in industrial or clinical settings\\nExperience in deploying data pipelines to production, including large-scale cloud deployments\\nAdaptability within a dynamic and collaborative environment\\nCommitment to improve processes and reduce inefficiencies\\nDeep curiosity to dive into the details of human research studies\\n\\n\\nWe also appreciate if you have familiarity with the following:\\n\\nPython\\nSQL\\nCloud platforms (e.g. AWS)\\nRelational and no-SQL database systems\\nMulti-modal and sensor data\",\n",
       "  'JobSalary': '$66K - $94K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Accounting & Tax',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Financial Services',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Amazon.com\\n3.8',\n",
       "  'JobTitle': 'Data Center Engineer',\n",
       "  'JobLocation': 'Sterling, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'JOB RESPONSIBILITIES\\nResponsible for the on-site management of shift technicians, senior shift technicians, sub-contractors and vendors, ensuring that all work performed is in accordance with established practices and procedures.\\nEstablish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance.\\nWork with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency.\\nOperate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc.\\nDeliver quality service and ensure all customer demands are met.\\nJOB REQUIREMENTS\\n2-4 years of Data Center Engineering Experience\\n2-4 years of Data Center Management Experience\\nFundamental knowledge of network design and layout as well as low voltage (copper/ fiber) cabling\\nJob Type: Contract\\nPay: $30.00 - $37.49 per hour\\nSchedule:\\n12 hour shift\\nDay shift\\nNight shift\\nAbility to commute/relocate:\\nSterling, VA: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData Center: 2 years (Required)\\nLinux: 1 year (Required)\\nWork Location: In person',\n",
       "  'JobSalary': 'Employer Provided Salary:$30.00 - $37.49 Per Hour',\n",
       "  'CompanyRating': '3.8',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1994',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Allen Integrated Solutions',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Washington, DC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Data Engineer\\nTS/SCI Required\\nBuild and maintain data systems and construct datasets that are easy to analyze and support customer requirements. Implement methods to improve data reliability and quality. Combine raw information from different sources to create consistent and machine-readable formats. Develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. Develop and deploy Application Programming Interfaces (API) to expose IDST maintained data to the enterprise.\\nRequired Skills: Python, SQL, noSQL, Cypher, POSTGRES\\nPreferred Skills (not limited to): SQLAlchemy, Flask, Swagger, JavaScript, Spark, Hadoop, Kafka, Hive, R, storm, Matlab, Neo4J, MongoDB\\nPerformance Duties:\\nAcquire and assemble large, complex datasets that align with USSOCOM enterprise requirements\\nBuilding, testing, and maintaining data infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using modern data technologies\\nDevelop analytics tools and transformative algorithms for data to provide actionable insights into customer processes, operational efficiency and other key business performance metrics\\nCreate new data validation methods for analytics and data scientist team members that assist them in building and optimizing products to fulfill customer objectives\\nWork with IDST stakeholders including USSOCOM leadership, customers, and design teams to assist with data-related technical issues and support their data infrastructure needs\\nAnalyze procedures for USSOCOM data separation, access, and security across users and the enterprise data architecture\\nWork with IDST data and analytics experts to strive for greater functionality in our data systems and capability integration\\nCreate and maintain optimal data pipeline architecture and support associated process improvements for automating manual processes, data delivery, and infrastructure re-design for scalability\\nRequired Skills/Qualifications:\\nPossess a minimum of a bachelor's degree in computer science, IT, or similar field.\\n8 years' experience as a data engineer or in a similar role.\\nDemonstrated experience employing data models, data mining, and segmentation techniques.\\nDemonstrated experience developing, deploying and/or maintaining enterprise level data solutions\\nExperience with SQL database design\\nData engineering certification is a plus\\nCurrent DoD Top Secret clearance and eligible for SCI access and ACCM read-on\",\n",
       "  'JobSalary': '$88K - $126K (Glassdoor est.)',\n",
       "  'CompanyRating': None,\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Aerospace & Defense',\n",
       "  'CompanyYearFounded': '2014',\n",
       "  'CompanyIndustry': 'Aerospace & Defense',\n",
       "  'CompanyRevenue': '$1 to $5 million (USD)'},\n",
       " {'CompanyName': 'Merkle\\n3.7',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'New York, NY',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Company Description\\n\\nDentsu/Merkle is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsu mcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.\\nWe are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.\\n\\nJob Description\\nApplicants must be currently authorized to work in the United States on a full-time basis. No sponsorship or work transfers are available for this position\\nMust currently live in and have a permanent residence in the United States\\nThis position will be part of Merkle DBA team. This team is responsible for guiding the full lifecycle of a Relational and/or NO-SQL based solutions, including infrastructure analysis, platform and infrastructure selection, technical architecture design, application design and development, testing, and deployment.\\nTeam also serves as a point of escalation for support services on multiple data platforms. Team acts as an authoritative source for data technologies providing information on problem resolution and configuration best practices.\\nPosition Responsibilities:\\nAdministration, Installation, Configuration, Support, Maintenance, and Troubleshooting of multiple data management platforms like MS SQL Server, Netezza, RedShift, Azure SQL/Synapse, Big Query, MySQL, Snowflake etc.\\nMay be involved with the following: troubleshooting, recovery, tuning of the database, software installation and upgrades, resolving errors and failures, auditing activities, and resource utilization\\nSchedule, plan, and document installation and testing\\nDevelop standards and guidelines\\nModify existing databases and database implementations or direct programmers and analysts to make changes\\nPlan, coordinate and implement database security measures to safeguard information.\\nCommunicate with internal users and answer questions, documentation.\\nOther duties as assigned\\n\\nQualifications\\nAt least 2 years of experience on a commercial or open-source Relational Database Management System (RDMS) including but not limited to Database Administration, development, and/or maintenance with the desire and motivation to expand upon current skill set\\nExperience with any of these: Oracle, MySQL, Postgres, SQL Server, Redshift, Snowflake etc.\\nMotivation to receive internal training and mentoring in different Data Management Technologies\\nFamiliarity with Cloud, MPP and Data Warehousing concepts is preferred\\nBachelor’s Degree in computer science, Information Systems, or a related field\\nMust be authorized to work for our company in the United States now and in the future.\\n\\nAdditional Information\\n\\nAt Merkle, we believe that a diverse environment improves us as a community and as a business. We want to foster an environment of growth, where all ideas and contributions are encouraged. We need this culture of courage to continue to thrive in our fast-paced industry. We embrace differences of opinion. We value diversity of experience and thought, which help us to challenge and define industry-leading solutions, and support our goal of being a great place to work.\\nAll your information will be kept confidential according to EEO guidelines.\\nThe anticipated salary range for this position is $78,000-$121,000. Salary is based on a wide range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit https://dentsubenefitsplus.com/\\nEmployees from diverse or underrepresented backgrounds encouraged to apply.\\nDentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.\\n#LI-CR1\\nAbout dentsu\\nDentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.\\nWe are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.\\nDentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.',\n",
       "  'JobSalary': 'Employer Provided Salary:$78K - $121K',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Media & Communication',\n",
       "  'CompanyYearFounded': '1971',\n",
       "  'CompanyIndustry': 'Advertising & Public Relations',\n",
       "  'CompanyRevenue': '$500 million to $1 billion (USD)'},\n",
       " {'CompanyName': 'Loparex LLC\\n3.2',\n",
       "  'JobTitle': 'ETL Data Warehouse Engineer / ETL Developer',\n",
       "  'JobLocation': 'Cary, NC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"The ETL Data Warehouse Engineer / ETL Developer, is a critical role that is instrumental to Loparex achieving its operational and strategic performance goals. The position provides financial and data analytics support to the Global Analytics team as well as Global Operational Finance through helping develop standardized data in a global data warehouse so our business intelligence team can utilize.\\n\\nPRIMARY JOB DUTIES AND RESPONSIBILITIES\\nAssist in designing and developing an enterprise-wide data warehouse solution to empower data driven decision making throughout all levels of the organization\\nWork with the SAP, APRISO, and Salesforce teams to align data and organize into star schemas\\nUnderstand business requirements and convert them into detailed technical specifications for Data models and ETL programs\\nDesign the Logical and Physical data models using the Dimensional modeling techniques\\nCoach and guide the technical team in terms of the development of ETL jobs\\nPerformance improvement & tuning of Matillion mappings\\nTechnical reviews, Data validation & end testing of ETL Objects, Source data analysis, and data profiling\\nCreate necessary documentation as per the Standards and Update the project documentation\\nSupport QE on End-to-End testing and provide daily/weekly project status to the Project manager\\nWork with the deployment team in the code migration across the environments\\n\\nJOB SKILLS, EXPERIENCE, AND KNOWLEDGE REQUIREMENTS\\nDesign & develop ETL processes for loading data into different layers of the Cloud-based Snowflake Data Ecosystem\\nData Engineering and Data Modeling to optimally integrate data for use in the analytics layer\\nIdentify and share development best-practices across the team\\n5+ years of experience in ETL, Data Migration, API, EDI or similar integration patterns\\nProven experience using Matillion to load Snowflake database\\nExperience in loading data into Relational and Dimensional models\\nDevelops Methodologies GIT, CI/CD\\nBachelor's Degree required\\n\\nBENEFITS\\nMedical, Dental, and Vision Insurance\\nTelehealth\\nHealth savings account with generous annual employer contribution\\nFitness reimbursement program\\nDevelopment and career growth opportunities\\nCompetitive 401(k) matching program\\n\\nWe are an equal opportunity employer. All qualified applicants will be considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other category protected by applicable law.\\n\\n#LI-ND1\",\n",
       "  'JobSalary': '$86K - $114K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.2',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Manufacturing',\n",
       "  'CompanyYearFounded': '1906',\n",
       "  'CompanyIndustry': 'Consumer Product Manufacturing',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Juul Labs\\n3.4',\n",
       "  'JobTitle': 'AI/Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"THE COMPANY:\\nJuul Labs' mission is to impact the lives of the world's one billion adult smokers by eliminating combustible cigarettes. We have the opportunity to address one of the world's most intractable challenges through a commitment to exceptional quality, research, design, and innovation. Backed by leading technology investors, we are committed to the same excellence when it comes to hiring great talent.\\nWe are a diverse team that is united by this common purpose and we are hiring the world's best engineers, scientists, designers, product managers, operations experts, and customer service and business professionals. If the opportunity to build your career at one of the fastest growing companies is compelling, read on for more details.\\nUSA REMOTE\\nKEY RESPONSIBILITIES:\\nLeverage Python to design robust, reusable and scalable data solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured data, ensuring data cleanliness and formatting\\nDrive development of large-scale data engineering and data science projects\\nMonitor model performance and update models as new data becomes available, employing automated pipelines for retraining and deployment\\nPartner with other Data Scientists, Data Engineers and Business Analysts to build configurable, scalable, and robust data processing infrastructure\\nWork closely with our sales, operations, research, and finance teams on data storage, retrieval, and analysis\\nDevelop new systems and tools to enable stakeholders to consume and understand data more intuitively\\nCreate and establish design standards and assurance processes to ensure compatibility and operability of data connections, flows and storage requirements\\nValidate model transformations for data integrity (source/target tables values and counts are expected, ensurance of proper data cleansing)\\nInterpret and communicate model decisions and feature importance using methods like SHAP or LIME to gain insights and improve model transparency\\nLeverage ensemble techniques, such as stacking or bagging, to combine multiple models and enhance the accuracy of lead forecasts\\nUtilize VertexAI or MLflow, an open-source platform for managing the machine learning lifecycle, to track and reproduce experiments, manage model versions, and facilitate model deployment\\nOUR DATA STACK:\\nGoogle Cloud Platform - GCP (BigQuery, Storage, Dataflow, Pub/Sub, Cloud Functions/Run, Vertex AI, Cloud Build)\\nDatabricks, Delta Lake, MLflow\\nAirflow, Fivetran\\nDBT\\nPERSONAL AND PROFESSIONAL QUALIFICATIONS:\\nComfortable working in a scrum agile environment using Jira\\nAdvanced knowledge in Python to process large-scale data engineer models and workflows\\nProficiency in relevant libraries/frameworks, including pandas, NumPy, scikit-learn, PyMC3\\nKnowledge of prior work with TensorFlow, Keras, and/or PyTorch is a plus\\nSkilled in analytical SQL in support of data modeling and manipulating multiple data formats\\nExperience with version control (Git) and containers (Docker)\\nKnowledge of bash/shell and orchestration tools (e.g. Airflow), is preferred\\nEDUCATION:\\nBachelor's degree or Master's degree in Computer Science, Engineering, Math, or equivalent experience (Bachelor's degree with 2-5 years of experience or Master's degree with 1-3 years of experience)\\nJUUL LABS PERKS & BENEFITS:\\nA place to grow your career. We'll help you set big goals - and exceed them\\nPeople. Work with talented, committed and supportive teammates\\nEquity and performance bonuses. Every employee is a stakeholder in our success\\nCell phone subsidy, commuter benefits and discounts on JUUL products\\nExcellent medical, dental and vision benefits\\nJuul Labs is proud to be an equal opportunity employer and is committed to creating a diverse and inclusive work environment for all employees and job applicants, without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. We will consider for employment qualified applicants with arrest and conviction records, pursuant to the San Francisco Fair Chance Ordinance. Juul Labs also complies with the employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for Juul Labs in the US.\\nSALARY RANGES:\\nSalary varies by role, level and location, and is dependent on the cost of labor in a given\\ngeographic region among other factors. These ranges may be modified at any time.\\n\\nLOCATIONS:\\nTier 1 Locations: Greater New York City, and San Francisco Bay Area\\nTier 2 Locations: Greater Boston, Washington DC Metropolitan Area, Seattle/Tacoma,\\nGreater Sacramento, Los Angeles/OC/San Diego\\nTier 3 Locations: Rest of New England, NY Capital District, Rest of New Jersey, Greater\\nPhiladelphia, Pittsburgh, Delaware, Rest of Maryland, Rest of Virginia, North Carolina,\\nAtlanta, Miami-Fort Lauderdale-WPB, Chicagoland, Dallas, Houston, Austin,\\nMinneapolis/St. Paul, Colorado, Phoenix, Reno, Las Vegas, Portland Ore./Vancouver\\nWash., Rest of California, Hawaii\\nTier 4 Locations: Rest of US including Alaska and Puerto Rico\\nTier 1 Range:\\n$110,000—$165,000 USD\\nTier 2 Range:\\n$102,000—$153,000 USD\\nTier 3 Range:\\n$95,000—$143,000 USD\\nTier 4 Range:\\n$87,000—$131,000 USD\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.4',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Manufacturing',\n",
       "  'CompanyYearFounded': '2007',\n",
       "  'CompanyIndustry': 'Consumer Product Manufacturing',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Trellance, Inc.\\n2.4',\n",
       "  'JobTitle': 'Data Engineer (Python)',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Description:\\nCompany Overview:\\nTrellance is the leading provider of data analytics and business intelligence solutions, professional services and consulting for credit unions. The company’s solutions and services, together with the patented common data model, are used by credit unions to find actionable insights, improve member experience and achieve portfolio growth.\\n\\nOverall Responsibility:\\nThe Data Engineer will have a strong background in software development and be proficient in Python programming language. The successful candidate will need at least three years of hands-on experience in PySpark, Pandas, and Unix.\\n\\nApplicants must be authorized to work in the U.S. on a full-time basis. We are unable to sponsor or take over the sponsorship of employment visas for this role.\\nEssential Functions:\\nCollaborate with cross-functional teams to design, develop, and maintain software solutions using Python, PySpark, Pandas, and Unix.\\nDevelop and maintain data processing pipelines and ETL workflows using PySpark and Pandas.\\nWrite efficient and scalable code that meets the company's coding standards and best practices.\\nOptimize and improve the performance of existing code and processes.\\nTroubleshoot and debug software issues in a timely manner.\\nCommunicate with stakeholders to understand their requirements and deliver solutions that meet their needs.\\nParticipate in code reviews and contribute to the team's codebase.\\nPerforms other operational related duties as assigned.\\nRequirements:\\nMinimum Education/Experience:\\nBachelor’s degree (BA or BS) from an accredited college or university plus a minimum of three (3) years of experience; degree in computer science, or related field preferred. Or high school diploma plus a minimum of six (6) years of experience in this field.\\nExperience:\\n3+ years of experience in Python software development.\\nExperience with PySpark, Pandas, and Unix.\\nExperience with Hive, SQL Server.\\nStrong understanding of software development principles, such as data structures, algorithms, and design patterns.\\nExperience with data processing pipelines, ETL workflows, and distributed systems.\\nFamiliarity with software development tools and methodologies, such as Agile, Scrum, Git, and JIRA.\\nExcellent problem-solving skills and attention to detail.\\nStrong communication and interpersonal skills.\\nAbility to work independently and as part of a team.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '2.4',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Software Development',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Nonprofit Organization',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'PRIMUS Global Services, Inc\\n4.0',\n",
       "  'JobTitle': 'Data Engineer – SQL development, Azure Data Factory – REMOTE WORK 44298',\n",
       "  'JobLocation': 'Texas',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'We have an immediate long-term contract opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.\\n\\nAs a Data Engineer, you will play a critical role in developing and maintaining our data infrastructure, ensuring the smooth flow of information and enabling effective data-driven decision-making.\\n\\nRequirements:\\nStrong proficiency in SQL development, with hands-on experience in building complex queries, stored procedures, and functions. Solid experience with SSIS (SQL Server Integration Services) for data integration and ETL processes. Proficiency in Azure cloud services, including Azure Data Factory, Azure SQL Database, and Azure Data Lake. Knowledge of mining or industry-related data and processes is highly advantageous.\\n**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**\\nFor immediate consideration, please contact:\\nSneha\\nPRIMUS Global Services\\nPhone No: 972-753-6500 Ext: 405\\nDirect: (972) 471-9498\\nEmail: jobs@primusglobal.com',\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Private Practice / Firm',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2002',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': 'Ampcus Incorporated\\n3.6',\n",
       "  'JobTitle': 'Azure Data Engineer',\n",
       "  'JobLocation': 'Washington, DC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'The Azure Data Engineer will be responsible for designing, developing, and maintaining data pipelines, data models, and data storage solutions using Azure services. The candidate will work closely with cross-functional teams to ensure successful delivery of projects and support business objectives. The ideal candidate should have a strong technical background in managing data platforms and architecture using Azure services and experience in data warehousing, data integration, and ETL tools.\\n\\nResponsibilities:\\nDesign, develop, and maintain data pipelines using Azure Data Factory to ingest data from various sources such as SQL Server, MongoDB, and Blob Storage into Azure SQL Database and Azure Cosmos DB and Kafka.\\nBuild data transformation logic using Databricks and Spark for data processing and analytics. Experience in tuning Databricks environments.\\nDesign / implement data models for data warehousing on Azure SQL DB.\\nConfigure and manage Hadoop and Spark clusters in Azure HDInsight for big data processing.\\nCreate and manage data storage solutions using Azure Blob and Data Lake.\\nImplement data governance and security measures to protect sensitive data.\\nCollaborate with cross-functional teams to ensure successful delivery of projects and support business objectives.\\nMentor junior data engineers and developers and stay up-to-date with emerging technologies and trends in Azure data engineering.\\nQualifications:\\nBS or MS degree in Computer Science or related fields\\nMinimum of 3 years of experience in managing data platforms and architecture using Azure services such as Azure Data Factory, Azure SQL Database, Azure Cosmos DB, and Azure HDInsight.\\nStrong experience in data warehousing, data integration, and ETL tools.\\nProficient in programming languages such as Python, SQL, and C#.\\nFamiliarity with NoSQL databases such as MongoDB and Cassandra.\\nExperience in version control tools such as Git and Bitbucket.\\nExcellent problem-solving and analytical skills.\\nStrong communication and collaboration skills.\\nAbility to work independently and in a team environment.\\nDesirable: familiarity with Aveva/OSISoft PI system and SCADA system',\n",
       "  'JobSalary': '$91K - $121K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'IntelliBridge LLC\\n3.7',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Washington, DC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Overview:\\n\\nIntelliBridge is an award-winning national security company looking for a Data Engineer to support our contract with the Department of Homeland Security.\\n\\nAs a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.\\n\\nResponsibilities:\\nDesign and develop software systems and applications that efficiently handle and process large datasets.\\nBuild and maintain data pipelines for extracting, transforming, and loading data from various sources into target systems.\\nCollaborate with data scientists, analysts, and stakeholders to understand data requirements and translate them into technical solutions.\\nOptimize data storage, retrieval, and processing techniques to ensure high performance and scalability.\\nImplement data quality and validation processes to ensure accuracy and reliability of data.\\nDevelop and maintain data models, schemas, and database structures for efficient data storage and retrieval.\\nIdentify and resolve issues related to data integrity, data transformation, and system performance.\\nImplement data security and privacy measures to protect sensitive information.\\nWrite clean, modular, and maintainable code and conduct thorough testing to ensure software quality.\\nStay updated with the latest trends and advancements in data engineering and software development.\\n\\nQualifications:\\nMaster’s degree (MA/MS) in Computer Science, Software Engineering, or a related field.\\nStrong proficiency in software development using programming languages such as Python, Java, or Scala.\\nSolid understanding of data engineering concepts, data structures, and algorithms.\\nExperience with distributed computing frameworks like Hadoop, Spark, or similar technologies.\\nFamiliarity with SQL and NoSQL databases, data modeling, and database design principles.\\nKnowledge of data integration tools and techniques, such as ETL (Extract, Transform, Load) processes.\\nExperience with version control systems, agile development methodologies, and software development best practices.\\nStrong problem-solving skills and the ability to analyze complex data-related issues.\\nExcellent communication skills and the ability to work effectively in a collaborative team environment.\\nPrior experience in working with big data platforms, cloud services is a plus.\\n\\nAbout Us:\\nIntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.',\n",
       "  'JobSalary': '$85K - $122K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'National Agencies',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Government & Public Administration',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Source Mantra\\n3.3',\n",
       "  'JobTitle': 'Data Engineer - Fulltime - Only GC & USC',\n",
       "  'JobLocation': 'New York, NY',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Role : Data Engineer\\nLocation: NY\\nDuration: Full Time\\nVisa Status: USC & GC (Only W2)\\nNeed 9-10+ Years of Experience/With Passport No & LinkedIn Mandatory – Full Time Role\\nMust Have:\\nSnowflake experience\\nGood experience on writing SQL Scripts / stored procedures / functions etc.\\nKnowledge of GitHub\\nKnowledge of CI/CD\\nKnowledge of AWS\\nAbility to Interpret and convert business rules into SQL scripts.\\nExperience in building ETL/ELT Pipelines\\nGood To Have:\\nExperience Java\\nUnderstanding of Kotlin\\nKnowledge of NO SQL database like DynamoDB\\nKnowledge of AWS Lambda & step functions\\nExperience in Python\\nJavaScript\\nPlease Fill The Below Details Reply Asap, With Updated Resume Along With H1B Or EAD Or GC With DL copy\\nCandidate Detail:\\nFull Name (as per SSN/ Passport):\\nEmail:\\nContact Number (Primary and Alternate :\\nCurrent Location:\\nWork Authorization and Visa Validity:\\nLast 4 Digits Of SSN\\nCurrently on a Project:\\nOnsite availability (Post-Selection)\\nWilling To Relocate:\\nLinkedIn ID :\\nYear of Bachelor’s & Education Branch, University Name\\nYear of Master’s & Education University Name\\nPassport Number :\\nNotice Period:\\nAvailability For Interview\\nPaul David\\nSr. Recruiter - Talent Acquisition, SourceMantra Inc\\nhttp://sourcemantra.com/\\nE: pauldavid@sourcemantra.com\\nP: 908-402-6934 ; Ext : 241\\nJob Type: Full-time\\nSalary: $110,000.00 - $120,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nNew York, NY: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData Engineer: 10 years (Preferred)\\nSnowflake: 5 years (Preferred)\\nData warehouse: 4 years (Preferred)\\nETL/ELT Pipelines: 5 years (Preferred)\\nSQL Scripts: 4 years (Preferred)\\nStored Procedures: 4 years (Preferred)\\nGitHub: 4 years (Preferred)\\nAWS: 3 years (Preferred)\\nPython: 5 years (Preferred)\\nDynamoDB: 1 year (Preferred)\\nWork Location: In person',\n",
       "  'JobSalary': 'Employer Provided Salary:$110K - $120K',\n",
       "  'CompanyRating': '3.3',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2007',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$1 to $5 million (USD)'},\n",
       " {'CompanyName': 'Denken Solutions Inc\\n4.2',\n",
       "  'JobTitle': 'AWS Data Engineer',\n",
       "  'JobLocation': 'Foster City, CA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Job Description:\\nAs a Data Engineer of Ride and Fleet software, you will:\\nCollaborate on our reliability KPIs, analyze trends, find gaps, and come up with mitigation plans\\nHelp us arrive to the best metrics to be on top of the end user's experience\\nBuild data pipelines for these metrics\\nCapture data in a way that will get the best signals to make hypotheses and run experiments to validate\\nHelp us with evaluating our dashboards for long-term Client needs\\nTo be successful, you should have:\\n4+ years of Data Engineering experience\\nPassion for building the best possible end-user experience\\nCuriosity, a desire to figure out how to make things better\\nFast learner\\nGreat communication and presentation skills\\nExperience with Looker, Elastic Search, Kibana, Grafana, Databricks, and other analytics tools\\nBonus:\\n2 years of Data Science experience\\nExperience with AWS technologies (e.g. Lambdas)\\nJob Type: Contract\\nPay: $70.00 - $75.00 per hour\\nAbility to commute/relocate:\\nFoster City, CA: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nLooker: 5 years (Required)\\nElasticsearch: 5 years (Required)\\nDatabrick: 5 years (Required)\\nData science: 2 years (Required)\\nAWS: 10 years (Required)\\nWork Location: In person\",\n",
       "  'JobSalary': 'Employer Provided Salary:$70.00 - $75.00 Per Hour',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2010',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Cube Hub Inc\\n3.2',\n",
       "  'JobTitle': 'Sr. Network Data Center Engineer III - LG',\n",
       "  'JobLocation': 'Santa Clara, CA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"5+ years of IT network operations/support and engineering experience in highly complex network environments containing Cisco and/or Arista, supported with a recognized industry certification such as: Arista Ace L3 or above and/or Cisco CCNA/CCNP/CCIE.\\nPreferred Qualifications:\\nAssociates/Bachelor's and/or Master's degree in Computer Engineering, Computer Science, Software Engineering, Mechanical Engineering or any other Engineering/Science related field\\nExperience with:\\nManaging a network fabric that includes supporting integrated components e.g., firewalls, load balancers, web app firewalls.\\nTCP/IP variable length subnet masking and address allocation\\nDynamic routing protocols e.g., OSPF, BGP\\nSDN solutions such as: Cisco ACI or Arista CVP, implementing Layer 2/3 redundancy such as link aggregation protocols and loop prevention protocols for example MLAG, LACP, STP, HSRP, VRRP\\nIndustry cabling standards, and the layers of the OSI model\\nNetwork security concepts\\nDNS/DHCP/NTP concepts and solutions\\nAlso, advantageous but not essential\\nDevelopment of network-oriented applications using Python and/or JavaScript with a React Framework\\nCI/CD methodology and GitHub/GitLab repository distribution\\nRelational Databases and developing advanced SQL queries.\\nBackend API development with Rest API and XML/JSON\\nUser level experience on Windows, Linux based platforms/appliances\\nResponsibilities will be, but are not limited to:\\nDetermination and deployment of robust, stable, automated, and manageable industry leading Data Center Network technologies as part of a global team.\\nDevelops automation for global data center management through SDN managed capabilities.\\nManage technical projects.\\nResearch new methods for automation to enable efficiencies in the global data center environment.\\nMonitor and perform capacity/feasibility studies and resolve network capacity issues. Identify, develop, and deploy tools supporting network services.\\nPlans and schedules work to meet deadlines established by others to ensure the completion of several related tasks.\\n#IND2\\nJob Type: Contract\\nPay: $60.00 - $65.00 per hour\\nBenefits:\\nHealth insurance\\nSchedule:\\n10 hour shift\\n8 hour shift\\nSupplemental pay types:\\nBonus pay\\nWork Location: In person\",\n",
       "  'JobSalary': 'Employer Provided Salary:$60.00 - $65.00 Per Hour',\n",
       "  'CompanyRating': '3.2',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'The Hartford\\n3.9',\n",
       "  'JobTitle': 'Sr. Data Engineer (REMOTE)',\n",
       "  'JobLocation': 'Chicago, IL',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day & not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place.\\nThe Hartford is seeking a Senior Data Engineer to join the Actuarial Technology and Innovation team to design, develop, and implement modern and sustainable data assets to fuel machine learning and artificial intelligence solutions across a wide range of strategic initiatives.\\nThe Actuarial Technology and Innovation team is a dynamic mix of Actuarial and Data Science professionals utilizing statistical modeling, machine learning, and advanced data engineering techniques to enhance core Actuarial processes. As a member of the LOB Engineering pillar, you will work directly with our Actuarial partners in building & designing tailor-fit data pipelines & processes. We are a forward-focused organization that fosters collaboration, encourages creative design, and offers abundant opportunities for visibility, allowing candidates to shape innovative solutions and showcase their talents to a wide audience.\\nAs a Senior Data Engineer, you will be at the forefront of driving impactful solutions throughout the entire software development lifecycle, ensuring reliable data delivery. Our team's culture is deeply rooted in embracing emerging technologies and empowering you to select the optimal tools for each unique project, so curiosity and adaptability are highly valued. Strong candidates will also demonstrate a solid foundation in data management, software engineering, and process automation along with an enthusiasm for delivering efficient solutions to partners.\\nResponsibilities:\\nDesign and develop high quality, scalable software modules for next generation analytics solution suite\\nPrototype high impact innovations, catering to changing business needs, by leveraging new technologies\\nConsult with cross-functional stakeholders in the analysis of short and long-range business requirements and recommend innovations which anticipate the future impact of changing business needs\\nFormulate logical statements of business problems and devises, tests and implements efficient, cost-effective application program solutions\\nIdentify and validate internal and external data sources for availability and quality. Work with SMEs to describe and understand data lineage and suitability for a use case\\nCreate data assets and build data pipelines that align to modern software development principles for further analytical consumption. Perform data analysis to ensure quality of data assets.\\nDevelop code that enables real-time modeling solutions to be ingested into front-end systems\\nProduce code artifacts and documentation using GitHub for reproducible results and hand-off to other data science teams\\nMinimum Qualifications:\\n3+ years of relevant experience recommended\\nBachelor’s degree in Computer Science, Engineering, IT, Management Information Systems, or a related discipline\\nProficiency in SQL and R or Python\\nProficiency in ingesting data from a variety of structures including relational databases, Hadoop/Spark, cloud data sources, XML, JSON\\nProficiency in ETL concerning metadata management and data validation\\nProficiency in Unix and Git\\nInterest & willingness to deeply understand Insurance & Actuarial business\\nAble to communicate effectively with both technical and non-technical teams\\nAble to translate complex technical topics into business solutions and strategies as well as turn business requirements into a technical solution\\nExperience with leading project execution and driving change to core business processes through the innovative use of quantitative techniques\\nPreferred Skills and Experience:\\nExperience with Machine Learning, Statistical Modeling, or Actuarial Science\\nExperience with agile software development\\nAWS Certification or experience with AWS Services (S3, EMR, etc.)\\nProficiency in Automation tools (Airflow, Cron, Autosys, etc.)\\nExperience with Cloud data warehouses, automation, and data pipelines\\nExperience with containerized computing (Docker, Kubernetes, etc.)\\nExperience building CICD pipelines or IAC (Infrastructure as Code)\\nCompensation\\nThe listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:\\n$110,560 - $165,840\\nEqual Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age\\nAbout Us | Culture & Employee Insights | Diversity, Equity and Inclusion | Benefits\\nSr Data Engineer - GE07BE\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Insurance',\n",
       "  'CompanyYearFounded': '1810',\n",
       "  'CompanyIndustry': 'Insurance Carriers',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Perforce Software\\n3.4',\n",
       "  'JobTitle': 'Data Center Engineer',\n",
       "  'JobLocation': 'Lowell, MA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Role: Data Center Engineer\\nLocation: Lowell, MA US\\nPerforce develops DevOps tools that improve software quality and security as well as team productivity for several of the world’s leading companies, such as PIXAR, CD Projekt Red, NASA, Verizon, Honda and NVIDIA.\\nPosition Summary:\\nPerfecto Mobile is looking for bright and talented people in every position. We believe that a great work environment is vital to the growth of our employees and business as a whole.\\nResponsibilities:\\nDeploy new mini servers into the Perfecto production environments\\nRemote management of servers.\\nTroubleshooting computer configuration, network and Mobile device issues.\\nConnect mobile devices to Perfecto Handset Servers and validate functionality\\nDrive new deployments of customer mobile device and web solutions\\nPerform device and Operating System upgrades (Windows, Linux, MacOS and Mobile devices)\\nUpdate Mobile device firmware and software.\\nMaintain inventory database\\nCreate and maintain procedures and technical documentation.\\nDocumentation of problems and their resolution to assist in future efforts.\\nEffectively communicate status to customers and internal staff\\nInterface with other internal support teams, both local and remote, to install new hardware and devices into the production environment, as well as work on standards, resolve issues and assist with the implementation of new technologies in the global Data Centers\\nOn-Boarding of new systems and devices into Perfecto Mobile, Partner, or customers’ data centers and lab environments.\\nPromote professional management and appearance of all data center cabling and assets\\nUse of Salesforce ticketing system to respond to trouble issues and to track efforts\\nRequirements:\\nMust be able to work in a team environment and perform routine as well as technical tasks.\\nWorking knowledge of Windows, MacOS and Linux. Understanding of basic Windows registry and Windows event log analysis.\\nBasic networking skills a plus\\nAbility to learn new technologies and technical concepts quickly.\\nPC literate – MS Word, Excel, Outlook, Access- an advantage\\nDetail oriented\\nHigh organizational ability: able to work and follow processes by instructions and methods.\\nStrong analytical, documentation, and communication skills.\\nExperience with trouble ticketing and change management tools.\\nManaged Service experience preferred.\\nExcellent English - written and verbal.\\nAbility to effectively communicate with international colleagues, customers and partners.\\nEnergetic individual who thrives in a high growth, entrepreneurial environment\\nExcellent customer skills are required.\\nAbility to work nights, weekends, and holidays as needed during crunch times or in support of maintenance activities.\\nBe open to occasional travel to other data centers in the US and internationally\\nIf you are passionate about the technology that impacts our day-to-day lives and want to work with people as talented and dedicated as yourself, apply today!\\nwww.perforce.com\\n\\nPerforce is an equal opportunity employer. We value diversity and celebrate its strengths.',\n",
       "  'JobSalary': '$66K - $102K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.4',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1996',\n",
       "  'CompanyIndustry': 'Enterprise Software & Network Solutions',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'Volvo Group\\n4.2',\n",
       "  'JobTitle': 'Engineer - Data Analytics',\n",
       "  'JobLocation': 'Greensboro, NC',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"United States\\nPosition Description\\n\\n\\n\\nDo you want to be a part of an organization developing solutions to address today’s global challenges? Would you thrive in an environment where you can work alongside highly skilled engineers in the transportation industry? Does being a part of shaping the future landscape of sustainable transport and infrastructure solutions excite you?\\n\\nIf this sounds exciting to you, we have a great opportunity at Volvo Group for a Data Analytics Engineer to join our Complete Vehicle team in Greensboro, NC.\\n\\nRead on for more details!\\n\\nWho We Are\\nOur team, Features Verification & Validation, is a key piece of the Complete Vehicle function for Volvo Group Trucks Technology, part of Volvo Group North America. Complete Vehicle contributes to deliver the right Products & Services to all Volvo Group brand customers according to product segmentation and brand positioning. Features Verification & Validation uses deep customer knowledge to break down customer features into technical requirements. We drive validation plans to reach the requested feature level and secure end customer satisfaction. We also manage the verification plans thanks to world class physical and virtual tests on the complete vehicle such as durability testing in rigs, fuel economy testing, reliability testing with customers and a variety of simulations and tests with trucks on proving grounds and open roads.\\n\\nTogether with us, you will be part of a global and diverse team of highly skilled professionals. We have a strong culture based on our company values, which are central to our work. We believe in a work environment where:\\nWe constantly strive for outstanding Performance.\\nWe are obsessed with Customer Success.\\nWe initiate Change to stay ahead.\\nWe willingly place our Trust in each other.\\nWe have a huge Passion for what we do.\\n\\nWhat You Will be Doing\\nAs a Data Analytics Engineer you will lead the pursuit in developing new ways of working with and utilizing data analytics tools to support new product development. You will do this by engaging with various teams and stakeholders to identify needs and develop end-to-end solutions based on those requirements. This requires the ability to interpret the principals, theories, and concepts of various specialized engineering areas at any given time.\\n\\nAdditional Responsibilities Include:\\nLead all data analytics activities for the Features Verification & Validation function in North America.\\nDevelop procedures and methods for data processing and storage.\\nGenerate high quality source codes based on company testing requirements.\\nTroubleshoot and debug applications in a timely manner.\\nDevelop and update technical documentation.\\nConduct trainings with engineers and other stakeholders.\\nDevelop roadmap for data analytics tools for FVV North America.\\nWork closely with external vendors to develop tools for data analytics.\\nSupport implementation of projects and tools related to data analytics.\\nEstablish and maintain computational hardware strategy for data processing.\\nAct as voice of Complete Vehicle North America in regard to data analytics which will include participation in various cross-functional, global forums.\\nActively follow industry trends for data analytics related to data science.\\n\\nWho You Are\\nBachelor’s Degree in Computer Science, Data Analytics, or a related field; Master’s Degree preferred.\\n3-5 years of related data science experience.\\nStrong understanding data science concepts including data exploration, statistical modeling, and machine learning.\\nFamiliarity with data engineering principles, automation, databases, data architecture and data visualizations.\\nExperience working with Time series and Text based data.\\nIn depth knowledge of code generation in Python and SQL any other programming language is a plus\\nPrevious experience in Power BI is desired\\nAbility to efficiently manage multiple activities simultaneously\\n\\nCompensation & Benefits\\nCompetitive medical, dental and vision insurance\\nGenerous paid time off including paid caregiver and parental leave policies\\nCompetitive matching retirement savings plans\\nWorking environment where your safety, health and wellbeing come first\\nFocus on professional and personal development through Volvo Group University\\nPrograms that make today’s challenging reality of combining work and personal life easier\\nWant to learn more about these programs? Continue your exploratory journey with us here\\n\\nAre you ready to join our team and shape the future of the transportation industry together with us?\\nAbout us\\nThe Volvo Group is one of the world’s leading manufacturers of trucks, buses, construction equipment and marine and industrial engines under the leading brands Volvo, Renault Trucks, Mack, UD Trucks, Eicher, SDLG, Terex Trucks, Prevost, Nova Bus, UD Bus and Volvo Penta.\\nVolvo Group Trucks Technology provides Volvo Group Trucks and Business Area's with state-of-the-art research, cutting-edge engineering, product planning and purchasing services, as well as aftermarket product support. With Volvo Group Trucks Technology you will be part of a global and diverse team of highly skilled professionals who work with passion, trust each other and embrace change to stay ahead. We make our customers win.\\nAuto req ID\\n142888BR\\nOrganization\\nGroup Trucks Technology\\nState / Province\\nNorth Carolina\\nCity/Town\\nGreensboro\\nEmployment/Assignment Type\\nRegular\\nTravel Required (maximum)\\nNo Travel Required\\nFunctional Area\\nTechnology\\nLast application date\\n25-Aug-2023\\nUS Disclaimer text\\nVolvo Group North America is an Equal Opportunity Employer\\n\\n\\nE.O.E./M/F/Disability/Veteran\",\n",
       "  'JobSalary': '$74K - $102K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Manufacturing',\n",
       "  'CompanyYearFounded': '1927',\n",
       "  'CompanyIndustry': 'Transportation Equipment Manufacturing',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'CGI Group, Inc.\\n3.9',\n",
       "  'JobTitle': 'Azure Data Engineer',\n",
       "  'JobLocation': 'Lafayette, LA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Azure Data Engineer\\n\\nPosition Description\\nJoin this exciting new project as a Azure Data Engineer! With your required expertise in developing and deploying application on MS Azure Databricks, you will work with a high-performing team to implement a Cloud Modernization Project. You will support and interact with the client daily in an Agile way of working. You will have the opportunity to show/grow with technology in a customer-facing role. You should be self-motivated to drive solutions and proactively create (in sufficient detail) the documentation required to support and describe technical solutions. You should be able to work independently under limited supervision and apply your knowledge of Azure-based ETL. You will work with cloud architects to establish pipelines and develop spark code using Databricks platform. You should have sufficient knowledge and maturity to effectively deal with technical issues and help to support the broader team. Your success will be directly related to CGI\\'s and our client\\'s success.\\n\\nYour future duties and responsibilities\\nThe data engineer will be responsible for development, deployment, maintenance, diagnostics and support of spark ETL jobs on the MS Databricks platform.\\nWill streamline code, rationalize datasets and tables to arrive at single source of truth\\nWill build detailed architecture diagrams for all workflows and processes\\nMigrate existing clients from the existing On-Prem solution to the Azure Solution\\n\\nRequired qualifications to be successful in this role\\n5 years of hands on experience with cloud automation and scripting in an Azure environment.\\nExperience in development of apache Spark code using PySpark/Scala\\nExperience in using Azure Databricks Platform\\nExperience in using Azure Data Factory to call Azure Databricks notebook activities\\nExperience in using Azure Storage (Blob/Data Lake Store), Databricks Delta\\nExperience in using Git based repositories\\nGood fundamental knowledge about distributed computing, RDBMS and Dimensional modeling concepts\\nExposure to Azure DevOps\\n\\nCGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skills, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $84,000 - 161,600.\\n\\nAt CGI, we call our professionals \"members\" to reinforce that all who join our organization are, as owners, empowered to take part in the challenges and rewards that come from building an outstanding company. CGI\\'s benefits include:\\nCompetitive base salaries\\nEligibility to participate in an attractive Share Purchase Plan (SPP) in which the company matches dollar-for-dollar contributions made by eligible employees, up to a maximum, for their job category\\n401(k) Plan and Profit Participation for eligible members\\nGenerous holidays, vacation, and sick leave plans\\nComprehensive insurance plans that include, among other benefits, medical, dental, vision, life, disability, out-of-county emergency coverage in all countries of employment;\\nBack-up childcare, Pet insurance, a Member Assistance Program, a 529 college savings program, a personal financial management tool, lifestyle management programs and more\\n\\nInsights you can act on\\n\\nWhile technology is at the heart of our clients\\' digital transformation, we understand that people are at the heart of business success.\\n\\nWhen you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees \"members\" because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today - one of the world\\'s largest independent providers of IT and business consulting services.\\n\\nAt CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.\\n\\nReady to become part of our success story? Join CGI - where your ideas and actions make a difference.\\n\\nQualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, or any other legally protected status or characteristics.\\n\\nCGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com . You will need to reference the requisition number of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a requisition number will not be returned.\\n\\nWe make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.\\n\\nAll CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.\\n\\nCGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI\\'s legal duty to furnish information.',\n",
       "  'JobSalary': '$94K - $126K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Management & Consulting',\n",
       "  'CompanyYearFounded': '1976',\n",
       "  'CompanyIndustry': 'Business Consulting',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'GeneDx\\n2.6',\n",
       "  'JobTitle': 'Senior Data Engineer - Data Warehouse',\n",
       "  'JobLocation': 'Gaithersburg, MD',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"GeneDx is seeking a senior data engineer for our data warehouse and business intelligence platform team, which allows our engineering teams to publish key data so that our internal analysts can guide our business decisions.\\nIn this role, you will work with other senior data engineers to create a data warehouse allowing our operations, product, and commercial teams to analyze heterogeneous data to guide our business decisions. As a senior member of our engineering team, you will be able to lead significant technical initiatives or contribute advanced functional knowledge as an individual contributor. While you will not be asked to manage a team in this role, your depth and breadth of knowledge of data engineering and other adjacent engineering skills will allow you to make strong technical designs for complex systems, and your senior leadership abilities would allow you to manage a team as required in the future. You will work easily with members of our technical and non-technical teams to craft technical implementation plans that identify and create key user features quickly and incrementally, while not sacrificing the long term value of our software systems.\\nYour experience with contemporary data engineering practices for governance and data quality will serve your colleagues in analytical roles by keeping data well-organized, discoverable, and consistent. Your strong proficiency in Python, Scala, and SQL along with your knowledge of contemporary ETL/ELT, CDC, pub-sub, and other event architectures will ensure you help our team implement cost-controlled, performant code for populating and maintaining our data warehouse. Your knowledge of general software engineering will allow you to make pull requests into other codebases in the company as required, with a quick understanding and strong technical communication to coordinate with owners of upstream systems. Your ability to contribute from a mindset of supporting your colleagues will contribute to our culture of empowerment with responsibility and leadership through service from all team members.\\nExperience, Traits, and Skills\\nProven experience designing and implementing data warehouses including data modeling, schema design, and performance optimization.\\nStrong Proficiency relevant languages (SQL, Python, Scala) for data manipulation and automation tasks.\\nSolid understanding of ETL/ELT concepts and experience with ETL tools and frameworks\\nExperience in implementing cloud-based infrastructure as code including version control and CI/CD with best practices\\nExperience with best practices for data governance, data quality, and privacy frameworks (GDPR, CCPA)\\nStrong analytical and problem-solving skills, with an attention to detail and a drive for delivering high-quality solutions with the most important features first\\nExcellent communication and collaboration skills to work effectively with cross-functional teams and stakeholders.\\nUnderstanding of business intelligence job roles and objectives, including tooling like Tableau\\nThe Responsibilities\\nDesigning and developing architecture of our data warehouse, ensuring scalability, efficiency, and reliability.\\nDesigning, building, and maintaining ETL processes and integration pipelines to efficiently extract, transform, and load data from various sources into unified warehouse.\\nCollaborate with cross-functional teams, including data scientists, analysts, and software engineers, and stakeholders to gather requirements and understand data needs.\\nOptimize data models, schemas, and SQL queries to enhance the performance and speed of data retrieval and processing.\\nImplement data quality checks, validation rules, and monitoring mechanisms to ensure the accuracy, consistency, and integrity of data stored in unified warehouse.\\nDevelop and maintain documentation, including data dictionaries, system diagrams, and technical specifications, to facilitate knowledge sharing and maintain data governance standards.\\nParticipate in code reviews, testing, and deployment activities to ensure high-quality and reliable data engineering solutions.\\nThe Objectives\\nData warehouse and business intelligence dashboard houses data from key systems of record\\nData are added quickly in an incremental, adaptable, and easily maintained way\\nAnalyst users can access reliable data and create their own analyses and dashboards with ease\\nEngineers of upstream systems can add and update data into the data warehouse with no dependency on data warehouse engineers for specific updates\\n\\nPay Transparency, Budgeted Range\\n$191,318—$229,579 USD\\n~\\nScience - Minded, Patient - Focused.\\nAt GeneDx, we create, follow, and are informed by cutting-edge science. With over 20 years of expertise in diagnosing rare disorders and diseases, and pioneering work in the identification of new disease-causing genes, our commitment to genetic disease detection, discovery, and diagnosis is based on sound science and is focused on enhancing patient care.\\nExperts in what matters most.\\nWith hundreds of genetic counselors, MD/PhD scientists, and clinical and molecular genomics specialists on staff, we are the industry's genetic testing experts and proud of it. We share the same goal as healthcare providers, patients, and families: to provide clear, accurate, and meaningful answers we all can trust.\\nSEQUENCING HAS THE POWER TO SOLVE DIAGNOSTIC CHALLENGES.\\nFrom sequencing to reporting and beyond, our technical and clinical experts are providing guidance every step of the way:\\nTECHNICAL EXPERTISE\\nHigh-quality testing: Our laboratory is CLIA certified and CAP accredited and most of our tests are also New York State approved.\\nAdvanced detection: By interrogating genes for complex variants, we can identify the underlying causes of conditions that may otherwise be missed.\\nCLINICAL EXPERTISE\\nThorough analysis: We classify variants according to our custom adaptation of the most recent guidelines. We then leverage our rich internal database for additional interpretation evidence.\\nCustomized care: Our experts review all test results and write reports in a clear, concise, and personalized way. We also include information for research studies in specific clinical situations.\\nImpactful discovery: Our researchers continue working to find answers even after testing is complete. Through both internal research efforts and global collaborations, we have identified and published hundreds of new disease-gene relationships and developed novel tools for genomic data analysis. These efforts ultimately deliver more diagnostic findings to individuals.\\nLearn more About Us here.\\n\\n~\\nEssential on-site and customer facing employees may be required to provide proof of COVID-19 vaccinations. Medical or religious exemptions considered.\\nBenefits include:\\nPaid Time Off (PTO)\\nHealth, Dental, Vision and Life insurance\\n401k Retirement Savings Plan\\nEmployee Discounts\\nVoluntary benefits\\nGeneDx is an Equal Opportunity Employer.\\n\\nAll privacy policy information can be found here.\",\n",
       "  'JobSalary': 'Employer Provided Salary:$191K - $230K',\n",
       "  'CompanyRating': '2.6',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Biotech & Pharmaceuticals',\n",
       "  'CompanySector': '$5 to $25 million (USD)',\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': 'Pharmaceutical & Biotechnology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'ALTA IT Services\\n4.4',\n",
       "  'JobTitle': 'Data Engineer (Sr. and Mid)',\n",
       "  'JobLocation': 'Alexandria, VA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Title: Sr. & Mid Data Engineer ( w/ active Secret clearance) Location: hybrid in office / remote: 2-3 days a week on-site in DC metro area ( Arlington / DC) Security clearance needed: TS/SCI Compensation range: Open, based on extent of position-relevant experience *** FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly: TEXT: ( 240-601-8546) OR CALL: ( 301-212-7355) ALTA IT Services is seeking a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform. Responsibilities As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members. Required Skills: Clearance: Secret 8+ years of experience with SQL 8+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets. 8+ years of experience with a modern programming language such as Python or Java 8 years of experience working in a big data and cloud environment Experience with distributed computer understanding and experience with SQL, Spark, ETL. Documented experience with AWS, EC2, S3, and/or RDS Preferred Skills: 4 years of experience working in an agile development environment Ability to quickly learn technical concepts and communicate with multiple functional groups Ability to display a positive, can-do attitude to solve the challenges of tomorrow Possession of excellent verbal and written communication skills Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly: TEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\\nTitle: Sr. & Mid Data Engineer ( w/ active Secret clearance)\\nLocation: hybrid in office / remote: 2-3 days a week on-site in DC metro area ( Arlington / DC)\\nSecurity clearance needed: TS/SCI\\nCompensation range: Open, based on extent of position-relevant experience\\n\\n*** FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly:\\nTEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\\n\\nALTA IT Services is seeking a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform.\\nResponsibilities\\nAs a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.\\nRequired Skills:\\nClearance: Secret\\n8+ years of experience with SQL\\n8+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.\\n8+ years of experience with a modern programming language such as Python or Java\\n8 years of experience working in a big data and cloud environment\\nExperience with distributed computer understanding and experience with SQL, Spark, ETL.\\nDocumented experience with AWS, EC2, S3, and/or RDS\\nPreferred Skills:\\n4 years of experience working in an agile development environment\\nAbility to quickly learn technical concepts and communicate with multiple functional groups\\nAbility to display a positive, can-do attitude to solve the challenges of tomorrow\\nPossession of excellent verbal and written communication skills\\nPreferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes\\n\\nFOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly:\\nTEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\",\n",
       "  'JobSalary': '$76K - $116K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': \"Moody's\\n3.8\",\n",
       "  'JobTitle': 'Data Engineer - (Python, SQL, ETL, AWS) 100% Remote',\n",
       "  'JobLocation': 'Lebanon, KS',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"We are seeking an exceptionally skilled and highly effective Data Engineering to join our Digital Insights team. They will be player-coach technologist that will contribute towards the architecture, development, and maintenance of the next generation content delivery platform for moodys.com.\\nAs part of the Digital Insights team, they will be supporting one or more engineering teams, lead technology transformation and accelerate the journey towards the highly-available, scalable Cloud native technologies.\\nThis key member of our team is integral to the entire data engineering practice, including analysis, requirement specification, use case and technical design, development, testing, and implementation. This individual will be focused on delivering assigned tasks and/or specific tracks of the solution on time, escalating issues when appropriate as well as educating and leading others in the form of code reviews, workshops, and documentation.\\n\\nThe Work:\\nHelp the larger Digital Insights team in defining solutions, integrations, and technical architectures\\nAssist with the estimation of development tasks, project planning and roadmap initiatives, risk identification and mitigation planning\\nParticipate in the software design, development, and implementation of data pipelines, data processing and ETL operations, data access services alongside product development and application development teams supporting both large structured and unstructured data sets\\nSupport and contribute to the continuous improvement, tuning, applications, infrastructure developments, process controls, and upgrades of the data systems\\nProvide guidance, hands-on development and operational support for the deployment of database scripts and changes across multiple environments\\nCollaborate with Moody's technical teams and business owners as needed during the design and implementation\\nManage individual time and tasks as well as guide and mentor junior team members\\nBachelor's degree or equivalent, Master's is a plus - or equivalent experience\\n3+ year of experience contributing to and providing technical leadership in a data engineering or software development team\\nExperience within all phases of software development working across Agile teams, product owners and external stakeholders\\nExperience driving technical ideas and communicating clearly to both technical and non-technical audiences at all levels of the organization\\nStrong development, testing, debugging skills at all levels (unit, system, integration, and performance testing) along with detail-oriented documentation skills\\nEffective communication and problem-solving skills.\\nHands on experience implementing, managing, supporting, and developing services using NodeJS with AWS database technologies, AWS RDS (Microsoft SQL Server, PostgreSQL) and AWS DynamoDB\\nStrong database engineering skills with emphasis in MongoDB and AWS DynamoDB\\nExperience working with big data technologies: Apache Spark, Apache Kafka, AWS Kinesis, AWS Redshift, Snowflake Data Cloud, Apache Airflow\\nExperience working with search technologies: Elastic, AWS OpenSearch, Apache Solr\\n\\nThe Bonus Points:\\nExperience with development languages: Python, Java (Spring Boot), Scala\\nStrong understanding of object-oriented programming, design, and architectural patterns\\nExperience working with website/product data and analytics\\nExperience with development technologies: Git (Bitbucket or GitHub), Jira, Rally, Asana, Selenium, SonarQube, maven, Jenkins, Cypress, Postman/Newman\\nExperience with AWS cloud technologies: ECS/Fargate, ECR (EC2 Container Registry), Lambda, DynamoDB, API (Application Programming Interface) Gateway, VPC (Virtual Private Cloud), ALB, NLB, Elasticsearch, Neptune, Elasticache, Aurora PostgreSQL\\nExperience with container technologies: Docker, Kubernetes, AWS ECS (EC2 Container Service)\\nWorking knowledge of other web technologies: Content Delivery Networks (CDN) (Akamai, CloudFlare), Web Application Firewall (WAF), Nginx, HAProxy, Apache Httpd\\nIn Digital Insights, we leverage rich content and workflow capabilities to comprehensively evaluate risk and support better decisions. Our flagship platform CreditView incorporates credit ratings, research and Moody’s data. It is an essential tool that helps our clients in the professional services, commercial and financial industries to conduct fundamental credit analysis. Our diverse team is made up of marketing, technology, product strategy and customer experience experts.\\nMoody’s is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody’s also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.\\n\\nFor San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.\\n\\nThis position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.\\n\\nClick here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement.\\nMoody’s is a developmental culture where we value candidates who are willing to grow. So, if you are excited about this opportunity but don’t meet every single requirement, please apply! You may be a perfect fit for this role or other open roles.\\n\\nMoody's is a global integrated risk assessment firm that empowers organizations to make better decisions.\\n\\nAt Moody’s, we’re taking action. We’re hiring diverse talent and providing underrepresented groups with equitable opportunities in their careers. We’re educating, empowering and elevating our people, and creating a workplace where each person can be their true selves, reach their full potential and thrive on every level. Learn more about our DE&I initiatives, employee development programs and view our annual DE&I Report at moodys.com/diversity\\nFor US-based roles only: the anticipated hiring base salary range for this position is $130,800 to $229,750, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody’s also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.\",\n",
       "  'JobSalary': 'Employer Provided Salary:$131K - $230K',\n",
       "  'CompanyRating': '3.8',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Management & Consulting',\n",
       "  'CompanyYearFounded': '1900',\n",
       "  'CompanyIndustry': 'Research & Development',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Intellibee\\n4.5',\n",
       "  'JobTitle': 'AWS DATA ENGINEER',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'This position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\n\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\n\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\nRESPONSIBILITIES\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\nREQUIREMENTS\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\nSkill Matrix\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field. Required\\nProfessional experience as a Data Engineer, with a focus on AWS data services and technologies. Required 5 Years\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies. Required 5 Years\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing a Required 5 Years\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB. Required 5 Years\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices. Required 5 Years\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus. Desired\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle. Required 5 Years\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions. Required\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment Required',\n",
       "  'JobSalary': '$72K - $99K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Wells Fargo\\n3.7',\n",
       "  'JobTitle': 'Fraud Data Engineer',\n",
       "  'JobLocation': 'Chandler, AZ',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"About this role:\\n\\nWe are looking for a Data Engineer to Join the Fraud AI Platform team. The Fraud AI Platform team is a newly created team at the bank that is building a new Data Science platform to solve real world fraud problems and help reduce our fraud loss year over year. Members of this team will be coming in at the ground floor, with the ability to help drive the vision, design, and implementation of the new platform. Team members will have the freedom to innovate and deliver real capabilities back to our bank, saving our customers time and money as we reduce fraudulent activity. Furthermore, this team is working on the cutting edge of data science, making it a very exciting environment to continue to build out a career with many opportunities for excellence and recognition.\\n\\nThe Data engineer will be responsible for working with our various data platforms, understanding supporting a team of data scientists and analysts. The Data engineer will work on data modeling in a graph database, support ETL on data ingestion, serve as a consultant to data scientists on data issues. The individual will be working across a data intake team in the fraud platform.\\n\\nIn this role, you will:\\nLead moderately complex initiatives within Technology and contribute to large scale data processing framework initiatives related to enterprise strategy deliverables\\nBuild and maintain optimized and highly available data pipelines that facilitate deeper analysis and reporting\\nReview and analyze moderately complex business, operational or technical challenges that require an in-depth evaluation of variable factors\\nOversee the data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis\\nResolve moderately complex issues and lead teams to meet data engineering deliverables while leveraging solid understanding of data information policies, procedures and compliance requirements\\nCollaborate and consult with colleagues and managers to resolve data engineering issues and achieve strategic goals\\nRequired Qualifications, US:\\n1+ Years of experience with Fraud data supporting FI Fraud, Cyber, or AML use cases\\n4+ Years using scripting language such as python, javascript, or shell scripting to process, clean, and prepare data\\n3+ years with ingesting and extracting data from large data store\\n2+ years with SQL\\n4+ years of Data Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\\nRequired Qualifications, International:\\nExperience in Data Engineering, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\\nDesired Qualifications:\\n1+ Year working with a graph database such as neo4J, ArangoDB, Tigergraph\\nExperience working with Apache Spark\\nExperience with Elasticsearch\\nWe Value Diversity\\n\\nAt Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.\\n\\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.\\n\\nCandidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\\n\\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\\n\\nDrug and Alcohol Policy\\n\\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1852',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Octo\\n4.1',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Boston, MA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"You…\\nAs a Data Engineer , you will provide expertise on all data concepts for the broader advanced analytics group, and inspire the adoption of advanced analytics, data engineering and data science across the organization. This will include Installing continuous pipelines of large pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.\\nUs…\\nWe were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.\\nMission\\nStrengthen and accelerate OCTO vision for a multi-tenant, highly-available, distributed, resilient, governed, and MTW capable cloud data platform.\\nVision\\nTo provide a single, consistent and persistent copy of data with Multi-Tenancy, releasability, criticality, and classification.\\nObjectives and Scope\\nEstablish a centralized data organization under OCTO to provide enterprise data strategy, data services, data operations, and data resources to all Kessel Run product lines including OPSC2, WingC2, and ADCP.\\nWhat we’d like to see…\\nFamiliarity with the manipulation of unstructured data in a data analytics environment, and the use of open-source tools, cloud computing, machine learning and data visualization.\\nFamiliar with specialized languages relevant to the technologies employed such as Apache, Hadoop, etc…\\nStrong written and oral communication skills\\nYears of Experience: 7+ years of experience in the data engineering field, at least three of which must have been in a data analytics environment preferably in DoD or the intelligence community.\\nEducation: Bachelor's degree in the requisite relevant field. A Master's degree in a relevant field may be substituted for 3 years of general experience.\\nLocation: Boston, MA or Hanscom AFB, MA\\nClearance: DoD Secret or higher\",\n",
       "  'JobSalary': '$88K - $119K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2006',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'Inside Real Estate\\n3.7',\n",
       "  'JobTitle': 'Business Intelligence Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Inside Real Estate is a fast-growing, profitable, privately-held company and a technology leader in the real estate industry. In an industry where fluctuation is often the norm, we don’t merely adapt. We create. We innovate. We lead. We discover solutions to new challenges and make something remarkable. We are on a mission to simplify workflows for Real Estate companies around the country, and we are looking to add to our team.\\n\\nIdeal candidates for the Data Engineer position at Inside Real Estate thrive in a fast paced data informed environment, are inquisitive in understanding the “why” and can proactively seek out opportunities for continuous improvement across broad organizational goals.\\n\\nData Engineer Responsibilities:\\nAssemble large, complex sets of data to meet functional business requirements.\\nIdentify, design and implement data collection and aggregation for greater scalability, optimizing data delivery, and automating manual processes.\\nWork with various stakeholders throughout the company to support their data infrastructure needs and assist them with data-related technical issues.\\nWork with other members of the data engineering team to model data structures, architect and execute data orchestration, and extract/transform/load (ETL) procedures.\\nAssist analyst team with the development of Tableau dashboards and ad hoc data analysis.\\nMonitor ongoing ETL and data orchestration processes.\\nCollaborate with cross-functional leadership to help teams achieve goals through data informed decision making.\\nOther data engineering projects as identified by your manager.\\nRequirements:\\nBachelor's degree in Business, Statistics, Computer Science, or other related technical or project management related major with 2+ years’ experience in a related role OR a minimum of 5 years’ experience in a related role.\\nDeep expertise with data warehousing languages Python and SQL data orchestration tools like Apache Airflow or equivalent technologies.\\n3+ years experience leveraging AWS technologies (Redshift, SQS/SNS, Cloud formation, RDS, Glue, S3, EC2) in a big data and decision support environment.\\nDirect experience in managing large data sets from a variety of SaaS business solutions and integrating into an environment to facilitate reporting.\\nFamiliarity with other SaaS business solutions, including at least one or more of: Intercom, Chargify, Netsuite, Salesforce, Zapier, Amplitude, Segment, Outreach, HubSpot.\\nAble to manage a project, including generating, explaining, and presenting detailed project metrics, schedules, milestone objectives, status reports, and other documentation clearly and accurately.\\nExcellent analytical skills associated with working on unstructured datasets.\\nStrong written and verbal communication skills; able to collaborate with a variety of stakeholders including senior leadership and cross-functional teams.\\nExperience with data streams from Kafka or other similar technologies is a plus.\\nGeneral understanding of the residential real estate industry.\\nAt Inside Real Estate enjoy:\\nSmall company feel, with big company growth, support, and stability\\nCompetitive compensation, bonus opportunities, and great benefits including Medical, Dental, Vision, HSA, FSA, 401k Retirement Savings, voluntary benefits, paid time off and paid holidays\\n401K Employer Matching\\nCompany-paid Parental Leave\\nA focus on driving top results in a fun environment\\nOpportunities to grow within our company\\nPotential to work in a remote setting\\nExciting/energetic work environment and fun, creative culture\\n\\nEEO - We believe that the unique contributions of all Insiders are the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experiences we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, marital, veteran, or disability status.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2006',\n",
       "  'CompanyIndustry': 'Computer Hardware Development',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': 'Optimal Inc.\\n3.6',\n",
       "  'JobTitle': 'Data Engineer-GCP',\n",
       "  'JobLocation': 'Dearborn, MI',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Position Description:\\nWe are seeking a talented MLOps Engineer to join our Quality Analytics team within the Global Data Insight and Analytics (GDIA) organization at our Company. As an MLOps Engineer, you will play a critical role in the migration of existing NLP pipelines to the Google Cloud Platform (GCP), as well as collaborating with the modeling team to bring new models from proof of concept (PoC) to production. Additionally, you will be responsible for creating a robust back-end infrastructure to deploy our NLP models as APIs.\\nResponsibilities:\\nMigrate existing NLP pipelines to the Google Cloud Platform (GCP) to leverage its powerful capabilities and scalability.\\nCollaborate with the modeling team to operationalize and deploy new machine learning models from proof of concept (PoC) to production, ensuring their seamless integration into our existing infrastructure.\\nDevelop and maintain a scalable and reliable back-end infrastructure to deploy our NLP models as APIs, enabling easy consumption by downstream applications.\\nSkills Required:\\nMinimum of 1 year of hands-on experience with the Google Cloud Platform (GCP), including proficiency in deploying and managing services.\\nAt least 2 years of professional experience in Python programming, with a strong understanding of software engineering best practices and version control.\\nSkills Preferred:\\nPractical experience working with Kubernetes including deploying and managing containerized applications.\\nProficiency in designing, implementing, and maintaining ETL pipelines, with a deep understanding of data processing frameworks and tools.\\nExperience Required:\\nKnowledge and experience with Language Models and NLP techniques, including pre-trained models like LLMs (Large Language Models).\\nExperience Preferred:\\nPrevious experience working in a large, data-driven organization, navigating complex data ecosystems, and collaborating with cross-functional teams.\\nEducation Required:\\nBachelor's degree in computer science, Data Science, Statistics, or a related field.\\nEducation Preferred:\\nMaster's degree in computer science, Data Science, Statistics, or a related field.\\nAdditional Information:\\nExcellent communication and interpersonal skills, with the ability to effectively collaborate with team members, stakeholders, and other business units.\",\n",
       "  'JobSalary': '$74K - $101K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Nonprofit Organization',\n",
       "  'CompanySector': 'Education',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Education & Training Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Locus Recruiting\\n4.0',\n",
       "  'JobTitle': 'Data Center Engineer (Structured Cabling)',\n",
       "  'JobLocation': 'United States',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Locus (Recruitment Firm) is recruiting for a Data Center (Structured Cabling) Engineers for our client.\\nPosition: Data Center (Structured Cabling) Engineer\\nDuration: 6 month contract to hire (Full-time work)\\nPay: $50-60/hr. ($100-$125,000 conversion)\\nLocation: United states\\nOvernight Shift (11pm-7am)\\nTravel 75% + each week (leave Sunday evening-return Thursday or Friday)\\nExpenses Reimbursed (Hotel, Meals, etc.)\\n40 hours a week\\nHave a need to hire 6 individuals-2 Western US, 2 Central US and 2 Eastern US\\nSkills:\\n· 3rd Shift to cover Datacenters and Critical facilities at airports while flights are not flying.\\n· Doing remediation work\\n· Following a run book\\n· Taking 12 largest airports, re-cabling and moving racks forward, putting in AC, separate power\\n· Worked in Data centers, rack and stack at airports\\n· Low voltage electrician backgrounds work is great, fibers/cabling experience\\nJob Types: Full-time, Contract\\nPay: $50.00 - $60.00 per hour\\nBenefits:\\nDental insurance\\nHealth insurance\\nVision insurance\\nSchedule:\\n8 hour shift\\nEvening shift\\nMonday to Friday\\nNight shift\\nWeekends as needed\\nApplication Question(s):\\nAre you open to a 6-month contract-to-hire position working 40 hours a week?\\nAre you comfortable traveling Sunday-Thursday each week?\\nAre you comfortable working 11pm-7am?\\nAre the pay requirements in line with what you are looking for?\\nDo you have any experience working within the Airline Industry?\\nExperience:\\nData center: 5 years (Preferred)\\nRack and Stack: 5 years (Preferred)\\nCabling: 5 years (Preferred)\\nLow Voltage: 5 years (Preferred)\\nShift availability:\\nOvernight Shift (Required)\\nWillingness to travel:\\n75% (Required)\\nWork Location: On the road',\n",
       "  'JobSalary': 'Employer Provided Salary:$50.00 - $60.00 Per Hour',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Enterprise Software & Network Solutions',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'HCL America, Inc.\\n3.7',\n",
       "  'JobTitle': 'Data Center Engineer',\n",
       "  'JobLocation': 'Washington, DC',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Data Center Asset Management\\n· Perform data center asset management within the framework and guidelines of the World Bank Group’s IT asset management structure.\\n· Periodically audit and direct the performance of audits of all data center IT assets per the guidelines as required.\\nData Center Capacity Planning\\n· Support capacity planning and coordinate resource allocation.\\n· Provide capacity management reports on a regular basis.\\nMiscellaneous Data Center Support\\n· Utilize knowledge of network topology, redundant power systems, structured cabling, HVAC systems relating to rack cooling, data center equipment enclosure requirements, and the physical data center layouts to perform and oversee equipment placement and planning for multiple data centers.\\n· Build and maintain databases for use in generation of regular and ad-hoc queries, forms and reports.\\nMohammad Mudassir.\\nmohammad.mudassir@hcltechnologies.com\\nph:(984) 208-5879\\nHCLTech\\nSupercharging Progress™\\nhcltech.com\\nhcltech.com\\nJob Type: Full-time\\nPay: $60,000.00 - $80,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nSupplemental pay types:\\nBonus pay\\nAbility to commute/relocate:\\nWashington, DC 20001: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nComputer networking: 1 year (Preferred)\\nLAN: 1 year (Preferred)\\nSecurity clearance:\\nConfidential (Preferred)\\nWork Location: In person',\n",
       "  'JobSalary': 'Employer Provided Salary:$60K - $80K',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1991',\n",
       "  'CompanyIndustry': 'Computer Hardware Development',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Delaware Nation Industries\\n4.8',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Aberdeen Proving Ground, MD',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Overview:\\nDNI Emerging Technologies is seeking a qualified Data Engineer candidate to support the Army Systems Engineering and Program Management Services (Army SEPM).\\n\\nThe Program Executive Office, Command, Control and Communications Tactical (PEO C3T) is responsible for all facets of developing, fielding and sustaining Command, Control, Communications, Cyber, Computers, Intelligence, Surveillance, and Reconnaissance (C5ISR) Systems, and for the acquisition, development, and integration of secure tactical communications for the digitized battlefield.\\n\\nThis is a full time/permanent position with excellent benefits and outstanding compensation including full medical, dental, 401k, vacation, and holiday pay.\\nResponsibilities:\\nBuild the infrastructure required for transformation and loading of data from a wide variety of data sources using “big data” data pipelines, architectures and data sets.\\nBuild analytic tools utilizing the data pipeline to provide actionable insights for operational efficiency.\\nWork as a liaison supporting internal and external stakeholders to design, build and resolve data related technical issues and support data infrastructure needs.\\nWork with relational databases and possess strong analytic skills related to working with unstructured datasets and integrate into the tactical environment.\\nMust be able to lift and carry up to 25 pounds with or without accommodation.\\nOther relevant duties as assigned.\\nQualifications:\\nBachelor’s Degree in one of the following: Computer Science, Information Systems, Engineering or a related scientific or technical discipline.\\nTen (10) years of experience in data engineering supporting DOD programs.\\nActive Secret Security Clearance.\\n\\nAAP/EEO Statement:\\nDNI complies with all federal, state, and local laws designed to protect employees and job applicants from discrimination based on race, religion, color, sex, parental status, national origin, age, disability, genetic information, military service, or other non-merit-based factors.',\n",
       "  'JobSalary': '$87K - $128K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.8',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'National Agencies',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Contract',\n",
       "  'CompanyIndustry': 'Government & Public Administration',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Bamboo Health\\n3.5',\n",
       "  'JobTitle': 'Sr. Data Integration Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We are driven by our mission of enabling better care for patients across the continuum. Our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. Join us in improving healthcare for all!\\nSummary:\\nWe are actively hiring a full-time Sr. Data Integration Engineer to focus on supporting and extending our data platform. Bamboo Health receives HL7 data from hospitals, EHRs and HIEs around the country and this role will be responsible for integrating new HL7 EHR senders to the data pipeline using in-house tools, scripts, and custom applications. The ideal candidate will work well in a team, have a data-first mentality, and thrive in customer-facing projects.\\nWhat You Will Do:\\nPartner with Operations to ensure on-boarding HL7 integrations meet target deadlines through task resolution in a timely and organized manner\\nPartner with broader Platform Engineering team on cross-functional initiatives focused on infrastructure scalability and stability.\\nPartner with Software Engineers focused on improving our data pipeline\\nDesign and execute HL7 test plans for on-boarding new integrations\\nBuild a standard integration process to receive data and post events to new EHR systems\\nWork with ADT senders to resolve customer issues and maintain high quality interfaces\\nOn-board and track standard HL7 integrations\\nTriage customer issues related to HL7 integrations\\n\\nWhat Success Looks Like…\\nIn 3 months…\\nExecute:\\nDevelop solid understanding of Bamboo Health onboarding process for Technical Implementation Services\\nContribute to HL7 data validation, mapping, and testing processes\\nDevelop an understanding of our HL7 data pipeline\\nBuild relationships across the broader Product Platform organization\\n\\nIn 6 months…Manage:\\nWork with our Product, Operations and Network Operations Center teams to drive streamlined data processing and continuous improvement initiatives.\\nContribute to the development and reporting of data quality metrics\\n\\nIn 12 months…Scale:\\nDevelop a comprehensive knowledge of our data ingestion architecture\\nManage complex customer integrations with a heavy focus on service and quality outcomes\\n\\nWhat You Need:\\n5+ years professional experience in or around software development\\nExperience in or around the Healthcare domain\\nExperience in at least one modern language such as Java, Python, JavaScript\\nProficient in SQL\\nWillingness to learn healthcare data exchange formats\\nAbility to self-start project tasks and communicate progress clearly\\nAbility to work autonomously on multiple concurrent projects and prioritize appropriately\\nExperience organizing and delivering on several lines of work with clear communication on progress\\nDesire to work in a fast-paced collaborative environment\\nA work environment that is conducive to high quality virtual interactions. This includes but is not limited to being able to work from a quiet space with minimal interruptions or distractions, and a strong internet connection.\\n\\nHelpful/Preferred Experience:\\nHealthcare data integration tools (Mirth preferred)\\nCloud-native AWS solutions\\nSDLC – Git, pull requests\\nDocker & Kubernetes\\nAtlassian product suite\\nSumoLogic, Prometheus/Grafana, or equivalent\\n\\nWhat You Get:\\nJoin one of the most innovative healthcare technology companies in the country.\\nHave the autonomy to build something with an enthusiastically supportive team.\\nLearn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors.\\nReceive competitive compensation, including equity, with health, dental, vision and other benefits.\\n\\nBamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\\n#LI-Remote',\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.5',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1994',\n",
       "  'CompanyIndustry': 'Enterprise Software & Network Solutions',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'Sr. Software Development Engineer in Test (Data Focused)',\n",
       "  'JobLocation': 'Austin, TX',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.\\nJob Overview:\\nWe are seeking a Senior Quality Assurance Engineer. Our ideal candidate will be responsible for designing and executing tests manually (15%) and automatically (85%) to identify and resolve all data related issues to meet quality standards. Our tight-knit team combines a Scrum process with a strong testing culture.\\nResponsibilities:\\nBuild and perform End to End QA effort on Data Warehouse/ETL/BI Platforms\\nReview requirements, specifications and technical design documents to provide timely and meaningful feedback\\nUnderstand data flow and test strategy for ETL , Data warehouse and Business Intelligence testing\\nETL testing of mapping, transformations and data pipelines.\\nData warehouse testing involving the testing of stored procedures, SSIS packages, slowly changing dimension tables, Change Data capture etc.\\nBusiness Intelligence testing involving the validation of DataMart, ODS, Data models and SSRS reports.\\nCreate detailed, comprehensive and well-structured test plans and test cases\\nIdentify, document and track software defects\\nWork closely with development teams to perform root cause analysis of issues\\nManage multiple tasks and adjust to shifting priorities, as necessary\\nWrite advanced SQL queries for ETL/data warehousing/Business Intelligence testing\\nIdentify test cases which has to be automated. Prepare test automation strategy.\\nDesign and develop test automation for backend.\\nWork with the team to continually improve test processes and practices based on inspection/adaption of previous iterations and to ensure adherence to process, tools and metrics standards within the project team\\nShow initiatives and accountability with strong time management skills with project teams.\\nExhibit strong collaboration/interpersonal skills\\nBe a self-starter and possess the ability to research issues and improve processes\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nBachelor’s Degree, Computer Science or similar, plus 6+ years of experience OR Master’s Degree, Computer Science or similar, plus 5+ years of experience OR no degree, plus 10+ years of experience working as a Developer or QA Engineer or SDET working in Agile scrum teams\\n5 –10 years of experience with Software Quality Assurance- QA project life cycle, test plan, test strategies, test scenarios, test cases, traceability matrix.\\n3 - 5+ years of experience in Database/Data Warehouse/ETL Testing.\\n3 - 5+ years of Strong experience in advanced SQL scripting.\\nExperience in MS SQL server, SSIS and SSRS.\\n3+ years’ experience of Data Lake/Hadoop platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Hadoop/Spark implementations.\\nCore Competencies:\\nExperience in ETL testing techniques\\nGood understanding of Data Warehousing and business intelligence concepts and testing techniques\\nExperience in Working with Star Schema, ODS, multi-dimensional models, slowly changing dimensions\\nExperience in working with ETL framework, Change Data capture, DataMart, Data models etc.\\nWorking knowledge of test automation for backend using Java and associated frameworks\\nExperience in continuous testing practices in a CI/CD development pipeline, and deploying test automation\\nSolid experience in strategizing and planning all testing activities including automation.\\nAbility to review and analyze business requirements in order to produce test strategy and test cases.\\nExpertise working in Cloud data environment\\nFamiliarity with one or more SQL-on-Hadoop technology (Hive, Impala, Spark SQL, Presto)\\nExperience in Agile projects (Scrum, Kanban etc.).\\nWorking Knowledge in Test Management software (JIRA,qTest).\\nSolid experience with Defect Management Process.\\nQuick learner and self-starter who requires minimal supervision to excel in a dynamic environment.\\nExcellent analytical and problem solving skills.\\nStrong verbal and written communications skills.\\nExperience in Banking or Financial services domain is preferred.\\nExperience in working with financial platforms\\nExperience in Cluster, Containers/VMs, On Premise, On Cloud\\nStrong knowledge on Data Lake, Azure, ELT & ETL, Data Warehouse, BI, Data Factory Tools\\nStrong in SQL preparation in Oracle/SQL Server/NoSQL/RDMS/Big Data\\nPreferences:\\nAwareness of Agile QA Software development life cycle –backlog, sprints, standups, burndowns.\\n\\nPay Range:\\n$90,080-$135,120/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$90K - $135K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Stress Engineering Services, Inc.\\n2.9',\n",
       "  'JobTitle': 'Data Engineer',\n",
       "  'JobLocation': 'Cincinnati, OH',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Overview:\\nStress Engineering Services, Inc. is the premier provider of engineered solutions and professional consulting services across a wide range of engineering disciplines. Since 1972, our clients have benefited from our special in-depth technical knowledge and proven performance in failure analysis, solid mechanics and structural design, reliability and predictive engineering, instrumentation and testing, heat transfer, fluid mechanics, floating systems, forensics, and material science. From the ground up, Stress Engineering Services, Inc. is a culture defined by innovation, creativity and teamwork tackling and solving industry’s most difficult challenges.\\n\\nNote: Only shortlisted candidates will be contacted. We thank you in advance for your interest in joining our team at Stress Engineering Solutions\\nResponsibilities:\\nAs a data engineer, you will play a crucial role in designing, developing, and maintaining the data infrastructure and pipelines that power our commercial solutions and our customer work. You will work closely with data scientists, analysts, and other stakeholders to ensure the availability, reliability, and efficiency of data systems.\\nYour responsibilities will include:\\nDesigning, building, and optimizing data pipelines and workflows to ingest, transform, and load large volumes of structured and unstructured data from various sources.\\nDeveloping and maintaining scalable data storage solutions, including databases, data lakes, and data warehouses.\\nImplementing data governance and security measures to ensure data integrity and compliance with regulatory requirements.\\nCollaborating with cross-functional teams to identify and address data engineering needs, such as data cleansing, normalization, and enrichment.\\nTroubleshooting and resolving data-related issues and performance bottlenecks in a timely manner.\\nConducting data modeling and schema design to support data analytics and reporting requirements.\\nImplementing data quality monitoring and ensuring data accuracy and consistency across systems.\\nStaying up to date with emerging technologies and industry trends in data engineering and recommending relevant tools and techniques for continuous improvement.\\nQualifications:\\nBachelors with master’s degree preferred in Computer Science, Engineering, or related field.\\n5 years of experience as a data engineer or in a similar role.\\nStrong proficiency in programming languages such as Python, Java, or Scala.\\nHands-on experience with distributed data processing frameworks, such as Apache Spark or Hadoop.\\nProficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra).\\nSolid understanding of data modeling concepts and techniques.\\nExperience with cloud platforms such as AWS, Azure, GCP\\nFamiliarity with data integration tools and ETL (Extract, Transform, Load) processes.\\nKnowledge of data warehousing concepts and technologies (e.g., Snowflake, Redshift).\\nStrong problem-solving skills and the ability to work in a fast-paced, collaborative environment.\\nExcellent communication skills to effectively collaborate with cross-functional teams and present complex ideas.\\nPreferred Skills:\\nExperience with real-time streaming technologies (e.g., Apache Kafka, Apache Flink).\\nFamiliarity with data orchestration and workflow management tools (e.g., Airflow, Luigi).\\nKnowledge of containerization technologies such as Docker and container orchestration tools like Kubernetes.\\nUnderstanding of machine learning concepts and frameworks (e.g., TensorFlow, PyTorch).\\nUnderstanding of Source Control and Dev Ops Tools (e.g. Git, ADO)\\nWorked in Agile software teams and help develop best practices for the organization',\n",
       "  'JobSalary': '$73K - $101K (Glassdoor est.)',\n",
       "  'CompanyRating': '2.9',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Private Practice / Firm',\n",
       "  'CompanySector': 'Construction, Repair & Maintenance Services',\n",
       "  'CompanyYearFounded': '1972',\n",
       "  'CompanyIndustry': 'Architectural & Engineering Services',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'American Airlines\\n3.8',\n",
       "  'JobTitle': 'Data Engineer/Senior Data Engineer, IT Analytics',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Location: DFW Headquarters Building 8 (DFW-SV08)\\nAdditional Locations: None\\nRequisition ID: 67295\\nIntro\\nAre you ready to explore a world of possibilities, both at work and during your time off? Join our American Airlines family, and you’ll travel the world, grow your expertise and become the best version of you. As you embark on a new journey, you’ll tackle challenges with flexibility and grace, learning new skills and advancing your career while having the time of your life. Feel free to enrich both your personal and work life and hop on board!\\nWhy you'll love this job\\nYou will help enable data engineering solutions at AA\\nYou will be part of a team that innovates.\\nThis role is a part of the Revenue & Planning Technology team within our Technology group. You’ll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective for American Airlines.\\nWhat you'll do\\nAs noted above, this list is intended to reflect the current job but there may be additional essential functions (and certainly non-essential job functions) that are not referenced. Management will modify the job or require other tasks be performed whenever it is deemed appropriate to do so, observing, of course, any legal obligations including any collective bargaining obligations.\\nWork closely with source data application teams and product owners to design, implement and support analytics solutions that provide insights to make better decisions\\nImplement data migration and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools.\\nPerform multiple aspects involved in the development lifecycle – design, cloud engineering (Infrastructure, network, security, and administration), ingestion, preparation, data modeling, testing, CICD pipelines, performance tuning, deployments, consumption, BI, alerting, prod support.\\nProvide technical leadership and collaborate within a team environment as well as work independently.\\nBe a part of a DevOps team that completely owns and supports their product\\nImplement batch and streaming data pipelines using cloud technologies\\nLeads development of coding standards, best practices and privacy and security guidelines.\\nMentors others on technical and domain skills to create multi-functional teams\\nAll you'll need for success\\nMinimum Qualifications- Education & Prior Job Experience\\nBachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training\\n3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions\\n3 years data analytics experience using SQL\\n2 years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI.\\nCombination of Development, Administration & Support experience in several of the following tools/platforms required:\\nScripting: Python, Spark, Unix, SQL\\nData Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake\\nAzure Data Explorer. Administration skills a plus\\nAzure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions\\nCI/CD: GitHub, Jenkins, Azure DevOps, Terraform\\nBI Analytics Tool Stack - Cognos, Tableau, Power BI, Alteryx, Denodo, and Grafana\\nData Warehousing: DataStage, Informatica\\nData Governance and Privacy: Informatica Axon and EDC, BigID\\nPreferred Qualifications- Education & Prior Job Experience\\n5+ years software solution development using agile, dev ops, product model that includes designing, developing, and implementing large-scale applications or data engineering solutions.\\n5+ years data analytics experience using SQL\\n3+ years full-stack development experience, preferably in Azure\\n3+ years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Functions, ADX, ASA, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI.\\nAirline Industry Experience\\nSkills, Licenses & Certifications\\nExpertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers.\\nExpertise in providing practical direction within the Azure Native cloud services.\\nAzure Development Track Certification (preferred)\\nSpark Certification (preferred)\\nWhat you'll get\\nFeel free to take advantage of all that American Airlines has to offer:\\nTravel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network.\\nHealth Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more.\\nWellness Programs: We want you to be the best version of yourself – that’s why our wellness programs provide you with all the right tools, resources and support you need.\\n401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year.\\nAdditional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more\\nFeel free to be yourself at American\\nFrom the team members we hire to the customers we serve, inclusion and diversity are the foundation of the dynamic workforce at American Airlines. Our 20+ Employee Business Resource Groups are focused on connecting our team members to our customers, suppliers, communities and shareholders, helping team members reach their full potential and creating an inclusive work environment to meet and exceed the needs of our diverse world.\\n\\nAre you ready to feel a tremendous sense of pride and satisfaction as you do your part to keep the largest airline in the world running smoothly as we care for people on life’s journey? Feel free to be yourself at American.\\nAdditional Locations: None\\nRequisition ID: 67295\",\n",
       "  'JobSalary': '$89K - $117K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.8',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Transportation & Logistics',\n",
       "  'CompanyYearFounded': '1926',\n",
       "  'CompanyIndustry': 'Airlines, Airports & Air Transportation',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'The Walt Disney Company (Corporate)\\n3.9',\n",
       "  'JobTitle': 'Sr Data Engineer',\n",
       "  'JobLocation': 'Lake Buena Vista, FL',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': '***This position will require working in the Orlando office 4 days a week. Relocation may be required.**\\nDepartment/Group Description:\\nThe Disney Decision Science + Integration (DDSI) organization is responsible for supporting clients across The Walt Disney Company including the Disney Parks, Experiences and Products segment which include Parks & Resorts both domestic and international, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which include Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.\\nDDSI leverages technology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business decisions and drive business value.\\nThe Data Engineering team is responsible for partnering with Decision Science Products, Decision Science, Client and Technology team members on various development and sustainment projects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering services.\\nThe DDSI Data Engineering (DE) team is seeking a Senior Data Engineer to design, develop, enhance, and implement systems for multiple analytical applications. The work will involve multiple aspects of a project: data requirements, design, pipeline development, database design, automation, orchestration and support.\\nResponsibilities of the Role:\\nWork assignments may cover activities such as data requirements gathering, source-to-target mapping, data validation scripting and review, developing and monitoring ETL/ELT data pipelines, designing and implementing database schema/tables/views, and producing datasets as input to science models and visualizations. Engaged in technical reviews of the data engineering team to ensure quality deliverables as well as potentially guiding other team members.\\nIn addition to technical capabilities, the role is responsible for understanding the business domain and processes, then applying that knowledge to the data design and solution. This role communicates data engineering progress to the project leadership team, and actively participates in meetings and discussions.\\nTechnologies generally leveraged to fulfill the work include, but not limited to PostgreSQL, Snowflake, Python, Docker, and Gitlab.\\nBasic Qualifications:\\n3-5 years of work experience in database design, tuning queries, and developing data pipelines\\nExperience and understanding of one or more business domains to assist in gathering data requirements and data design solutions\\nProven experience using Python, SQL, and cloud storage (such as AWS S3)\\nExperience with developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code deployment/promotion\\nStrong understanding of database design and proficiency utilizing various database platforms, such as PostgreSQL, or Snowflake\\nDemonstrated proficiency with API development\\nExperience managing and deploying code using a source control product such as GitLab/GitHub\\nAble to effectively formulate solutions and communicate complex technical concepts to non-technical team members\\nBachelor’s degree (Computer Science, Mathematics, Software Engineering or related field, or equivalent experience)\\nDesired Qualifications:\\nMaster’s degree (Computer Science, Mathematics, Engineering or related field preferred)\\nExperience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake, Databricks, or similar\\nKnowledgeable on cloud architecture and product offerings, preferably AWS\\nExperience leveraging containerization technologies such as Docker or Kubernetes\\nHands-on knowledge of job scheduling software like Apache Airflow, Amazon MWAA, or UC4',\n",
       "  'JobSalary': '$91K - $127K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Media & Communication',\n",
       "  'CompanyYearFounded': '1923',\n",
       "  'CompanyIndustry': 'Film Production',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Grammarly, Inc.\\n4.5',\n",
       "  'JobTitle': 'Data Engineer, Data Platform',\n",
       "  'JobLocation': 'United States',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.\\nAll roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.\\nGrammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.\\nThe opportunity\\nEvery day, tens of millions of people and 50,000 professional teams worldwide trust Grammarly’s AI and human expertise to help ideate, compose, revise, and comprehend communications. Our team members have the autonomy to take on exciting challenges in pursuit of our mission to improve lives by improving communication. Together, we’re building on more than a decade of steady growth and profitability. We’re defining the communication assistance with our tailored service offerings: Grammarly Free, Grammarly Premium, Grammarly Business, and Grammarly for Education. Our latest product offering, GrammarlyGO, brings the power of generative AI to our users. It all begins with our team collaborating in an inclusive, values-driven, and learning-oriented environment.\\nTo achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.\\nGrammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure.\\nYour impact\\nAs a Data Engineer on our Data Engineering Platform team, you will:\\nDrive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.\\nMake analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.\\nModel structure, storage, and access of data at very high volumes for our data lakehouse.\\nImprove developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).\\nOwn data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.\\nBuild a world-class process that will allow our systems to scale.\\nMentor other back-end engineers on the team and help them grow.\\nBuild and contribute to AWS high-scale distributed systems on the back-end.\\nWe’re looking for someone who\\nEmbodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.\\nIs able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.\\nHas experience with Python, Scala, or Java.\\nHas experience with designing database objects and writing relational queries\\nHas experience designing and standing up APIs and services.\\nHas experience with system design and building internal tools.\\nHas experience handling applications that work with data from data lakes.\\nHas at least some experience building internal Admin sites.\\nHas good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).\\nCan knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.\\nSupport for you, professionally and personally\\nProfessional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.\\nA connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.\\nCompensation and benefits\\nGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more:\\nExcellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)\\nDisability and life insurance options\\n401(k) and RRSP matching\\nPaid parental leave\\nTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days\\nHome office stipends\\nCaregiver and pet care stipends\\nWellness stipends\\nAdmission discounts\\nLearning and development opportunities\\nGrammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. . If a location of interest is not listed, please speak with a recruiter for additional information.\\nBase pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.\\nUnited States:\\nZone 1: $167,000 - $242,000/year (USD)\\nZone 2: $150,000 – $218,000/year (USD)\\nZone 3: $142,000 – $206,000/year (USD)\\nZone 4: $134,000 – $194,000/year (USD)\\nWe encourage you to apply\\nAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).\\nPlease note that EEOC is optional and specific to US-based candidates.\\n#NA\\n#LI-DT1\\nAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.\\n#LI-Hybrid\",\n",
       "  'JobSalary': 'Employer Provided Salary:$167K - $242K',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2009',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Redstone Federal Credit Union\\n3.9',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Job Description Summary\\nCollaborates with business owners, data analysts, and data scientists to create and deploy data products that identify insights, predict behavior and prescribe action. Responsible for creating, managing and continuously improving our data architecture, pipelines, and data warehouse. Establishes data quality standards and implements processes and tools to assess and improve data accuracy. Partners with data scientists, analysts and software engineers to deploy data models. Develops other team members within the department.\\nJob Description\\nESSENTIAL DUTIES AND RESPONSIBILITIES\\nMaintains a professional image and demeanor at all times consistently demonstrating Credit Union RISE Values and adhering to the Code of Ethics. Delivers friendly, caring service to internal customers.\\nBuilds, maintains and improves the enterprise data warehouse. Monitor & improve performance, implement patches and upgrades, minimize database downtime and manage parameters to provide fast query responses.\\nCollaborates with data analysts and data scientists and software engineers to build, maintain and improve data models.\\nEstablishes, maintains and improves metadata management tools and processes.\\nBuilds, maintains and improves the infrastructure required for optimal ETL/ELT from a wide variety of data sources.\\nCreates and maintains optimal data pipeline architecture.\\nAssembles large, complex data sets that meet functional and non-functional business requirements.\\nEstablishes data quality standards and implements processes and tools to assess and improve data accuracy, completeness, reliability, relevance and timeliness.\\nAssists with development and implementation of disaster recovery plans and processes to protect the integrity and availability of enterprise databases and data pipelines.\\nPartners with data scientists, analysts and software engineers to deploy data models to predict behavior and prescribe action.\\nDevelops and implements disaster recovery plans and processes to protect the integrity and availability of enterprise databases and data pipelines.\\nContinuously builds domain expertise in various credit union subject areas, such as (for example) marketing, underwriting, fraud, and servicing channels.\\nCompletes all required training programs to maintain a current knowledge applicable to assigned duties and responsibilities, including regulatory compliance requirements.\\nCompletes training and self-study to achieve and maintain required knowledge of Credit Union products, services and overall operations.\\nComplies with all applicable regulatory requirements and Credit Union policies and procedures.\\nAdheres to all security procedures and maintains strict confidentiality of all member information.\\nCompletes required on-line regulatory and compliance training, on a semi-annual basis, including but not limited to; Bank Secrecy Act, Anti-Money Laundering and USA Patriot Act.\\nWorks scheduled hours and maintains punctuality.\\nPerforms other related duties as assigned or requested.\\nMINIMUM QUALIFICATIONS\\nTo perform this job satisfactorily, an employee must be able to carry out each essential duty competently.\\nThe requirements listed below are representative of the education, experience, skills and abilities required.\\nEDUCATION / EXPERIENCE\\nA bachelor's degree in Computer Science, Statistics, Informatics, Information Technology or another quantitative field.\\nFive+ years responsible experience in a data engineering, database administration, or data science role.\\nExperience with core programming languages (especially but not limited to SQL, Python, Scala).\\nExperience with administration and use of relational databases and data warehouses .\\nExperience with data modeling, especially dimensional modeling.\\nExperience in distributed computing (e.g. Apache Spark) and Linux/Unix commands, scripting (bash) and file management.\\nExperience with NoSQL databases (e.g. Cassandra) and stream-processing systems (e.g. Kafka, Spark streaming, Storm, Kinesis), preferred.\\nAn equivalent combination of education and experience may be considered.\\nSKILLS / ABILITIES\\nEffectively apply internal/external customer service practices and processes to meet quality service standards and achieve member satisfaction.\\nLearn and apply information, on a wide range of Credit Union products, services and regulatory compliance requirements, in order to assess member situations and develop solutions.\\nCommunicate in a professional manner and deliver information clearly and effectively. Actively listen to questions, opinions and ideas of others. Use tact and diplomacy in sensitive and confidential situations.\\nLead and model RISE values and Code of Ethics through daily interactions and conduct.\\nProvide guidance in the resolution of complex problems utilizing advanced knowledge and experience within areas of responsibility.\\nUse correct English including spelling, grammar and punctuation.\\nUnderstand and follow written and oral instructions.\\nStrong analytical aptitude with a driving curiosity to identify, formulate, and solve problems.\\nA driving curiosity to identify, formulate, and solve problems.\\nAbility to assess cost trade-offs related to decision-making, model deployment, and further data acquisition/analysis in a business context.\\nComfortable deriving business implications from data/analyses and making recommendations for business investments and action - even in the absence of complete information.\\nDesire and ability to continuously learn new data science methods and tools in order to have increasing impact on business results.\\nAbility to train and develop others.\\nSet priorities and manage one’s own time effectively.\\nPHYSICAL DEMANDS\\nThe physical demands described here are representative of those that must be met by employees to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\\nIntermittent standing, sitting and walking.\\nUsing hands repetitively to handle, feel or operate computers and other standard office equipment.\\nReaching with hands and arms.\\nIntermittent lifting and carrying between 5 and 25 pounds.\\nWORK ENVIRONMENT\\nAn employee in this job works in a typical technology office environment. The employee is subject to providing on-call guidance and direction and/or technical assistance on a 24x7 basis.\\nRedstone Federal Credit Union is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status or status as an individual with disability. All qualified applicants will not be discriminated against on the basis of disability.\\nWe are proud to be a Drug-Free and Tobacco Free Workplace.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Nonprofit Organization',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1951',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'IronArch Technology\\n4.8',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Description:\\n\\nWho We Are\\nKnown for being a Best Place to Work and a People First company, IronArch Technology is an award-winning Service-Disabled Veteran-Owned Small Business (SDVOSB) specializing in providing innovative solutions and world class services to Federal Government clients.\\nOur employees have voted us as a 'Best Place to Work' 7 times and we are an INC 5000 recipient for being one of the fastest growing businesses in the United States.\\nOur Values: People First, Servant Leadership, Deliver and Inspire Excellence, Do the right thing!\\nResponsibilities will include:\\nDevelop, construct, expand, and optimize data and data pipeline architecture\\nDefine data assets to populate data models\\nDesign data integrations and data quality framework\\nDesign and evaluate open source and vendor tools for data lineage\\nWork closely with all business units and engineering teams to develop strategy for long term data platform architecture\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics\\nSupport software developers, database architects, and data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects\\nWork with stakeholders to assist with data-related technical issues and support their data infrastructure needs\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing products into an innovative environment\\nPlan and conduct data engineering projects and/or studies to evaluate, design, and tabulate statistical sampling plans and analytical procedures and processes. Responsibilities include detailed problem definition; identification and investigation of key variables or parameters; development and validation of assumptions; formulation of alternatives; development of performance measures of merit; establishment of data collection requirements; selection, adaptation, or development of appropriate analytical methodology; systems analysis or evaluation of alternatives; development of recommendations, and presentation of results or findings in multiple formats, forums, and to audiences of varying degree of technical knowledge and seniority\\nInterface with senior leadership on complex problems and resolves major issues, based on sound data engineering principles and affordable solutions. Serves as the principal investigator for data engineering requiring a high degree of technical competence to gauge the extent to which the perimeters of the state-of-the-art can be pushed\\nEngage with organizations, Industry and Academia to conduct necessary stakeholder analysis to ensure data engineering projects support IVC and end-user requirements.\\nDevelop and aids in the production of documents, roadmaps and strategies for data environment and architecture\\nProvide support to the development of requirements documents and program baseline and program recommendations as appropriate\\nCollaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization\\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\\nWrite unit/integration tests, contributes to engineering wiki, and documents work\\nPerform data analysis required to troubleshoot data related issues and assist in the resolution of data issues\\nRequirements:\\n\\nBachelor’s or master’s degree in computer/data science technical or related experience\\n5 years experience with using the following software/big data tools: Hadoop, Spark, and Kafka\\nRelational SQL and NoSQL databases, including Postgres and Cassandra Data pipeline and workflow management tools: Azkaban, Luigi, and Airflow\\nCloud services: Elastic Compute Cloud (EC2), Elastic MapReduce (EMR), Remote Desktop Services (RDS), and Redshift\\nStream-processing systems: Storm, Spark-Streaming\\nObject-oriented/object function scripting languages: Python, Java, C Object-Oriented Programming Language(C++), and Scala\\nMust have knowledge of and skill in the use of various software, data integration services, data visualization tools, on premises and in cloud analytics such as but not limited to the following:\\nStructured Query Language (SQL),\\nPython,Hypertext Preprocessor (PHP), S,Statistical Analysis System (SAS), Tableau, Power Business Intelligence (BI), Github, PySpark, DataBricks, Palantir, Data Lakes, Collibra, Azure DevOps, AWS Cloud Services, Azure Cloud Services, Machine Learning\\nExcellent interpersonal, communication, presentation, writing, analytical, problem solving, and information gathering skills along with fundamental technique troubleshooting abilities\\nPreferred qualifications:\\nExperience working with VHA and familiar with VHA data\\nWhy IronArch Technology?\\nAwarded Best Place to Work 7 times!\\nCompetitive compensation and market-leading bonus opportunities\\nMedical, dental and vision benefits where a significant portion of the premium is subsidized by IronArch. For qualifying high deductible health plans, IronArch also contributes towards a Health Reimbursement Account to cover eligible medical expenses\\nCompany-provided healthcare concierge assistance to help explain your coverage in plain language; help you find, choose, and schedule quality care; and address billing, benefit, or claims concerns, potentially saving hours of your time\\n401(k) retirement plan where the company contributes dollar for dollar up to 3 percent, and 50 cents on the dollar for the 4th and 5th percent with immediate entry and immediate vesting\\n20 days of PTO accumulated per calendar year\\n11 paid holidays\\nBereavement, jury duty, parental (maternity/paternity/adoption), and military leaves\\nSabbatical programs\\nCompany-paid short- and long-term disability\\nCompany-paid life insurance\\nVoluntary life, accidental and indemnity income replacement benefits\\nProfessional development reimbursement\\nHealth club reimbursement\\nMatching donation program and annual philanthropic activities\\nPet insurance\\nAnd more!\\nApply today to learn why IronArch Technology has been recognized as “Best Place to Work” for 7 years!\\nIronArch Technology is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.8',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2013',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Pyramid Consulting, Inc\\n4.3',\n",
       "  'JobTitle': 'Big Data Engineer with Active VOS',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Immediate need for a talented Big Data Engineer with Active VOS. This is a 06+ Months contract opportunity with long-term potential and is located in Dallas, TX, Atlanta, GA (Initial Remote for 1 month and then Onsite). Please review the job description below and contact me ASAP if you are interested.\\n\\nJob ID: 23-29355\\n\\nPay Range: $115k/Annum. Employee benefits include, but are not limited to, health insurance (medical, dental, vision), 401(k) plan, and paid sick leave (depending on work location).\\n\\nKey Requirements and Technology Experience:\\nOverall, 8+ years of experience in Applications development/deployments/implementation with AIOPs being focus area.\\nMust have development experience with Java, Cramer, ActiveVOS, BPEL, Flex.\\nWell versed with product/solution implementation life cycle.\\nExperience in implementing applications with AIOPS open-source technologies Big Data, Python, NodeJS/Angular with NoSCH.\\nAs part of implementation, should have good knowledge of Integrating front-end and back-end application components developed.\\nExperience with NoSQL databases such as MongoDB, Elasticsearch and Big data stacks like ELK.\\nExposure to cloud-based deployments (GCP/AWS/AZURE/Private).\\nKnowledge of Machine learning and relevant certification.\\nExperience with Client is a plus.\\nOur client is a leading IT Industry and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration.\\n\\nPyramid Consulting, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\\n\\n#DEL',\n",
       "  'JobSalary': 'Employer Provided Salary:$115K',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Human Resources & Staffing',\n",
       "  'CompanyYearFounded': '1996',\n",
       "  'CompanyIndustry': 'HR Consulting',\n",
       "  'CompanyRevenue': '$500 million to $1 billion (USD)'},\n",
       " {'CompanyName': 'Lucid Technologies Inc\\n4.5',\n",
       "  'JobTitle': 'AWS DATA ENGINEER',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Role/Title: - AWS DATA ENGINEER\\nRemote, Des Moines , Iowa\\n\\nGeneral Description:\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services.\\n\\nJob Description:\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\n\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\n\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\n\\nRESPONSIBILITIES:\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\n\\nREQUIREMENTS:\\nBachelors or masters degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\n\\nSkills Matrix:\\nBachelors or masters degree in computer science, Data Engineering, or a related field.Required\\nProfessional experience as a Data Engineer, with a focus on AWS data services and technologies.Required5Years\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.Required5Years\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing aRequired5Years\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.Required5Years\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.Required5Years\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.Desired\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.Required5Years\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.Required\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environmentRequired\\n\\nThanks and Regards,\\nManala Priyanka\\nUS IT Recruiter\\nLucid Technologies Inc\\nEmail:priyanka.m@lucidtechinc.com\\nW:www.LucidTechINC.com',\n",
       "  'JobSalary': '$84K - $113K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Computer Hardware Development',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Adientone\\n2.8',\n",
       "  'JobTitle': 'Big Data Cloud Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Description:\\nAs a Cloud Engineer of the Data Platform and Infrastructure Engineering team, you will have an outstanding opportunity to use your technical expertise and creativity in Big Data to design, develop and innovative a global data analytics platform.\\nWe are seeking a talented, ambitious, and highly creative software engineer to join our team to continue to evolve our big data platform.\\nWork with team to build big data platform, deliver task with good quality, establish a data platform for the entire Cisco Collaboration solution.\\nWork on our large-scale and across geography data platform, design, build, manage and monitor them.\\nWork closely with global team on engagements by estimating the complexity and duration of technical tasks.\\nWho you are: Have a good understanding of Big Data platform components such as HDFS and its eco-systems. Familiar with data analytics best practices, including at least 2 categories of knowledge below:\\nHadoop/ELK administration - Contribute to the evolving architecture to meet growth requirements for scaling, reliability, performance and security.\\nData governance – experienced in developing and integrating software allowing for flexible and scalable data transformation with data quality controls.\\nData analytics –backgrounds in statistics, machine learning and similar technologies.\\nData visualization – knowledge of tools that are cost-effective and make it easy for end users to better understand and produce reports and graphs.\\nQualifications: * US citizenship is required\\nBachelor of Science in Engineering or Computer Science\\n3+ years hands-on experienced in Hadoop, ELK, Kafka, Apache Spark etc\\nProficient in Java/Python/Puppet/Chef\\nAble to leverage automation frameworks such as Jenkins or puppet for deployment and test automation\\nFlexible, Self-motivated, Problem solver who can work both individually as well as an effective team player with excellent attention to detail and great interpersonal skills (both verbal and written)\\nBe willing/able to work across boundary to reach the best solution in world class cloud services.\\nJob Types: Full-time, Contract\\nSalary: $50.00 - $58.00 per hour\\nBenefits:\\n401(k) matching\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nExperience level:\\n3 years\\nSchedule:\\n10 hour shift\\nMonday to Friday\\nApplication Question(s):\\nDo you have understanding of HDFS/ELK administration?\\nExperience:\\nHadoop: 3 years (Required)\\nKafka: 1 year (Required)\\nWork Location: Remote',\n",
       "  'JobSalary': 'Employer Provided Salary:$50.00 - $58.00 Per Hour',\n",
       "  'CompanyRating': '2.8',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Deloitte\\n4.0',\n",
       "  'JobTitle': 'Cloud Data Engineer - Healthcare',\n",
       "  'JobLocation': 'Hattiesburg, MS',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Are you an experienced, passionate pioneer in technology who wants to work in a collaborative environment? As an experienced Cloud Data Engineer - Healthcare you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. This position is working on a multi-year project for a major healthcare client. This is a remote role.\\n\\nWork you'll do/Responsibilities\\n\\nYou will determine processes and automation tools to reduce IT spend and increase efficiencies on multiple projects within the Healthcare domain.\\n\\nThis position includes collaborating with DevOps teams to implement CI/CD pipelines, automated deployments, and infrastructure as code (IaC) practices for AWS-based solutions. Document design, development, and deployment processes, as well as create technical specifications and user guides for developed solutions.\\n\\nYour role will be to design, develop, and deploy cloud-based solutions for data processing, analytics, and integration using cloud services and big data technologies. Collaborate with architects, data engineers, and business stakeholders to understand requirements and translate them into technical solutions.\\n\\nYou will implement data ingestion, transformation, and storage processes using cloud services like AWS's S3, Glue, Athena, Redshift, and EMR. Implement security, data governance, and compliance measures to ensure data integrity and protection in AWS-based solutions. Develop and optimize data pipelines using Snowpark, SnowSQL, Hadoop and PySpark to extract, transform, and load data efficiently.\\n\\nYou will conduct performance tuning and optimization of data processing and analytics workflows to maximize efficiency and scalability. Work with cross-functional teams to troubleshoot and resolve issues related to data processing, data integration, and analytics solutions.\\n\\nCommunicate regularly with Engagement Managers (Directors), project team members, and representatives from various functional and / or technical teams, including escalating any matters that require additional attention and consideration from engagement management\\n\\nThe Team\\n\\nAs a part of the US Strategy & Analytics Offering Portfolio, the AI & Data Operations offering provides managed AI, Intelligent Automation, and Data DevOps services across the advise-implement-operate spectrum.\\n\\nQualifications\\n\\nRequired\\n\\n5+ years' experience as a Cloud Data Engineer\\n\\n5+ years' hands on experience in Snowpark, SnowSQL, Hadoop and PySpark\\n\\n5+ years' experience in AWS services such as S3, Glue, Athena, Redshift, EMR, Lambda and Cloud Formation.\\n\\n5+ years' experience in Python with a focus on data processing and analytics\\n\\n5+ years in healthcare domain\\n\\n5+ years in consulting\\n\\nStrong knowledge and hands-on experience in designing, developing, and deploying scalable solutions on the cloud platforms\\n\\nExpertise in SQL and database technologies for data manipulation and querying\\n\\nBachelor's degree or equivalent experience\\n\\nLimited immigration sponsorship may be available\\n\\nPreferred\\n\\nFamiliarity with data modeling, data warehousing, and data integration concepts.\\n\\nExperience with DevOps practices, CI/CD pipelines, and infrastructure as code (IAAC) using tools like Jenkins, Git, and Terraform.\\n\\nStrong analytical and problem-solving skills, with the ability to troubleshoot and resolve complex technical issues.\\n\\nFamiliarity with agile development methodologies and experience working in Agile teams\\n\\nAbility to travel 10%, on average, based on the work you do and the clients and industries/sectors you serve\\n\\nBachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience\\n\\nAnalytical/ decision making responsibilities\\n\\nAnalytical ability to manage multiple projects and prioritize tasks into manageable work products\\n\\nCan operate independently or with minimum supervision\\n\\nExcellent communication skills\\n\\nAbility to deliver technical demonstrations\",\n",
       "  'JobSalary': '$82K - $115K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1850',\n",
       "  'CompanyIndustry': 'Accounting & Tax',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'The Data Sherpas',\n",
       "  'JobTitle': 'Senior Data Engineer - Big Data',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Who We Are:\\nThe Data Sherpas are a team of highly skilled and motivated engineers that help our clients at every phase of their cloud journey. If it touches the cloud, involves data, or lives as an application, we have either worked on it or have the skills and expertise to accomplish it.\\nWhat We Are Looking For:\\nThe Senior Data Engineer role will be centered around big data to extract insights, build data pipelines, and create data-driven applications. They will be involved in building and maintaining a large-scale data platform for both real-time and batch processing. They're responsible for creating a highly scalable, high-performance infrastructure for big data applications in the cloud. This will involve using various big data technologies like Spark, Flink, Kafka, etc., and will work with massive volumes of data at a petabyte scale.\\nThis is a remote contract position for a minimum of one year. Work will be performed in Pacific Time Zone.\\nWhat You'll Do:\\nBuild components of a large-scale data platform for both real-time and batch processing.\\nContribute to data manipulation tasks and coding responsibilities.\\nParticipate in building and improving CI/CD pipelines.\\nContribute to the best engineering practices, including design patterns, code review, and automated testing.\\nWhat You Have:\\n4+ years of professional programming experience, Python preferred, but proficiency in any language is accepted.\\n3+ years of big data development experience with technical stacks like Spark, Flink, Singlestore, Kafka, Nifi, and AWS big data technologies.\\nSolid experience with AWS is a must.\\nExperience with processing large volumes of data at the petabyte level.\\nProficiency with cloud infrastructure technologies, including Terraform, K8S, Spinnaker, IAM, ALB, etc.\\nFamiliarity with ClickHouse, Druid, Snowflake, Impala, Presto, Kinesis, etc.\\nExperience with widely used Web frameworks (React.js, Vue.js, Angular, etc.) and good knowledge of Web stack HTML, CSS, and Webpack.\\nSecurity experience is a plus.\\nWe cannot work with third-party agencies at this time. Resumes submitted via unapproved agencies will be automatically rejected.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': None,\n",
       "  'CompanySize': None,\n",
       "  'CompanyType': None,\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': None,\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Cube Hub Inc\\n3.2',\n",
       "  'JobTitle': 'Sr. Network Data Center Engineer III - LG',\n",
       "  'JobLocation': 'Santa Clara, CA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"5+ years of IT network operations/support and engineering experience in highly complex network environments containing Cisco and/or Arista, supported with a recognized industry certification such as: Arista Ace L3 or above and/or Cisco CCNA/CCNP/CCIE.\\nPreferred Qualifications:\\nAssociates/Bachelor's and/or Master's degree in Computer Engineering, Computer Science, Software Engineering, Mechanical Engineering or any other Engineering/Science related field\\nExperience with:\\nManaging a network fabric that includes supporting integrated components e.g., firewalls, load balancers, web app firewalls.\\nTCP/IP variable length subnet masking and address allocation\\nDynamic routing protocols e.g., OSPF, BGP\\nSDN solutions such as: Cisco ACI or Arista CVP, implementing Layer 2/3 redundancy such as link aggregation protocols and loop prevention protocols for example MLAG, LACP, STP, HSRP, VRRP\\nIndustry cabling standards, and the layers of the OSI model\\nNetwork security concepts\\nDNS/DHCP/NTP concepts and solutions\\nAlso, advantageous but not essential\\nDevelopment of network-oriented applications using Python and/or JavaScript with a React Framework\\nCI/CD methodology and GitHub/GitLab repository distribution\\nRelational Databases and developing advanced SQL queries.\\nBackend API development with Rest API and XML/JSON\\nUser level experience on Windows, Linux based platforms/appliances\\nResponsibilities will be, but are not limited to:\\nDetermination and deployment of robust, stable, automated, and manageable industry leading Data Center Network technologies as part of a global team.\\nDevelops automation for global data center management through SDN managed capabilities.\\nManage technical projects.\\nResearch new methods for automation to enable efficiencies in the global data center environment.\\nMonitor and perform capacity/feasibility studies and resolve network capacity issues. Identify, develop, and deploy tools supporting network services.\\nPlans and schedules work to meet deadlines established by others to ensure the completion of several related tasks.\\n#IND2\\nJob Type: Contract\\nPay: $60.00 - $65.00 per hour\\nBenefits:\\nHealth insurance\\nSchedule:\\n10 hour shift\\n8 hour shift\\nSupplemental pay types:\\nBonus pay\\nWork Location: In person\",\n",
       "  'JobSalary': 'Employer Provided Salary:$60.00 - $65.00 Per Hour',\n",
       "  'CompanyRating': '3.2',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'ALTA IT Services\\n4.4',\n",
       "  'JobTitle': 'Data Engineer (Sr. and Mid)',\n",
       "  'JobLocation': 'Alexandria, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Title: Sr. & Mid Data Engineer ( w/ active Secret clearance) Location: hybrid in office / remote: 2-3 days a week on-site in DC metro area ( Arlington / DC) Security clearance needed: TS/SCI Compensation range: Open, based on extent of position-relevant experience *** FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly: TEXT: ( 240-601-8546) OR CALL: ( 301-212-7355) ALTA IT Services is seeking a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform. Responsibilities As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members. Required Skills: Clearance: Secret 8+ years of experience with SQL 8+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets. 8+ years of experience with a modern programming language such as Python or Java 8 years of experience working in a big data and cloud environment Experience with distributed computer understanding and experience with SQL, Spark, ETL. Documented experience with AWS, EC2, S3, and/or RDS Preferred Skills: 4 years of experience working in an agile development environment Ability to quickly learn technical concepts and communicate with multiple functional groups Ability to display a positive, can-do attitude to solve the challenges of tomorrow Possession of excellent verbal and written communication skills Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly: TEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\\nTitle: Sr. & Mid Data Engineer ( w/ active Secret clearance)\\nLocation: hybrid in office / remote: 2-3 days a week on-site in DC metro area ( Arlington / DC)\\nSecurity clearance needed: TS/SCI\\nCompensation range: Open, based on extent of position-relevant experience\\n\\n*** FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly:\\nTEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\\n\\nALTA IT Services is seeking a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform.\\nResponsibilities\\nAs a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.\\nRequired Skills:\\nClearance: Secret\\n8+ years of experience with SQL\\n8+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.\\n8+ years of experience with a modern programming language such as Python or Java\\n8 years of experience working in a big data and cloud environment\\nExperience with distributed computer understanding and experience with SQL, Spark, ETL.\\nDocumented experience with AWS, EC2, S3, and/or RDS\\nPreferred Skills:\\n4 years of experience working in an agile development environment\\nAbility to quickly learn technical concepts and communicate with multiple functional groups\\nAbility to display a positive, can-do attitude to solve the challenges of tomorrow\\nPossession of excellent verbal and written communication skills\\nPreferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes\\n\\nFOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly:\\nTEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\",\n",
       "  'JobSalary': '$76K - $116K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': 'Pyramid Consulting, Inc\\n4.3',\n",
       "  'JobTitle': 'Big Data Engineer with Active VOS',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Immediate need for a talented Big Data Engineer with Active VOS. This is a 06+ Months contract opportunity with long-term potential and is located in Dallas, TX, Atlanta, GA (Initial Remote for 1 month and then Onsite). Please review the job description below and contact me ASAP if you are interested.\\n\\nJob ID: 23-29355\\n\\nPay Range: $115k/Annum. Employee benefits include, but are not limited to, health insurance (medical, dental, vision), 401(k) plan, and paid sick leave (depending on work location).\\n\\nKey Requirements and Technology Experience:\\nOverall, 8+ years of experience in Applications development/deployments/implementation with AIOPs being focus area.\\nMust have development experience with Java, Cramer, ActiveVOS, BPEL, Flex.\\nWell versed with product/solution implementation life cycle.\\nExperience in implementing applications with AIOPS open-source technologies Big Data, Python, NodeJS/Angular with NoSCH.\\nAs part of implementation, should have good knowledge of Integrating front-end and back-end application components developed.\\nExperience with NoSQL databases such as MongoDB, Elasticsearch and Big data stacks like ELK.\\nExposure to cloud-based deployments (GCP/AWS/AZURE/Private).\\nKnowledge of Machine learning and relevant certification.\\nExperience with Client is a plus.\\nOur client is a leading IT Industry and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration.\\n\\nPyramid Consulting, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\\n\\n#DEL',\n",
       "  'JobSalary': 'Employer Provided Salary:$115K',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Human Resources & Staffing',\n",
       "  'CompanyYearFounded': '1996',\n",
       "  'CompanyIndustry': 'HR Consulting',\n",
       "  'CompanyRevenue': '$500 million to $1 billion (USD)'},\n",
       " {'CompanyName': 'Intellibee\\n4.5',\n",
       "  'JobTitle': 'AWS DATA ENGINEER',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'This position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\n\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\n\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\nRESPONSIBILITIES\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\nREQUIREMENTS\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\nSkill Matrix\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field. Required\\nProfessional experience as a Data Engineer, with a focus on AWS data services and technologies. Required 5 Years\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies. Required 5 Years\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing a Required 5 Years\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB. Required 5 Years\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices. Required 5 Years\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus. Desired\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle. Required 5 Years\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions. Required\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment Required',\n",
       "  'JobSalary': '$72K - $99K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Unknown / Non-Applicable',\n",
       "  'CompanySector': None,\n",
       "  'CompanyYearFounded': 'Company - Public',\n",
       "  'CompanyIndustry': None,\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Locus Recruiting\\n4.0',\n",
       "  'JobTitle': 'Data Center Engineer (Structured Cabling)',\n",
       "  'JobLocation': 'United States',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Locus (Recruitment Firm) is recruiting for a Data Center (Structured Cabling) Engineers for our client.\\nPosition: Data Center (Structured Cabling) Engineer\\nDuration: 6 month contract to hire (Full-time work)\\nPay: $50-60/hr. ($100-$125,000 conversion)\\nLocation: United states\\nOvernight Shift (11pm-7am)\\nTravel 75% + each week (leave Sunday evening-return Thursday or Friday)\\nExpenses Reimbursed (Hotel, Meals, etc.)\\n40 hours a week\\nHave a need to hire 6 individuals-2 Western US, 2 Central US and 2 Eastern US\\nSkills:\\n· 3rd Shift to cover Datacenters and Critical facilities at airports while flights are not flying.\\n· Doing remediation work\\n· Following a run book\\n· Taking 12 largest airports, re-cabling and moving racks forward, putting in AC, separate power\\n· Worked in Data centers, rack and stack at airports\\n· Low voltage electrician backgrounds work is great, fibers/cabling experience\\nJob Types: Full-time, Contract\\nPay: $50.00 - $60.00 per hour\\nBenefits:\\nDental insurance\\nHealth insurance\\nVision insurance\\nSchedule:\\n8 hour shift\\nEvening shift\\nMonday to Friday\\nNight shift\\nWeekends as needed\\nApplication Question(s):\\nAre you open to a 6-month contract-to-hire position working 40 hours a week?\\nAre you comfortable traveling Sunday-Thursday each week?\\nAre you comfortable working 11pm-7am?\\nAre the pay requirements in line with what you are looking for?\\nDo you have any experience working within the Airline Industry?\\nExperience:\\nData center: 5 years (Preferred)\\nRack and Stack: 5 years (Preferred)\\nCabling: 5 years (Preferred)\\nLow Voltage: 5 years (Preferred)\\nShift availability:\\nOvernight Shift (Required)\\nWillingness to travel:\\n75% (Required)\\nWork Location: On the road',\n",
       "  'JobSalary': 'Employer Provided Salary:$50.00 - $60.00 Per Hour',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Enterprise Software & Network Solutions',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Ford Motor Company\\n4.1',\n",
       "  'JobTitle': 'IT Data Engineer',\n",
       "  'JobLocation': 'Dearborn, MI',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"At Ford Motor Company, we believe freedom of movement drives human progress. We also believe in providing you with the freedom to define and realize your dreams. With our incredible plans for the future of mobility, we have a wide variety of opportunities for you to accelerate your career potential as you help us define tomorrow’s transportation.\\nCreating the future of smart mobility requires the highly intelligent use of data, metrics and analytics. That’s where you can make an impact as part of our Global Data Insight & Analytics team. We are the trusted advisers that enable Ford to clearly see business conditions, customer needs and the competitive landscape. With our support, key decision makers can act in meaningful, positive ways. Join us and use your data expertise and analytical skills to drive evidence-based, timely decision making.\\n\\nThe minimum requirements we seek:\\nBachelor’s degree in Computer Science, Information Technology, Electronics Engineering or a closely related field of study\\n2+ years of experience in development using scripting languages (e.g. Springboot, Kafka, JavaScript, JQuery, JSON, AJAX, Node JS, Angular JS, Angular 8/9)\\n2+ years of experience in Implementing and developing micro services by using spring boot from scratch.\\n1+ years of experience working with RDBMS like SQL Server and Teradata\\nOur preferred requirements:\\nMaster’s degree in Computer Science, Information Technology, Electronics Engineering or a closely related field of study\\nFamiliarity with PCF or GCP and tools like Postman, Gradle, GIT, Jenkins.\\nHands on development expertise in middleware, core business frameworks and Back-end database layer integration\\nKnowledge of Dynatrace, Splunk and other monitoring tools.\\nExpertise working with RDBMS and SQL.\\nSoftware Craftsmanship Experience in Test Driven Development (TDD),CI/CD. Experience helping solve application and performance issues\\nExcellent interpersonal and communication skills\\nProven ability to coordinate and establish trust and positive relationships across teams\\nStrong analytical and problem-solving skills Agile Mentality\\nWhat you’ll receive in return :\\nAs part of the Ford family, you’ll enjoy excellent compensation and a comprehensive benefits package that includes generous PTO, retirement, savings, and stock investment plans, incentive compensation, and much more. You’ll also experience exciting opportunities for professional and personal growth and recognition.\\nCandidates for positions with Ford Motor Company must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship may be available for this position.\\nWe are an Equal Opportunity Employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status, or protected veteran status.\\nFor information on Ford's salary and benefits, please visit:\\nhttps://corporate.ford.com/content/dam/corporate/us/en-us/documents/careers/2022-benefits-and-comp-GSR-sal-plan-2.pdf\\nAt Ford, the health and safety of our employees is our top priority. Vaccination has been proven to play a critical role in combating COVID-19. As a result, Ford has made the decision to require U.S. salaried employees to be fully vaccinated against COVID-19, unless employees require an accommodation for religious or medical reasons. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination or an approved medical or religious exemption. (As of June 19, 2023, we are no longer requiring a Covid-19 vaccination),\\n\\nWhat you’ll be able to do:\\nWe’re seeking an experienced data engineering professional to collaborate closely and continuously with fellow engineers and product owners to build, validate and release innovative software products.\\nDevelop and maintain data pipelines interacting with GCP Big Query across Data warehouses and Data marts\\nWork with Astronomer and develop DAG pipelines in Python\\nBe part of a pioneering team, moving data products from on-premises to GCP for a critical part of Ford’s business\\nEnhance testing frameworks to ensure code quality and develop CI/CD pipelines for automation\\nEnsure developed code and practices align to Ford Standards and Policies.\\nAutomate performance monitoring and notification in the event of failures using best practices and tools.\\nWork in a small agile team to deliver working software and adopt eXtreme Programming (XP) practices like paired programming and mobbing.\\nTake time to continuously learn and share with peers by playing roles of mentee and mentor over the course of delivery\",\n",
       "  'JobSalary': '$77K - $106K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Manufacturing',\n",
       "  'CompanyYearFounded': '1903',\n",
       "  'CompanyIndustry': 'Transportation Equipment Manufacturing',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Huckberry\\n4.7',\n",
       "  'JobTitle': 'Data Engineer - Contract',\n",
       "  'JobLocation': 'Austin, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Huckberry is seeking a Data Engineer to work closely in analytics to ensure the best data integrity and structuring to support insights. The Data Engineer will primarily work in DBT modeling with our site (Segment) and inventory (NetSuite) data to provide accessibility to our most important data. The Data Engineer will also support the movement and modeling of new data sources in Databricks. The candidate should be well organized, quick at code, and think beyond the task and toward the objective as a thought partner with the analytics team. Huckberry is a rapidly growing company, this data engineer will need to be flexible and step in as a technical support across multiple business facing tools – Toolio, Airtable, Looker, etc.\\nRequirements\\nCollaborate with software engineers and analysts in data collection and data usage to model analytics tables from raw format to useable fact tables\\nConfigure DBT models to keep the cost per query down and query speeds high for the team\\nWrite custom batch load and event streaming Python jobs to transfer data between systems (API, SFTP, etc.)\\nPartner with analysts to productionalize machine learning algorithms where helpful (NLP, product recommendations, time-series forecasting, etc.)\\nManage data orchestration and chron scheduling between Python and SQL jobs with tools like DBT Cloud, Dagster, Prefect, etc.\\nBring best practices for overall data governance as it relates to data integrity tooling, user-access permission setting, query testing, alerts, customer data protection, etc.\\nMitigate our “bus factor” and “technical debt” with organized folders and consistent field-naming methodology as well as conviction to document-as-you-go\\n\\n\\nResponsibilities\\n2-5 years of professional experience as a data engineer in a high-growth medium-sized company that uses tools like DBT, Airflow, GCP, Databricks, etc.\\nExperience with ETL modeling and Chron scheduling with tools like DBT, Airflow, Databricks etc.\\nStrong proficiency in both Python and SQL, Javascript experience a plus\\nProven ability to remain organized and flexible – often scoping the 80/20 solution\\nExperience in GitHub and can work effectively in version controlling best practices\\nDemonstrated experience with large scale data-warehouse and cloud technologies like GCP as well as relational databases like Postgres\\nDemonstrated ability to project manage and communicate progress to senior leadership on large-scale projects\\nProven ability to collaborate with software engineers and data analysts to create synergy between data collection and data usage\\nExperience in Machine Learning a plus\\nExperience with 3rd party pixels and SDKs a plus\\nBenefits\\nFlexibile schedule\\nCompetitive pay\\nSummer Fridays\\n\\n\\nCompany Description\\nHuckberry is a leading men’s lifestyle retailer and media company. Millions of guys trust us as their go-to resource for the coolest new gear, lifestyle inspiration, and a lot more. We were recently named one of IAB’s most disruptive consumer brands, and we’ve collaborated with everyone from Matthew McConaughey and Kelly Slater to brands like Danner, Timex, and RRL. We look forward to meeting you.\\n\\n\\nWant to get to know us better? Check out our:\\n\\nJournal: https://huckberry.com/journal\\nYouTube: https://www.youtube.com/c/Huckberryco\\nInstagram: http://instagram.com/huckberry\\n\\nHuckberry encourages candidates of all different backgrounds and identities to apply. We are always eager to further diversify our company, and we are committed to providing an inclusive environment of mutual respect where all can flourish. All of our employment decisions are based solely on merit and business need.\\nNotice to California Job Applicants',\n",
       "  'JobSalary': '$94K - $134K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.7',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Retail & Wholesale',\n",
       "  'CompanyYearFounded': '2011',\n",
       "  'CompanyIndustry': 'Department, Clothing & Shoe Stores',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': 'Lucid Technologies Inc\\n4.5',\n",
       "  'JobTitle': 'AWS DATA ENGINEER',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Role/Title: - AWS DATA ENGINEER\\nRemote, Des Moines , Iowa\\n\\nGeneral Description:\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services.\\n\\nJob Description:\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\n\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\n\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\n\\nRESPONSIBILITIES:\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\n\\nREQUIREMENTS:\\nBachelors or masters degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\n\\nSkills Matrix:\\nBachelors or masters degree in computer science, Data Engineering, or a related field.Required\\nProfessional experience as a Data Engineer, with a focus on AWS data services and technologies.Required5Years\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.Required5Years\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing aRequired5Years\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.Required5Years\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.Required5Years\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.Desired\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.Required5Years\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.Required\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environmentRequired\\n\\nThanks and Regards,\\nManala Priyanka\\nUS IT Recruiter\\nLucid Technologies Inc\\nEmail:priyanka.m@lucidtechinc.com\\nW:www.LucidTechINC.com',\n",
       "  'JobSalary': '$84K - $113K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Computer Hardware Development',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Redstone Federal Credit Union\\n3.9',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Job Description Summary\\nCollaborates with business owners, data analysts, and data scientists to create and deploy data products that identify insights, predict behavior and prescribe action. Responsible for creating, managing and continuously improving our data architecture, pipelines, and data warehouse. Establishes data quality standards and implements processes and tools to assess and improve data accuracy. Partners with data scientists, analysts and software engineers to deploy data models. Develops other team members within the department.\\nJob Description\\nESSENTIAL DUTIES AND RESPONSIBILITIES\\nMaintains a professional image and demeanor at all times consistently demonstrating Credit Union RISE Values and adhering to the Code of Ethics. Delivers friendly, caring service to internal customers.\\nBuilds, maintains and improves the enterprise data warehouse. Monitor & improve performance, implement patches and upgrades, minimize database downtime and manage parameters to provide fast query responses.\\nCollaborates with data analysts and data scientists and software engineers to build, maintain and improve data models.\\nEstablishes, maintains and improves metadata management tools and processes.\\nBuilds, maintains and improves the infrastructure required for optimal ETL/ELT from a wide variety of data sources.\\nCreates and maintains optimal data pipeline architecture.\\nAssembles large, complex data sets that meet functional and non-functional business requirements.\\nEstablishes data quality standards and implements processes and tools to assess and improve data accuracy, completeness, reliability, relevance and timeliness.\\nAssists with development and implementation of disaster recovery plans and processes to protect the integrity and availability of enterprise databases and data pipelines.\\nPartners with data scientists, analysts and software engineers to deploy data models to predict behavior and prescribe action.\\nDevelops and implements disaster recovery plans and processes to protect the integrity and availability of enterprise databases and data pipelines.\\nContinuously builds domain expertise in various credit union subject areas, such as (for example) marketing, underwriting, fraud, and servicing channels.\\nCompletes all required training programs to maintain a current knowledge applicable to assigned duties and responsibilities, including regulatory compliance requirements.\\nCompletes training and self-study to achieve and maintain required knowledge of Credit Union products, services and overall operations.\\nComplies with all applicable regulatory requirements and Credit Union policies and procedures.\\nAdheres to all security procedures and maintains strict confidentiality of all member information.\\nCompletes required on-line regulatory and compliance training, on a semi-annual basis, including but not limited to; Bank Secrecy Act, Anti-Money Laundering and USA Patriot Act.\\nWorks scheduled hours and maintains punctuality.\\nPerforms other related duties as assigned or requested.\\nMINIMUM QUALIFICATIONS\\nTo perform this job satisfactorily, an employee must be able to carry out each essential duty competently.\\nThe requirements listed below are representative of the education, experience, skills and abilities required.\\nEDUCATION / EXPERIENCE\\nA bachelor's degree in Computer Science, Statistics, Informatics, Information Technology or another quantitative field.\\nFive+ years responsible experience in a data engineering, database administration, or data science role.\\nExperience with core programming languages (especially but not limited to SQL, Python, Scala).\\nExperience with administration and use of relational databases and data warehouses .\\nExperience with data modeling, especially dimensional modeling.\\nExperience in distributed computing (e.g. Apache Spark) and Linux/Unix commands, scripting (bash) and file management.\\nExperience with NoSQL databases (e.g. Cassandra) and stream-processing systems (e.g. Kafka, Spark streaming, Storm, Kinesis), preferred.\\nAn equivalent combination of education and experience may be considered.\\nSKILLS / ABILITIES\\nEffectively apply internal/external customer service practices and processes to meet quality service standards and achieve member satisfaction.\\nLearn and apply information, on a wide range of Credit Union products, services and regulatory compliance requirements, in order to assess member situations and develop solutions.\\nCommunicate in a professional manner and deliver information clearly and effectively. Actively listen to questions, opinions and ideas of others. Use tact and diplomacy in sensitive and confidential situations.\\nLead and model RISE values and Code of Ethics through daily interactions and conduct.\\nProvide guidance in the resolution of complex problems utilizing advanced knowledge and experience within areas of responsibility.\\nUse correct English including spelling, grammar and punctuation.\\nUnderstand and follow written and oral instructions.\\nStrong analytical aptitude with a driving curiosity to identify, formulate, and solve problems.\\nA driving curiosity to identify, formulate, and solve problems.\\nAbility to assess cost trade-offs related to decision-making, model deployment, and further data acquisition/analysis in a business context.\\nComfortable deriving business implications from data/analyses and making recommendations for business investments and action - even in the absence of complete information.\\nDesire and ability to continuously learn new data science methods and tools in order to have increasing impact on business results.\\nAbility to train and develop others.\\nSet priorities and manage one’s own time effectively.\\nPHYSICAL DEMANDS\\nThe physical demands described here are representative of those that must be met by employees to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\\nIntermittent standing, sitting and walking.\\nUsing hands repetitively to handle, feel or operate computers and other standard office equipment.\\nReaching with hands and arms.\\nIntermittent lifting and carrying between 5 and 25 pounds.\\nWORK ENVIRONMENT\\nAn employee in this job works in a typical technology office environment. The employee is subject to providing on-call guidance and direction and/or technical assistance on a 24x7 basis.\\nRedstone Federal Credit Union is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status or status as an individual with disability. All qualified applicants will not be discriminated against on the basis of disability.\\nWe are proud to be a Drug-Free and Tobacco Free Workplace.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Nonprofit Organization',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1951',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'Bamboo Health\\n3.5',\n",
       "  'JobTitle': 'Sr. Data Integration Engineer',\n",
       "  'JobLocation': 'Remote',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We are driven by our mission of enabling better care for patients across the continuum. Our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. Join us in improving healthcare for all!\\nSummary:\\nWe are actively hiring a full-time Sr. Data Integration Engineer to focus on supporting and extending our data platform. Bamboo Health receives HL7 data from hospitals, EHRs and HIEs around the country and this role will be responsible for integrating new HL7 EHR senders to the data pipeline using in-house tools, scripts, and custom applications. The ideal candidate will work well in a team, have a data-first mentality, and thrive in customer-facing projects.\\nWhat You Will Do:\\nPartner with Operations to ensure on-boarding HL7 integrations meet target deadlines through task resolution in a timely and organized manner\\nPartner with broader Platform Engineering team on cross-functional initiatives focused on infrastructure scalability and stability.\\nPartner with Software Engineers focused on improving our data pipeline\\nDesign and execute HL7 test plans for on-boarding new integrations\\nBuild a standard integration process to receive data and post events to new EHR systems\\nWork with ADT senders to resolve customer issues and maintain high quality interfaces\\nOn-board and track standard HL7 integrations\\nTriage customer issues related to HL7 integrations\\n\\nWhat Success Looks Like…\\nIn 3 months…\\nExecute:\\nDevelop solid understanding of Bamboo Health onboarding process for Technical Implementation Services\\nContribute to HL7 data validation, mapping, and testing processes\\nDevelop an understanding of our HL7 data pipeline\\nBuild relationships across the broader Product Platform organization\\n\\nIn 6 months…Manage:\\nWork with our Product, Operations and Network Operations Center teams to drive streamlined data processing and continuous improvement initiatives.\\nContribute to the development and reporting of data quality metrics\\n\\nIn 12 months…Scale:\\nDevelop a comprehensive knowledge of our data ingestion architecture\\nManage complex customer integrations with a heavy focus on service and quality outcomes\\n\\nWhat You Need:\\n5+ years professional experience in or around software development\\nExperience in or around the Healthcare domain\\nExperience in at least one modern language such as Java, Python, JavaScript\\nProficient in SQL\\nWillingness to learn healthcare data exchange formats\\nAbility to self-start project tasks and communicate progress clearly\\nAbility to work autonomously on multiple concurrent projects and prioritize appropriately\\nExperience organizing and delivering on several lines of work with clear communication on progress\\nDesire to work in a fast-paced collaborative environment\\nA work environment that is conducive to high quality virtual interactions. This includes but is not limited to being able to work from a quiet space with minimal interruptions or distractions, and a strong internet connection.\\n\\nHelpful/Preferred Experience:\\nHealthcare data integration tools (Mirth preferred)\\nCloud-native AWS solutions\\nSDLC – Git, pull requests\\nDocker & Kubernetes\\nAtlassian product suite\\nSumoLogic, Prometheus/Grafana, or equivalent\\n\\nWhat You Get:\\nJoin one of the most innovative healthcare technology companies in the country.\\nHave the autonomy to build something with an enthusiastically supportive team.\\nLearn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors.\\nReceive competitive compensation, including equity, with health, dental, vision and other benefits.\\n\\nBamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\\n#LI-Remote',\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.5',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1994',\n",
       "  'CompanyIndustry': 'Enterprise Software & Network Solutions',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'Sr. Software Development Engineer in Test (Data Focused)',\n",
       "  'JobLocation': 'Austin, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.\\nJob Overview:\\nWe are seeking a Senior Quality Assurance Engineer. Our ideal candidate will be responsible for designing and executing tests manually (15%) and automatically (85%) to identify and resolve all data related issues to meet quality standards. Our tight-knit team combines a Scrum process with a strong testing culture.\\nResponsibilities:\\nBuild and perform End to End QA effort on Data Warehouse/ETL/BI Platforms\\nReview requirements, specifications and technical design documents to provide timely and meaningful feedback\\nUnderstand data flow and test strategy for ETL , Data warehouse and Business Intelligence testing\\nETL testing of mapping, transformations and data pipelines.\\nData warehouse testing involving the testing of stored procedures, SSIS packages, slowly changing dimension tables, Change Data capture etc.\\nBusiness Intelligence testing involving the validation of DataMart, ODS, Data models and SSRS reports.\\nCreate detailed, comprehensive and well-structured test plans and test cases\\nIdentify, document and track software defects\\nWork closely with development teams to perform root cause analysis of issues\\nManage multiple tasks and adjust to shifting priorities, as necessary\\nWrite advanced SQL queries for ETL/data warehousing/Business Intelligence testing\\nIdentify test cases which has to be automated. Prepare test automation strategy.\\nDesign and develop test automation for backend.\\nWork with the team to continually improve test processes and practices based on inspection/adaption of previous iterations and to ensure adherence to process, tools and metrics standards within the project team\\nShow initiatives and accountability with strong time management skills with project teams.\\nExhibit strong collaboration/interpersonal skills\\nBe a self-starter and possess the ability to research issues and improve processes\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nBachelor’s Degree, Computer Science or similar, plus 6+ years of experience OR Master’s Degree, Computer Science or similar, plus 5+ years of experience OR no degree, plus 10+ years of experience working as a Developer or QA Engineer or SDET working in Agile scrum teams\\n5 –10 years of experience with Software Quality Assurance- QA project life cycle, test plan, test strategies, test scenarios, test cases, traceability matrix.\\n3 - 5+ years of experience in Database/Data Warehouse/ETL Testing.\\n3 - 5+ years of Strong experience in advanced SQL scripting.\\nExperience in MS SQL server, SSIS and SSRS.\\n3+ years’ experience of Data Lake/Hadoop platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Hadoop/Spark implementations.\\nCore Competencies:\\nExperience in ETL testing techniques\\nGood understanding of Data Warehousing and business intelligence concepts and testing techniques\\nExperience in Working with Star Schema, ODS, multi-dimensional models, slowly changing dimensions\\nExperience in working with ETL framework, Change Data capture, DataMart, Data models etc.\\nWorking knowledge of test automation for backend using Java and associated frameworks\\nExperience in continuous testing practices in a CI/CD development pipeline, and deploying test automation\\nSolid experience in strategizing and planning all testing activities including automation.\\nAbility to review and analyze business requirements in order to produce test strategy and test cases.\\nExpertise working in Cloud data environment\\nFamiliarity with one or more SQL-on-Hadoop technology (Hive, Impala, Spark SQL, Presto)\\nExperience in Agile projects (Scrum, Kanban etc.).\\nWorking Knowledge in Test Management software (JIRA,qTest).\\nSolid experience with Defect Management Process.\\nQuick learner and self-starter who requires minimal supervision to excel in a dynamic environment.\\nExcellent analytical and problem solving skills.\\nStrong verbal and written communications skills.\\nExperience in Banking or Financial services domain is preferred.\\nExperience in working with financial platforms\\nExperience in Cluster, Containers/VMs, On Premise, On Cloud\\nStrong knowledge on Data Lake, Azure, ELT & ETL, Data Warehouse, BI, Data Factory Tools\\nStrong in SQL preparation in Oracle/SQL Server/NoSQL/RDMS/Big Data\\nPreferences:\\nAwareness of Agile QA Software development life cycle –backlog, sprints, standups, burndowns.\\n\\nPay Range:\\n$90,080-$135,120/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$90K - $135K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'VP, Principal Data Engineer (REMOTE)',\n",
       "  'JobLocation': 'New Jersey',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.\\nJob Overview:\\nAt LPL Financial we consider it our mission to take care of our advisors so they can take care of their client. Joining as a VP, Principal Data Engineer, you will be responsible for providing technical leadership and guidance to development teams responsible for our Enterprise Data Warehouse and Data Distribution functions. This position will help the teams driving modernization and strategic solutions for our on-premises and cloud architectures. Specifically, this position will interface with our architecture and development teams to help lead the strategic modernization of our data warehouse and distribution components as part of the larger Data Foundations program. The position requires experience with architecting and engineering data solutions using AWS cloud services, Big Data, and Snowflake.\\nResponsibilities:\\nProvide technical leadership, architecture and engineering to technology teams based on business objective and strategic direction for our Enterprise Data Warehouse and Distribution Groups\\nEngage in the design, architecture and scope discussions to frame the LPL Data Warehousing and Distribution approach across the broader Data Foundations program\\nCollaborate with other development teams, enterprise architecture and operations to design, develop and maintain on-premises and cloud based platforms integrating CI/CD capabilities.\\nLead engineering and technical design discussions with the development teams and monitor all agile ceremonies\\nServe as a technical subject matter expert to the broader development teams assisting with AWS services (Glue, Lambda, Event Bridge), Databases (Snowflake, Oracle, SQL Server), Big Data\\nCreates best practices for team members to follow to meet key objectives of the technology platform.\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nBachelor’s degree in Computer Science, Engineering, or related field\\nOverall 15+ years of experience with 8+ years of experience in designing and implementing complex systems within a data warehouse and distribution functions\\nExcellent understanding of IT architecture and experience in data platforms on premises and the cloud\\n5+ years of experience working with Python, Spark framework (Pyspark or Scala), AWS Cloud services Glue ETL, Lambda, Event Bridge, Airflow, and project implementation in Snowflake\\n5+ years of experience working with Snowflake, SQL Server, Oracle, MapR, and Hadoop databases\\n5+ years of leadership and management experience\\nCore Competencies:\\nCapable of effectively planning, prioritizing and executing tasks utilizing resources and tools\\nIs visible, vocal, and objective. Exemplary collaboration, interpersonal, and presentation skills\\nSuperior skill and ability in multi-tasking and appropriate prioritization\\nHas technical vision and can drive best in class solutions\\nExcellent verbal and written communication skills, both technical and non-technical\\nStrong analytical and problem-solving skills.\\nMotivated and driven by achieving strategic technical solutions\\nPreferences:\\nCloud Architecture certification preferred\\nSnowflake SnowPro certification preferred\\n5+ Technical leadership expertise within an agile delivery framework\\n5+ Experience with Data Platforms (Snowflake, SQL Server, Oracle)\\n5+ Financial services experience\\nScaled Agile (SAFe) certification (PO/PM) preferred\\n\\nPay Range:\\n$125,400-$188,100/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$125K - $188K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'TEKsystems\\n3.9',\n",
       "  'JobTitle': 'Informatica Data Engineer/Developer (Remote)',\n",
       "  'JobLocation': 'Bethesda, MD',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Equivalent Experience\\nDescription:\\nAnalytica is seeking an Informatica Data Engineer/Developer (Remote) to support a key, long-term federal government client program and software product (Federal Housing Finance Agency (FHFA) Data Analytics program). The ideal candidate will be responsible for loading and testing available data, analyzing data warehousing systems, and overseeing the resolution of any issues that may arise and providing solutions for them. You should be able to develop data warehousing systems by using Informatica tools and smoothly integrate these warehousing systems with an organization or company’s existing systems and troubleshoot and debug any issues, while ensuring that the requirements for all the processes have been met and regular quality checks on the stored data occur.\\nResponsibilities include (but not limited to):\\nProvide business analysis and develop ETL code and scripting to meet all technical specifications and business requirements according to the established designs.\\nSupports development and administration of ETL using Informatica Power Center 10.2 in RHEL 7 and 10.5.3 in RHEL 8 transformations.\\nUses the Informatica DI platform to extract data from external sources and ingest them into a Data warehouse or Data Lake\\nUses the Informatica Power Center to perform DI operations and/or build rulesets and apply Data quality.\\nDevelop application systems that comply with the standard system development methodology and concepts for design, programming, backup, and recovery to deliver solutions that have superior performance and integrity.\\nContribute to determining programming approach, tools, and techniques that best meet the business requirements.\\nProvide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.\\nDeploys application code and analytical models using CI/CD tools and techniques and provides support for deployed data applications and analytical models\\nDevelop data ingestion and stream-analytic solutions leveraging on premise and cloud based ETL technologies such as Informatica, Kafka, Python, Apache Spark, Python or AWS-based solutions.\\nIdentify weak points or gaps in the systems architecture and provide recommendations for improving operations and the end user experience.\\nProvides technical documentation of Source and Target mappings.\\nParticipates in design and development reviews as per best practices.\\nWorks with System owners to resolve source data issues and refine transformation rules.\\nLeading the support staff as the SME for troubleshooting issues.\\nBasic Qualifications:\\nBachelor’s degree in Computer Science, Engineering, IT, or other scientific or quantitative fields\\nMinimum of 5 years of Informatica ETL Application administration 10.2 and 10.5.3, Solaris 11 and RHEL OS\\nStrong Informatica Admin experience with ability to migrate and upgrade from Solaris to Linux environments; data migration experience from one environment to another...\\n5+ years' Experience with advanced SQL in databases like SQL Server or Oracle as well as Linux shell scripting\\n5+ years’ experience in strategic data planning, governance and standards\\n5+ years’ experience in leveraging enterprise data warehouse modeling constructs and best practices to ensure flexible, scalable and high performing physical databases\\nPrevious hands on experience integrating systems with big data / data lake repositories using ETL tools\\nExcellent communication skills, presentation and interpersonal skills are required. Ability to communicate clearly with both business and technical resources\\nMust be a US Citizen or Perm resident with ability to secure a US Federal Clearance.\\nSkills:\\netl, Informatica Power Center, RHEL, Informatica ETL Application administration, Solaris, advanced SQL in databases, data planning, data governance, data standards, data warehouse modeling, big data / data lake repositories\\nTop Skills Details:\\netl,Informatica Power Center,RHEL,Informatica ETL Application administration,Solaris,advanced SQL in databases,data planning,data governance,data standards,data warehouse modeling,big data / data lake repositories\\nAdditional Skills & Qualifications:\\nSelf-starter, highly motivated individual who adapts to a dynamic work environment.\\nStrong attention to detail and communication capabilities with an ability to operate effectively across multiple priorities.\\nExcellent analytical and problem-solving skills.\\nMust be a US Citizen or US Permanent Resident (GC) with the ability to secure a NACI Public Trust (Analytica can hold clearance up to TS level.)\\nExperience Level:\\nExpert Level\\nAbout TEKsystems:\\nWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.\\nThe company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.\",\n",
       "  'JobSalary': 'Employer Provided Salary:$65.00 - $75.00 Per Hour',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Subsidiary or Business Segment',\n",
       "  'CompanySector': 'Human Resources & Staffing',\n",
       "  'CompanyYearFounded': '1983',\n",
       "  'CompanyIndustry': 'HR Consulting',\n",
       "  'CompanyRevenue': '$5 to $10 billion (USD)'},\n",
       " {'CompanyName': 'Omaha Public Power District\\n4.2',\n",
       "  'JobTitle': 'Data Engineer - Distribution Planning',\n",
       "  'JobLocation': 'Omaha, NE',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Responsibilities\\n\\nAs a Data Engineer specializing in Distribution Planning at an electrical utility company, you will design, develop, and maintain the data infrastructure and systems required for efficient and effective distribution planning operations. The Data Engineer will work to enhance modeling, analytics, visualization, and automation processes to further augment the department's abilities to plan and execute an affordable, reliable, and environmentally sensitive distribution grid.\\n1. Support distribution system analysis to ensure economical and reliable service for OPPD¿s customers.\\n2. Define, develop, and optimize techniques to gather, transform, analyze, and visualize data using automation and streamlined data pipelines.\\nWork with large and complex data sets to derive useable insights.\\nImprove the distribution system model through automation and enhanced data sets.\\nCapture and combine an assortment of data from a variety of sources and identify new or experimental frameworks to collect or extrapolate absent data.\\nHelp build creative visualizations that effectively communicate detailed insights and results to both technical and non-technical audiences.\\nWork to automate manual tasks to allow time and focus on higher-value work.\\nDetermine new analytical methods for representing the distribution system.\\n\\n3. Support the study of and build trends for emerging topics in the electric utility space including distributed energy resources, electric vehicles, and advanced metering infrastructure (AMI).\\n\\n4. Support the creation of an integrated system planning model that balances generation, transmission, and distribution needs.\\nQualifications\\n\\nRequired:\\nFour (4) year bachelor¿s degree in a science, engineering, mathematics, or related major\\nExperience in data analytics or data engineering related to technical topics.\\nMastery of common data management tools and SQL\\nWorking knowledge of Microsoft Office applications\\nFamiliarity with R, Python, or similar programming languages\\nDesired:\\nExpertise in Tableau or related data visualization software.\\nPrevious experience with industry-standard power system analysis software.\\nExperience with Geographic Information Systems.\\nExperience related to the electric utility industry and knowledge of power delivery systems and infrastructure.\\nComputer science or mathematics experience in statistical modeling, machine learning, and other advanced techniques.\\nClosing Statement\\n\\nSalary: Min: $91,371 - Mid: $114,214.\\nOrg Marketing Statement\\n\\nEOE: Protected Veterans/Disability\\nHow To Apply\\n\\nApply online at www.oppd.com on July 12th or before August 10th.\\nRecruiter: Angie Case -acase@oppd.com #LI-AC\\n\\n**PLEASE NOTE** - Your application has not been submitted unless you have applied for a specific requisition. If you have not chosen a specific opening, your application will remain in 'DRAFT' form and will not be viewed by our Human Capital staff.\",\n",
       "  'JobSalary': 'Employer Provided Salary:$91K',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Energy, Mining & Utilities',\n",
       "  'CompanyYearFounded': '1946',\n",
       "  'CompanyIndustry': 'Energy & Utilities',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Honda Development and Manufacturing of America, LLC.\\n3.4',\n",
       "  'JobTitle': 'Mass Production Data Process Engineer',\n",
       "  'JobLocation': 'East Liberty, OH',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Mass Production Data Process Engineer\\nLocation: East Liberty, Ohio\\nWorkstyle: Onsite\\nWhat Makes a Honda, is Who makes a Honda\\nHonda has a clear vision for the future, and it’s a joyful one. We are looking for individuals with the skills, courage, persistence, and dreams that will help us reach our future-focused goals.\\nAt our core is innovation. Honda is constantly innovating and developing solutions to drive our business with record success. We strive to be a company which serves as a source of “power” that supports people around the world who are trying to do things based on their own initiative and that helps people expand their own potential. To this end, Honda strives to realize “the joy and freedom of mobility” by developing new technologies and an innovative approach to achieve a “zero environmental footprint.”\\nWe are looking for qualified individuals with diverse backgrounds, experiences, continuous improvement values, and a strong work ethic to join our team.\\nIf your goals and values align with Honda’s, we want you to join our team to Bring the Future!\\nAbout this Position:\\nProcess Data Engineer will utilize engineering methods to engage in continuous improvement activities, while improving SQDCM characteristics. Process Data Engineer will use analytical tools to optimize assembly line production functions.\\nResponsibilities include:\\nAudit big data to ensure accurate information across all operation standards. Utilize Excel and VBA to optimize tasks.\\nIdentify opportunities to reduce waste within the process and improve efficiency. Apply 5S work methods, workstation design, and set best practices for production.\\nUse M.O.S.T. To develop balanced processes across manufacturing areas. Work to achieve high process efficiency. Apply lean manufacturing techniques. Design processes that are simple and achieve SQCDM targets.\\nSupport New Model events and forecast design impact to department. Identify concerns and countermeasures before mass production.\\nRegularly lead meetings with other Honda functional groups to develop and launch improvement themes (quality, safety, delivery, production, purchasing, packaging, or logistics.\\nDevelop plans and schedules for projects. Set timelines and detailed activities. Hold meetings with cross functional engineering groups to ensure completeness.\\nComplete studies to understand project/plant characteristic impacts and perform time estimates for assembly line work, equipment loads, and develop operation standards.\\nWork with counterparts in other NA plants to understand system change points.\\nRoot cause identification and countermeasure for complex problems.\\nWho we are seeking:\\nRequired Experience:\\nPrefer intern or co-op experience (1-2 terms) or relevant work experience of two years.\\nRequired Education:\\nBachelor’s degree in Engineering(Mechanical, Manufacturing, Industrial or Systems) or equivalent relevant experience.\\nOther job-specific skills:\\nRoot cause analysis, strong communication skills, collaboration with teams, and self-driven, strong continuous improvement mindset; MS office; problem solving/decision making skills; project management capabilities\\nAdditional Position Factors:\\nWorkstyle: Onsite\\nTravel: 5%\\nAt Honda, you will play a key role in our journey to become a company that society wants to exist now, and in the future. Your endless curiosity will drive innovation and your courageous spirit will challenge the status quo. We believe having a workforce made up of diverse thinkers and innovators makes us a better Honda. Respect for each other and respect for diversity each day drives our associates to contribute at the highest level and work effectively in a team environment. We make the dream of mobility a reality with our innovative and high-quality products. Together, we Bring the Future to our customers, associates, and communities. We are Honda!\\nWhat differentiates Honda and make us an employer of choice?\\nTotal Rewards:\\nCompetitive Base Salary\\nAnnual Bonus\\nOvertime Pay\\nIndustry-leading Benefit Plans (Medical, Dental, Vision, Rx)\\nPaid time off, including vacation, paid holidays, sick time, and personal days\\n401K Plan with company match + additional contribution\\nRelocation assistance (if eligible)\\nCareer Growth:\\nAdvancement opportunities\\nCareer mobility\\nEducation reimbursement for continued learning\\nTraining and Development programs\\nAdditional Offerings:\\nWellbeing program\\nCommunity service and engagement programs\\nProduct programs\\nFree drinks onsite\\nHonda is an equal opportunity employer and considers qualified applicants for employment without regard to race, color, creed, religion, national origin, sex, sexual orientation, gender identity and expression, age, disability, veteran status, or any other protected factor.',\n",
       "  'JobSalary': '$56K - $80K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.4',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Vehicle Dealers',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Subsidiary or Business Segment',\n",
       "  'CompanyIndustry': 'Retail & Wholesale',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Grammarly, Inc.\\n4.5',\n",
       "  'JobTitle': 'Software Engineer, Data Platform',\n",
       "  'JobLocation': 'United States',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.\\nAll roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.\\nGrammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.\\nThe opportunity\\nEvery day, tens of millions of people and 50,000 professional teams worldwide trust Grammarly's AI and human expertise to help ideate, compose, revise, and comprehend communications. Our team members have the autonomy to take on exciting challenges in pursuit of our mission to improve lives by improving communication. Together, we're building on more than a decade of steady growth and profitability. We're defining the communication assistance category with our tailored service offerings: Grammarly Free, Grammarly Premium, Grammarly Business, and Grammarly for Education. Our latest product offering, GrammarlyGO, brings the power of generative AI to our users. It all begins with our team collaborating in an inclusive, values-driven, and learning-oriented environment.\\nTo achieve our ambitious goals, we’re looking for a Software Engineer to join our Data Engineering Platform team. This person will build back-end systems to enable data management and create full-stack software tools to help data engineers and end users work with data at scale.\\nGrammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure.\\nYour impact\\nAs a Software Engineer on our Data Engineering Platform team, you will:\\nInstall, provision, and manage data pipeline orchestration software as a service for all of Grammarly to use.\\nInstall, provision, and manage container management software that will enable seamless scaling as our data volume grows.\\nCreate alerting and monitoring services that allow engineers to accurately and efficiently debug and resolve failures in real time.\\nOwn data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.\\nBuild a world-class process that will allow our systems to scale.\\nMentor other back-end engineers on the team and help them grow.\\nBuild and contribute to AWS high-scale distributed systems on the back-end.\\nWe’re looking for someone who\\n.\\nEmbodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.\\nIs able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.\\nHas experience with Python, Scala, or Java.\\nHas experience with designing database objects and writing relational queries.\\nHas experience designing and standing up APIs and services.\\nHas experience with system design and building internal tools.\\nHas experience handling applications that work with data from data lakes.\\nHas at least some experience building internal admin sites.\\nHas good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).\\nCan knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.\\nSupport for you, professionally and personally\\nProfessional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.\\nA connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.\\nCompensation and benefits\\nGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more:\\nExcellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)\\nDisability and life insurance options\\n401(k) and RRSP matching\\nPaid parental leave\\nTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days\\nHome office stipends\\nCaregiver and pet care stipends\\nWellness stipends\\nAdmission discounts\\nLearning and development opportunities\\nGrammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. . If a location of interest is not listed, please speak with a recruiter for additional information.\\nBase pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.\\nUnited States:\\nUnited States:\\nZone 1: $226,000 - $275,000/year (USD)\\nZone 2: $203,000 – $245,000/year (USD)\\nZone 3: $192,000 – $225,000/year (USD)\\nZone 4: $181,000 – $215,000/year (USD)\\nWe encourage you to apply\\nAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).\\nPlease note that EEOC is optional and specific to US-based candidates.\\n#NA\\n#LI-DT1\\nAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.\\n#LI-Hybrid\",\n",
       "  'JobSalary': 'Employer Provided Salary:$226K - $275K',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2009',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'Sr. Software Engineer - API/Data Services',\n",
       "  'JobLocation': 'Fort Mill, SC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients\\nJob Overview:\\nAt LPL Financial we consider it our mission to take care of our advisors so they can take care of their clients. Joining as Senior Software Engineer – API/Data Services, you will be responsible for building and maintaining sustainable, scalable products and capabilities that support our advisors to deliver excellent services for their investors.\\nResponsibilities:\\nDesign, develop, test, tune, and implement n-tiered web-based applications while collaborating with Development, Enterprise Architecture and Product teams\\nDeveloping and maintaining web applications, microservices, and RESTful APIs using Python and AWS services such as EKS, S3, RDS, Lambda, and API Gateway.\\nDesigning and implementing microservices-based architecture using Python and AWS.\\nAssess opportunities for application and process improvements and prepare SDLC documentation\\nMaintain, troubleshoot, optimize and enhance existing systems\\nWork collaboratively with QA, DevOPS teams to adopt CI/CD tool chain and develop automation\\nCommunicate with technical and non-technical groups on a regular basis as part of product/project support\\nDesign and develop core services and components with expertise in service-oriented architecture\\nDesign patterns and coding best practices.\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nProven experience as a Senior Python or C# Developer\\nExperience in developing RESTful Microservice-based APIs using Python web development frameworks such as Django, AWS Powertools, or Flask.\\nProficiency in using AWS services such as EKS, AWS Fargate, EC2, S3, RDS, Lambda, EventBridge, Glue, and API Gateway.\\nStrong unit testing and debugging skills.\\n5+ years of experience in designing and implementing complex systems accompanied by a Bachelor’s degree or Master Degree in Computer Science, Information Systems, Business, or other related field.\\nExcellent understanding and writing of SQL server procedures, views, functions and designing skills, caching and exception handling.\\nCore Competencies:\\nExcellent verbal and written communication skills, both technical and non-technical\\nStrong analytical and problem-solving skills\\nMotivated and driven by achieving long-term business outcomes\\nCapable of effectively planning, prioritizing and executing tasks utilizing resources and tools\\nPreferences:\\nExperience with Agile using JIRA\\nC#, .NET 6, ELK Stack, TeamCity, TFS, GIT\\nMaster data management (MDM)\\nExperience in Message queue implementations (Kafka)\\n\\nPay Range:\\n$96,800-$145,200/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$97K - $145K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Siteimprove\\n3.0',\n",
       "  'JobTitle': 'Senior Software Engineer- Data',\n",
       "  'JobLocation': 'Bellevue, WA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"The Siteimprove Data Platform team is responsible for creating and managing the integrated technologies supporting the end-to-end lifecycle of data across the enterprise. Our engineers are building an event-driven, API first platform that will enable internal and external business partners to acquire, retain, process, govern, and secure data at a global scale.\\nOur team is looking for an excellent senior software engineer who is a highly collaborative and broad-minded problem solver willing to architect and build solutions that are scalable, resilient, and adaptable to the future needs of the business.\\nWhat you will be doing\\nDirectly contributes to designing software systems, writing code, and designing APIs\\nProvides technical guidance and mentoring to peers and engineers, helping to overcome obstacles and foster growth opportunities\\nCollaborates with engineering leaders in creating a vision for evolving the Siteimprove Data Platform\\nMaintains, monitors, and improves our solutions and systems with a focus on service excellence; apply industry standards and new technologies to improve efficiency, quality, and system performance\\nProposes initial technical implementation which supports architectural changes that solve scaling and performance problems\\nLooks for innovation opportunities between several teams with a willingness to experiment and to boldly confront problems of large complexity and scope\\nParticipates in multiple Agile teams as an individual contributor, helping with estimation, proto-typing, bug fixing, and support of deployed services; emphasis on ensuring the team is building software to the highest standards\\nPerform other related duties as assigned\\nWhat we will require of you\\nBachelor’s/Master’s in Computer Science or any related technical field; or equivalent related professional experience\\n5+ years of professional, post-college software development\\n3+ years experience designing and building APIs, scalable distributed systems, and developing data intensive services or applications on AWS infrastructure\\n3+ years providing software and/or system architecture to solve complex, multi-discipline problems\\nStrong technical proficiency, capable of problem solving and applying critical thinking. A deep understanding of software design principles, algorithms and data structures and commitment to technical excellence\\nTravel as needed\\nWhat we will love about you\\nKnowledge of modern data processing and storage paradigms\\nKnowledge of machine learning models and systems using Python, TensorFlow, PyTorch, or other frameworks\\nKnowledge and experience with generative AI and prompt engineering\\nGeneral understanding of cloud native services, including AWS managed offering\\nProficiency in data processing, analysis, and visualization using SQL, Pandas, NumPy, Matplotlib, or other tools\\nStrong communication skills and highly effective collaborator. You articulate your ideas to teammates, peers, and leaders, providing details and supporting your ideas with data where applicable. You incorporate others' input and feedback and strive to find common ground\\nBase pay will depend on the position, individual qualifications, market and other operational business needs.\\nHow to apply\\n\\nClick on the APPLY FOR THIS JOB button to submit your application.\\n\\nSiteimprove is an equal-opportunity employer\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.\\n\\nSiteimprove is a global corporation and has developed data practices designed to assure your personally identifiable information is appropriately protected. Please note that personal information may be transferred, accessed, and stored globally as necessary for the uses and disclosures stated in accordance with our Privacy Policy at https://siteimprove.com/en/privacy/.\",\n",
       "  'JobSalary': 'Employer Provided Salary:$160K - $180K',\n",
       "  'CompanyRating': '3.0',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2003',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Computer Enterprises, Inc. (CEI)\\n3.6',\n",
       "  'JobTitle': 'Data/Software Engineer III',\n",
       "  'JobLocation': '',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"CEI's Fortune 20 client in Banking is seeking a highly skilled and experienced Software Developer III to join our team.\\nJob at a Glance:\\nLocation: 100% Remote (EST preferred)\\nWorking Hours: Monday to Friday, 8-5\\nContract Length: 6 months, w/ potential for hire/extension based on performance\\n*Corp2Corp not accepted\\nKey Responsibilities:\\nAttend daily meetings, sprint planning, and sprint assignments\\nTake responsibility for assigned work and provide regular updates\\nSupport new application functionality related to data and reporting needs\\nCollaborate with a project team to add new data feeds into applications as part of Data Warehouse projects\\nModify existing databases and database management systems\\nWrite and code logical and physical database descriptions\\nCoordinate database development within the project team\\nReview project requests and estimate time and cost requirements\\nTroubleshoot issues and make system changes as needed\\nPerform other duties as assigned\\nRequired Skills:\\nHigh School Degree and Bachelor's degree in computer science, software engineering, or a relevant field; or equivalent work experience\\n6-8 years of experience in IT development, analysis, information management, or QA\\nMust haves: CA7, data warehouse, Hive/Hadoop, Spark, Kafka\\nAdditional Skills (Preferred):\\nExperience in Financial Services\\nProficiency in XML, Java, JSP, and other relevant software\\nExperience with database development and management systems\\nAbility to troubleshoot issues and make system changes\\nExcellent verbal and written communication skills\\nAbility to work independently and manage time effectively\\nAs a trusted technology partner, CEI delivers solutions that help our customers transform their business and achieve meaningful results. From strategy and custom application development through application management - our technology and digital experience services are tailored to meet each unique need of our customers. Our staffing solutions bring specialized skills to complement our customers' workforce and project requirements.\\nJob Types: Full-time, Contract\\nPay: $65.00 per hour\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nVision insurance\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nDallas, TX 75225: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nCA7: 4 years (Required)\\nHive/Hadoop: 4 years (Required)\\nData Warehouse: 4 years (Required)\\nSpark: 4 years (Required)\\nKafka: 4 years (Required)\\nWork Location: Hybrid remote in Dallas, TX 75225\",\n",
       "  'JobSalary': 'Employer Provided Salary:$65.00 Per Hour',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1992',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'AT&T\\n3.7',\n",
       "  'JobTitle': 'Senior-Big Data Software Engineer',\n",
       "  'JobLocation': 'Plano, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Join AT&T and reimagine the communications and technologies that connect the world. We’re committed to those who seek to discover the undiscoverable and dare to disrupt the norm. Bring your bold ideas and fearless risk-taking to redefine connectivity and transform how the world shares stories and experiences that matter. When you step into a career with AT&T, you won’t just imagine the future – you’ll create it.\\nThe Senior Big Data Software Engineer provides a critical role for the success of our Chief Data Office (CDO) Network Intelligence Data Collect & AI Platforms and the AT&T Network Tracking (ANT) Transformation program. This role is key in supporting our azure cloud technologies that enable RAN, Customer Care, Legal Demands and Mobility business partners enabling various use cases like RAN Augmentation, IP Analytics, Billing optimization, StreamSaver analytics and Network planning.\\nKey Roles and Responsibilities\\nAs a Sr. Big Data Software Engineer, you will:\\nProduce batch and real time data pipelines for our network and NLP based use cases for our data scientists and other developers\\nDevelop deep domain expertise and opportunities to expand your role into additional facets of data insights including modeling, extensive cloud-based environment architecting, and software development.\\nAnalyze, design, program, debug and modify software enhancements and/or new products used in distributed, large-scale analytics and visualization solutions.\\nWork in a highly agile environment.\\nResponsible for the development of high performance, distributed computing tasks using Big Data technologies such as Hadoop, NoSQL, text mining and other distributed environment technologies in azure and on-prem.\\nHadoop Administration, Apache NiFi support and Databricks workspace Administration, supporting the Network Intelligence Data Collect & AI Team and their business partners.\\nResponsible for Automation of infrastructure using CICD/ansible or comparable technologies in Azure cloud.\\nSetting up secure vents and core infrastructure components in Azure. UNIX Administration.\\nSupporting the Network Intelligence Flowlogic Hadoop clusters & Databricks workspaces that collects the mobile packet data via the network probes\\nManaging the core ingestion NiFi servers and supporting the platforms teams with all things azure And responsible for Automation of infrastructure using CICD/ansible.\\nExpert Azure Security Engineer responsible for setting up secure vents and core infrastructure components in Azure. Addressing & supporting azure admin and Databricks related issues for various in house and offshore teams.\\nQualifications\\nBachelor of Science in Computer Science, Computer Engineering, or Scientific Computing preferred.\\nRequires 5 - 8 years of related experience.\\nRequires hands on experience using and administrating Databricks workspaces.\\nRequires experience with Azure cloud administration and subscription management.\\nStrong communication skills – communicating with all levels of management, understanding business requirements, and defining project roadmaps.\\nRequires on site presence 3-5 days a week in Dallas/Plano.\\nOur Big Data Software Engineers earn between $128,400-$192,600. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training.\\nJoining our team comes with amazing perks and benefits:\\nMedical/Dental/Vision coverage\\n401(k) plan\\nTuition reimbursement program\\nPaid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)\\nPaid Parental Leave\\nPaid Caregiver Leave\\nAdditional sick leave beyond what state and local law require may be available but is unprotected.\\nAdoption Reimbursement\\nDisability Benefits (short term and long term)\\nLife and Accidental Death Insurance\\nSupplemental benefit programs: critical illness/accident hospital indemnity/group legal\\nEmployee Assistance Programs (EAP)\\nExtensive employee wellness programs\\nEmployee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone\\nA career with us, a global leader in communications and technology, comes with big rewards. As part of our team, you’ll lead transformation surrounded by trailblazing industry leaders like you. You’ll be empowered to go above and beyond – making a difference through company-sponsored initiatives or connecting and networking through one of our many employee groups. And regardless of where you’re at in your career trajectory, you’ll be rewarded by the impact that comes with making a difference in the lives of millions. With AT&T, you’ll be a part of something greater, do incredible things and be rewarded with a chance to change the world.\\nApply now!',\n",
       "  'JobSalary': 'Employer Provided Salary:$128K - $193K',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Telecommunications',\n",
       "  'CompanyYearFounded': '1876',\n",
       "  'CompanyIndustry': 'Telecommunications Services',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Vortalsoft Inc\\n3.2',\n",
       "  'JobTitle': 'Data Center Technician / Customer Engineer',\n",
       "  'JobLocation': 'Chicago, IL',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"We have an opening for a technical professional to function as an On-site Engineer. We are looking for a self-motivated, dedicated individual who is ready to put their technical skills to work in a fast-paced, customer focused environment. Utilizing your technical knowledge, you will have the responsibility of responding to customer calls in a timely and efficient manner to troubleshoot, analyze and diagnose Oracle and HP servers in a critical environment. You will provide problem resolution or refer more complex issues to a Sr. Support Engineer. Maintenance of inventory and documentation of activity will demonstrate strong organizational skills. Must be available and oncall 24x7x365.\\nThe ideal candidate will be responsible for the hardware break/fix support. Experience in software support would be a plus; however, candidate should have some knowledge of windows, Linux and Solaris.\\nInstalls, troubleshoots, and maintains products/equipment (DELL, IBM and HP Servers).\\nIdentifies, analyzes, and repairs product failures, orders, and replaces parts as needed.\\nDetermines and recommends which products or services best fit the customers' needs.\\nFamiliar with a variety of the field's concepts, practices, and procedures.\\nRelies on experience and judgment to plan and accomplish goals.\\nExperience and Skills Required:\\nPossess positive attitude and strong analytical skills\\nMust be a self-starter who is able to work independently without supervision and within a team environment\\nMust possess a professional demeanor and the ability to develop effective working relationships with end users and stakeholders\\nMust be a good team player\\nWorking experience with Oracle/Sun HW a must\\nWorking experience with Remote Access Cards (IBM RSA, HP iLo, Dell IDRAC)\\nWorking experience with HP BL\\\\DL\\\\M\\\\Blades\\\\Apollo\\nWorking experience with DELL 13G/14G servers\\nWorking experience with IBM servers\\nCompTia A+ Certification a plus\\nCompTia Server+ Certification a plus\\nHP Certification a plus\\nDell Certification a plus\\nIBM Certification a plus\\nJob Type: Full-time\\nPay: $17.25 - $30.00 per hour\\nSchedule:\\n8 hour shift\\nDay shift\\nMonday to Friday\\nOn call\\nWeekends as needed\\nWork setting:\\nIn-person\\nExperience:\\nData center: 1 year (Preferred)\\nserver breakfix: 1 year (Preferred)\\nWork Location: In person\\nSpeak with the employer\\n+91 7327082159\",\n",
       "  'JobSalary': 'Employer Provided Salary:$17.25 - $30.00 Per Hour',\n",
       "  'CompanyRating': '3.2',\n",
       "  'CompanySize': '1 to 50 Employees',\n",
       "  'CompanyType': 'Computer Hardware Development',\n",
       "  'CompanySector': '$5 to $25 million (USD)',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'CyberCoders\\n4.2',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Senior Data Engineer\\nJob Title: Senior Data Engineer\\nLocation: Downtown Dallas, TX (parking included)\\nSalary: $125,000 - $140,000 + 15% bonus\\nPosition is hybrid working 4 days a week in office\\nWhat You Will Be Doing\\nDesigning and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository\\nDeveloping and maintaining data processing and storage infrastructure\\nEstablish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes\\nCollaborating with data analysts to ensure that data is readily available for analysis and modeling\\nOptimizing database performance and troubleshooting issues as they arise\\nImplementing data security and access controls to protect sensitive data\\nHighlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization\\nStaying up-to-date with emerging trends and technologies in data engineering\\nLeverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance\\nEnsure scientific method and research are key drivers of the product roadmap\\nWhat You Need for this Position\\n5+ years of experience in data engineering or a related field\\nStrong background in data modeling, database design, and ETL best practices\\nProficiency in one or more programming languages such as Python, Java, or Scala\\nExperience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases\\nExperience in real estate investment and/or rental sector highly desirable\\nPrior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry highly desirable\\nSignificant Experience building, motivating, and retaining a high performing, flexible and collaborative data and analytics function\\nHands-on technical background in data science, business intelligence or data engineering, and demonstrated strategic impact working with executive teams\\nA proven problem solver, experienced in building technical strategy and understanding technical tradeoffs and risk\\nCollaborative team player, you are truly a \"do-er\", happy to be a hands-on problem-solver to move the data program forward\\nSo, if you are a Senior Data Engineer with experience, please apply today! OR email your resume to erinn@cybercoders.com\\nApplicants must be authorized to work in the U.S.\\n\\nCyberCoders is proud to be an Equal Opportunity Employer\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.\\n\\nYour Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.',\n",
       "  'JobSalary': 'Employer Provided Salary:$125K - $140K',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Subsidiary or Business Segment',\n",
       "  'CompanySector': 'Human Resources & Staffing',\n",
       "  'CompanyYearFounded': '1999',\n",
       "  'CompanyIndustry': 'Staffing & Subcontracting',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'EquipmentShare\\n3.9',\n",
       "  'JobTitle': 'Senior Data Platform Engineer',\n",
       "  'JobLocation': 'Columbia, MO',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"EquipmentShare is Hiring a Senior Data Platform Engineer\\nEquipmentShare is searching for a Senior Data Platform Engineer to join our team. This position is fully remote.\\nWhat You'll Do as a Senior Data Platform Engineer\\nDespite having been fundamentally altered by earlier industrial revolutions, the construction industry has hardly budged with the computer revolution. In fact, since 1970, labor productivity in the US construction industry has actually declined, despite it more than doubling in the rest of the economy. This has contributed to housing shortages and the parlous state of infrastructure in some places, and is sanding the gears of carbon reduction efforts.\\nWe think the industry is ripe for change, and we're pushing the leading edge of that change with our next generation T3 Platform, the OS for Construction. Through T3, we help contractors to coordinate humans and (increasingly smarter) machines to build more effectively.\\nAs a Senior Data Platform Engineer in our small and quickly growing team, you will play a major role in this effort. In particular, you will:\\nIncrease the velocity of scientific exploration, experimentation and ML development at EquipmentShare by developing tools that enable full-stack data science.\\nHelp with build vs buy decisions and integration planning.\\nHelp shape best practices within the team, along with mentoring and developing junior data platform engineers.\\nAbout You as a Senior Data Platform Engineer\\nOur mission is to change an entire industry, so we only hire people inspired by the goal and up for the challenge. In turn, our employees have every opportunity to grow with us, achieve personal and professional success and enjoy making a tangible difference in a crucial industry for human welfare.\\nMinimum Qualifications for a Senior Data Platform Engineer\\nGraduate degree, or equivalent practical experience, in computer science, machine learning, software engineering or related field\\n5+ years working on technology-powered products as either a data platform engineer, data engineer, ML engineer or a closely related role\\nDemonstrated knowledge and experience in building a modern data science, experimentation and machine learning technology stack for a data-driven company\\nDemonstrated expertise in Python\\nProven experience building durable infrastructure on AWS\\nExperience with Snowflake is preferred, but not required\\nExperience with Kubernetes, Airflow and dbt is preferred, but not required\\nMust be qualified to work in the United States or the United Kingdom - we are not sponsoring any candidates at this time\\nWhy We're a Better Place to Work\\nCompetitive compensation packages\\n401 (k) and company match\\nHealth insurance and medical coverage benefits\\nUnlimited paid time off\\nGenerous paid parental leave\\nVolunteering and local charity initiatives that help you nurture and grow the communities you call home\\nOpportunities for career and professional development with conferences, events, seminars, continued education\\nState of the art onsite gym (Corporate HQ)/Gym stipend for remote employees\\nSince our incorporation in 2015, we've had incredible growth — and we're not stopping anytime soon. Ready to invest in our mission, invest in yourself and discover a better place to work? Then, we'd love to meet you.\\nAt EquipmentShare, it's more than just a job — it's a calling. Apply today.\\nEquipmentShare is an EOE M/F/D/V\\n\\n#LI-Remote\",\n",
       "  'JobSalary': '$86K - $124K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Construction, Repair & Maintenance Services',\n",
       "  'CompanyYearFounded': '2014',\n",
       "  'CompanyIndustry': 'Commercial Equipment Services',\n",
       "  'CompanyRevenue': 'Unknown / Non-Applicable'},\n",
       " {'CompanyName': 'Cambay Consulting LLC\\n4.3',\n",
       "  'JobTitle': 'AWS Data Engineer',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'This position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services.\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\nRESPONSIBILITIES\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\nREQUIREMENTS\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\nJob Type: Contract\\nSchedule:\\n8 hour shift\\nExperience:\\nAWS data services and technologies: 5 years (Preferred)\\nETL processes using AWS Glue, AWS Lambda, Apache Spark: 5 years (Preferred)\\nprogramming languages such as Python, Scala, or Java: 5 years (Preferred)\\nWork Location: Remote',\n",
       "  'JobSalary': '$78K - $107K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2011',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Rang Technologies Inc.\\n4.4',\n",
       "  'JobTitle': 'Sr. AWS Data Engineer',\n",
       "  'JobLocation': 'Newark, NJ',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Title: AWS Data Engineer\\nLocation: Newark, NJ (Once in a week)\\nClient: EXL\\nJob Type- FTE\\nRole Overview:\\nThe person will be part of the Global Technology team for a major Insurance client. He/ She will work as a Senior Cloud Engineer within the AWS AI/ML platform team and will have the opportunity to work one-on-one with application and infrastructure developers to build and enhance the AI/ML infrastructure and application patterns that power mission-critical applications, ensuring that they’re engineered for high availability, durability, and resiliency\\nThey will be part of an agile team that combines various backgrounds, experiences, and perspectives to solve complex problems within AWS and beyond.\\nThe person will also be required to participate actively in brainstorming sessions for improvement opportunities and take it to completion\\nA suitable candidate should have 8+ years of IT experience with at least 5+ years of experience working as an AWS Data Engineer. He/ She should be able to deal with ambiguity.\\nExperience in life insurance preferred but not mandatory.\\nKey Responsibilities & Skillsets:\\nCommon Skillsets:Superior analytical and problem-solving skills.\\nShould be able to work on a problem independently and prepare client ready deliverable with minimal or no supervision.\\nGood communication skill for client interaction\\nApplication development Skillsets:\\nAbility to debug, optimize code, and automate routine tasks.\\nA systematic problem-solving approach coupled with a strong sense of ownership and drive.\\nAbility to quickly pickup and understand where newly released cloud services would be appropriate for business applications.\\nExperience with infrastructure automation tools such as Puppet, Ansible, CloudFormation, or Terraform\\nWorking knowledge of pipeline-automation tools such as Jenkins, CodePipeline, Azure DevOps, or other comparable tools\\nExperience using Git for source control management.\\nAbility to proficiently write code in Python, Node.js, Bash (shell), PowerShell, or other similar languages.\\nExperience using Docker within container orchestration platforms such as AWS ECS, EKS, Google Anthos, or others\\nComfortable in a Linux environment\\nUnderstanding of foundational AWS services such as VPCs, EC2, S3, RDS, Auto Scaling Groups, CloudWatch Logs, etc.\\nIn-depth knowledge of security and IAM within AWS, including the management and operation of Security Groups, KMS Keys, VPC NACLs, and SCPs\\nFamiliar with ETL and big data tool-chains such as those provided by Hadoop/EMR, Glue, Spark, Impala, or similar\\nUnderstanding of relational database systems and how applications interact with them\\nFamiliarity with one or more log and event aggregation and monitoring systems such as Splunk, Elasticsearch (ELK), Prometheus, Grafana, or similar\\nCandidate Profile:\\nBachelor’s/Master's degree in computer science/engineering, operations research or related analytics areas welcome to apply\\n5+ years’ experience in AWS Data Engineering, preferably in Life Insurance but not mandatory\\nApplication development experience\\nTeam Management\\nSuperior analytical and problem solving skills\\nOutstanding written and verbal communication skills\\nAble to work in fast pace continuously evolving environment and ready to take up uphill challenges\\nIs able to understand cross cultural differences and can work with clients across the globe\\nJob Type: Full-time\\nSalary: Up to $130,000.00 per year\\nExperience level:\\n10 years\\n11+ years\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: On the road\\nSpeak with the employer\\n+91 9311265465\",\n",
       "  'JobSalary': 'Employer Provided Salary:$130K',\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '51 to 200 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2005',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': 'Genesis10\\n3.9',\n",
       "  'JobTitle': 'Data Center Technical Operations Engineer I - Sterling, VA',\n",
       "  'JobLocation': 'Sterling, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'New Journey AI/Genesis10 is actively seeking a resource for a 6 month Contract position with possible extension or conversion.\\n\\nResponsibilities:\\nResponsible for the on-site management of shift technicians, senior shift technicians, sub-contractors and vendors, ensuring that all work performed is in accordance with established practices and procedures.\\nEstablish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance.\\nWork with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency.\\nOperate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc.\\nMay assist in the design and build out of new facilities.\\nMay assist in projects to increase current facility efficiency.\\nResponsible for asset and inventory management.\\nAssist in recruiting efforts\\nDeliver quality service and ensure all customer demands are met\\n\\nBasic Qualifications:\\nBachelor’s Degree or Technical (Military/ Trade School) Degree and relevant experience.\\n2-4 years of relevant work experience.\\n2-4 years of management experience.\\nStrong verbal and written communication skills.\\nStrong leadership and organizational skills.\\nStrong attention to detail.\\nAbility to prioritize in complex, fast-paced environment.\\nPreferred Qualifications:\\n2-4 years of Data Center Engineering Experience\\n2-4 years of Data Center Management Experience\\nBachelor’s Degree in Electrical Engineering, Mechanical Engineering or relevant discipline.\\nFundamental knowledge of network design and layout as well as low voltage (copper/ fiber) cab\\nHVAC technician experience\\nElectrician experience\\nNuclear power experience\\nMaintenance experience in large corporation (ex. Hotel)\\nOnly candidates available and ready to work directly as NJAI/Genesis10 employees will be considered for this position.\\n\\nCompensation: $37.00 per hour\\n\\nIf you have the described qualifications and are interested in this exciting opportunity, apply today!\\n\\nAbout New Journey/Genesis10:\\nNew Journey, a Genesis10 company, is a leader in staffing, providing opportunities in light industrial, finance/accounting, financial services, human resources, data, administrative, autonomous vehicles, business operations, and legal, amongst others. These opportunities provide professional growth with direct-hire, contract, & contract-to-hire roles at Fortune 1000 and mid-market companies.\\n\\nBenefits of working with New Journey include:\\nWeekly pay\\nMedical, Dental, Vision\\nBehavioral Health Platform\\nHealth Savings Account\\nVoluntary Term Life Insurance\\nVoluntary Hospital Indemnity (Critical Illness & Accident)\\n401K\\nSick Pay (for applicable states/municipalities\\nCommuter Benefits (Dallas, NYC, SF)\\nOur team of experienced recruiters can help you find the ideal job to help you build your career. We care about people. We care about you. To learn more and to view all of our available career opportunities, please find us by searching \"New Journey AI.\\n\\nNew Journey is an Equal Opportunity Employer. Candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\\nGenesis10-74123698',\n",
       "  'JobSalary': 'Employer Provided Salary:$37.00 Per Hour',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Management & Consulting',\n",
       "  'CompanyYearFounded': '1999',\n",
       "  'CompanyIndustry': 'Business Consulting',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'Palo Alto Networks\\n4.3',\n",
       "  'JobTitle': 'Principal Cloud Data Engineer (Prisma Access)',\n",
       "  'JobLocation': 'Santa Clara, CA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Company Description\\n\\nOur Mission\\nAt Palo Alto Networks® everything starts and ends with our mission:\\nBeing the cybersecurity partner of choice, protecting our digital way of life.\\nOur vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.\\nFLEXWORK is an employee-centric reimagining of how we work. We built FLEXWORK based on employee feedback – it is about flexibility, trust, and choice whenever possible. It’s been a journey of disruption that has yielded the best of our values. We offer as much flexibility as possible, and choices that enable you to be most productive, including benefits that meet your needs and learning opportunities that you feel passionate about.\\n\\nJob Description\\n\\nYour Career\\nPrisma Access™ (formally GlobalProtect Cloud Service) provides protection straight from the cloud to make access to the cloud secure. It combines the connectivity and security you need - and delivers it everywhere you need it. Using cutting-edge public and private cloud technologies extending the next-generation security protection to all cloud services, customers on-premise remote networks and mobile users.\\nWe are seeking an experienced Data Software Engineer to design, develop and deliver next-generation technologies within our Prisma Access team. We want passionate engineers who love to code and build great products. Engineers who bring new ideas in all facets of software development. Collaboration and teamwork are at the foundation of our culture and we need engineers who can communicate and work well with others towards achieving a common goal.\\nAt Palo Alto Networks, leaders of engineering are -\\nTechnical experts and thought leaders that help accelerate the adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends, and practices\\nVisionaries who help deliver on critical business needs and are recognized across the company as go-to engineering resources on given domains\\nRole models and mentors who exemplify the best of Palo alto Networks culture - They do the right thing even when it’s hard, treat challenges as a chance to learn, and provide honest opinions so the team can improve\\nLeaders who can communicate cogently with hands-on engineers as well as executives\\nWe expect office-based employees to be in the office four days per week, with one day working from where they choose. We believe being together facilitates casual conversations and those magic moments where we can work on issues and ideas informally. These moments build capability and deepen trusted relationships and allow our people to feel safe in taking risks and being disruptive. Like so many companies, we are working through the details and things could change …. but in general if a role is deemed office-based we want our teams to be together four days per week.\\nYour Impact\\nDesign, develop and implement highly scalable software features on our next-generation security platform as part of our Prisma Access\\nWork with different development and quality assurances groups to achieve the best quality\\nSuggest and implement improvements to the development process\\nWork with DevOps and the Technical Support teams to troubleshoot customer issues\\n\\nQualifications\\n\\nYour Experience\\n10+ years of development experience\\nExperience to developing services in the cloud/Kubernetes\\nExperience with building data pipelines and analytics pipelines using like dataflow, pubsub, GKE\\nStrong understanding of message queuing, stream processing, and highly scalable ‘big data’ data stores\\nExperience with RESTful interfaces and Build Management tools (Gradle, maven)\\nExperience in continuous integration and design\\nExperience with Test-Driven Development\\nExperience with distributed computing and object-oriented design and analysis\\nStrong understanding of microservices-based deployments with the ability to design services\\nShowcase your ability of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, peer review, and operations\\nFamiliarity with Agile (e.g., Scrum Process)\\nFamiliarity in Big Data technologies like Hive, Kafka, Hadoop, SQL, developing APIs\\nFamiliarity working with GCP or other Cloud platforms such as AWS and Azure\\nHigh energy and the ability to work in a fast-paced environment with a can-do attitude\\nEnjoys working with many different teams with strong collaboration and communications skills\\nFast learner and eager to absorb new emerging technologies\\nM.S./B.S. degree in Computer Science or Electrical Engineering or equivalent military experience required\\n\\nAdditional Information\\n\\nThe Team\\nWe are on a mission to build the industry's best Security large language model.\\nOur engineering team is at the core of our products – connected directly to the mission of preventing cyberattacks. We are constantly innovating – challenging the way we, and the industry, think about cybersecurity. Our engineers don’t shy away from building products to solve problems no one has pursued before.\\nWe define the industry, instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.\\n\\nOur Commitment\\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.\\nWe are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\\nPalo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.\\nAll your information will be kept confidential according to EEO guidelines.\\nThe compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/com-missioned roles) is expected to be between $140,100/yr to $226,600/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here.\",\n",
       "  'JobSalary': 'Employer Provided Salary:$140K - $227K',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2005',\n",
       "  'CompanyIndustry': 'Enterprise Software & Network Solutions',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'METROHEALTH SOUTH CAMPUS\\n3.9',\n",
       "  'JobTitle': 'Engineer - Server - Information Services Data Center',\n",
       "  'JobLocation': 'Cleveland, OH',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Location: MetroHealth Old Brooklyn Campus\\nBiweekly Hours: 80.00\\nShift: Mon-Fri 8-4:30\\n\\nThe MetroHealth System is redefining health care by going beyond medical treatment to improve the foundations of community health and well-being: affordable housing, a cleaner environment, economic opportunity and access to fresh food, convenient transportation, legal help and other services. The system strives to become as good at preventing disease as it is at treating it. Founded in 1837, Cuyahoga County’s safety-net health system operates four hospitals, four emergency departments and more than 20 health centers.\\n\\nS ummary:\\nPlans, designs and builds networks and/or computer systems for the MetroHealth System. Researches the latest technology as it applies to the MetroHealth infrastructure and technological potential. Monitors all networks and systems for maximum usage and performance. Evaluates vendor proposals and vendor solutions. Insures enterprise-wide computer and network security. Upholds the mission, vision, values, and customer service standards of The MetroHealth System.\\n\\nQualifications:\\nRequired Bachelor’s Degree in Engineering, Computer Science or related discipline. May substitute work experience if deemed of adequate complexity and responsibility. Five years of experience with Microsoft Active Directory 2008/2012 with three or more years of experience in design and/or implementation and support. Three or more years of experience with Windows 2008/2012 Server and Linux servers in a highly complex, multi-site, client server environment. Three or more years of experience with Microsoft System Center tools in a highly complex, multisite, client server environment. Server Virtualization experience with three or more ears in VMWare/HypeV design and/or implementation (minimally utilizing EMC and Brocade Switches) and implementation with Microsoft and/or Linux servers. Experience with and ability to program in one or more of the following: C#, HTML, PowerShell (other languages may substitute per Director/Manager). Ability to lead technical projects involving staff from multiple disciplines. Preferred Experience with EPIC, Citrix, Xen App Server, Netscaler, SNMP network monitoring, thin clients, databases such as Oracle, Microsoft SQL Server, TCP/IP and other network protocols. MCSE, MCSA Certifications. Physical Demands: May need to move around intermittently during the day, including sitting, standing, stooping, bending, and ambulating. May need to remain still for extended periods, including sitting and standing. Ability to communicate in face-to-face, phone, email, and other communications. Ability to read job related documents. Ability to use computer.',\n",
       "  'JobSalary': '$54K - $80K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Hospital',\n",
       "  'CompanySector': 'Healthcare',\n",
       "  'CompanyYearFounded': '1837',\n",
       "  'CompanyIndustry': 'Health Care Services & Hospitals',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'PubMatic\\n4.3',\n",
       "  'JobTitle': 'Senior Software Engineer (Data Analytics / Big Data Engineer )',\n",
       "  'JobLocation': 'Redwood City, CA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Company Description\\n\\nPubMatic (Nasdaq: PUBM) is an independent technology company maximizing customer value by delivering digital advertising’s supply chain of the future.\\nPubMatic’s sell-side platform empowers the world’s leading digital content creators across the open internet to control access to their inventory and increase monetization by enabling marketers to drive return on investment and reach addressable audiences across ad formats and devices.\\nSince 2006, our infrastructure-driven approach has allowed for the efficient processing and utilization of data in real time. By delivering scalable and flexible programmatic innovation, we improve outcomes for our customers while championing a vibrant and transparent digital advertising supply chain.\\n\\nJob Description\\n\\nOur Advertising technology team is building a holistic advertising platform which will be key to e-commerce company’s ad revenue growth strategy. We will build cutting edge machine learning and optimization algorithms to ingest, model and analyze online and in-store data from e-commerce companies. Importantly, we will build smart systems that deliver relevant retail ads and experiences that connect customers with the brands and products they love and enable advertisers to achieve their product sales goals via campaign activation.\\nThe Senior Software Engineer will be responsible for developing end-to-end product features for reporting system for advertising platform for e-commerce clients, that would have big data processing and will include the design and development of Analytics components.\\nWe are looking for self-motivated senior software engineers who enjoy working in dynamic, agile development environments as individual contributors to the team. The ideal candidate is a self-motivated problem solver with a strong background in big data tech stack, software design, development, and e-commerce AdTech domain.\\nJob Location: US (RWC/NYC)\\nResponsibilities:\\nBuild, design and implement our highly scalable, fault-tolerant, highly available big data platform to process terabytes of data and provide customers with in-depth analytics.\\nDeveloping Big Data pipelines using modern technology stack such as Spark, Hadoop, Kafka, HBase, Hive, Presto etc.\\nDeveloping analytics application ground up using modern technology stack such as Java, Spring, Tomcat, Jenkins, REST APIs, JDBC, Amazon Web Services, Hibernate.\\nBuilding data pipeline to automate high-volume data collection and processing to provide real-time data analytics.\\nCustomize PubMatic’s reporting and analytics platform based on customer’s requirements from customers and deliver scalable, production-ready solutions.\\nLead multiple projects to develop features for data processing and reporting platform, collaborate with product managers, cross-functional teams, other stakeholders and ensure successful delivery of projects.\\nUse various mechanisms established to fetch data from different external data sources and reconcile them with PubMatic’s processed data.\\nCollaborate with functional teams to build products to deliver end-to-end products and features and fix bugs for better performance.\\nDevelop robust & fault-tolerant systems and monitor implications of changes on data processing pipeline and performance.\\nLeveraging a broad range of PubMatic’s data architecture strategies and proposing both data flows and storage solutions.\\nManaging hadoop map reduce and spark jobs & solving any ongoing issues with operating the cluster.\\nWorking closely with cross functional teams on improving availability and scalability of large data platform and functionality of PubMatic software.\\nExpertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, performing code reviews, committing to Github, preparing documents in Confluence, continuous delivery using Jenkins, automated testing, and operations.\\nParticipate in Agile/Scrum processes such as sprint planning, sprint retrospective, backlog grooming, user story management, work item prioritization, etc.\\nFrequently discuss with product managers about the software features to include in PubMatic Data Analytics platform. Understand the technical aspects customer requirement from product managers.\\nKeep in regular touch with quality engineering team which ensure the quality of the platforms/products and performance SLAs of java based micro services and spark-based data pipeline.\\nSupport customer issues over email or JIRA (bug tracking system), provide updates, patches to customers to fix the issues.\\nDiscuss with technical writing team about the technical documents that are published on documentation portal.\\nPerform code and design reviews for code implemented by peers or as per the code review process.\\n\\nQualifications\\n\\n3+ years coding experience in Java.\\nExperience in implementing closed loop reporting for sponsored product listing campaigns for e-commerce companies.\\nBuilding analytics pipes for product search relevance modeling and click prediction\\nExperience in implementing reporting for display/video/native Ad campaigns for e-commerce companies.\\nSolid computer science fundamentals including data structure and algorithm design, and creation of architectural specifications.\\nExpertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, documentation, build processes, automated testing, and operations.\\nA passion for developing and maintaining a high-quality code and test base and enabling contributions from engineers across the team.\\nExpertise in big data technologies like Hadoop, Spark, Kafka, HBase etc would be an added advantage.\\nExperience in developing and delivering large-scale big-data pipelines, real-time systems & data warehouses would be preferred.\\nDemonstrated ability to achieve stretch goals in a very innovative and fast paced environment.\\nDemonstrated ability to learn new technologies quickly and independently.\\nExcellent verbal and written communication skills, especially in technical communications.\\nStrong interpersonal skills and a desire to work collaboratively.\\nBase Compensation Range: $120,000 - $160,000\\nIn accordance with applicable law, the above salary range provided is PubMatic’s reasonable estimate of the base salary for this role. The actual amount may vary, based on non-discriminatory factors such as location, experience, knowledge, skills and abilities. In addition to salary PubMatic also offers a bonus, restricted stock units and a competitive benefits package.\\n#LI-KS2\\n\\nAdditional Information\\n\\nReturn to Office: PubMatic employees throughout the global have returned to our offices via a hybrid work schedule (3 days “in office” and 2 days “working remotely”) that is intended to maximize collaboration, innovation, and productivity among teams and across functions. All PubMatic employees in the US and India are required to be fully vaccinated to return to our offices. Covid-19 boosters are not required at this point in time.\\nBenefits: Our benefits package includes the best of what leading organizations provide such as, paid leave programs, paid holidays, healthcare, dental and vision insurance, disability and life insurance, commuter benefits, physical and financial wellness programs, unlimited DTO in the US (that we actually require you to use!), reimbursement for mobile and internet expenses and fully stocked pantries plus in-office catered lunches 3 days per week.\\nDiversity and Inclusion: PubMatic is proud to be an equal opportunity employer; we don’t just value diversity, we promote and celebrate it. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.',\n",
       "  'JobSalary': 'Employer Provided Salary:$120K - $160K',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2006',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'AVP - Data Engineer Lead',\n",
       "  'JobLocation': 'Fort Mill, SC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.\\nJob Description\\nWe are currently looking to hire a tech lead (Technical Lead, AVP) within the Enterprise Data and Information Services team, which is part of LPL’s Technology organization. This position is responsible for contributing to a team focused on delivering the next generation cloud-based data platform.\\nResponsibilities:\\nLead development user stories for integration of data into cloud-native storage\\nAdhere to industry standards and methodologies for software development\\nFollow and provide feedback to technical guidance on standards, design patterns, architectural patterns and frameworks for services and integrations\\nCollaborate with analysts, modelers and subject matter experts\\nCreate system and architecture design documents to review with senior management and Enterprise Architecture teams\\nImplement best practices for cloud designs, service selection, and coding best practices\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nBachelor’s degree and/or relevant experience in IT related software development or support\\n7-10 years’ experience with developing on a major cloud computing provider (AWS, API Gateway, .Net Core, Docker) PostgreSQL, Kafka, SQLServer,\\nKnowledgeable of middleware and API patterns (GRPC and REST)\\nExperience with integration with message frameworks (ex: TIBCO EMS, Kafka)\\nExperience with DevOps toolchain that enables CI/CD pipeline (e.g., Git, TFS, Jenkins, TeamCity, Octopus, Puppet)\\nCore Competencies:\\nStrong problem-solving skills (e.g., root cause analysis) to debug or improve existing systems\\nStrong project leadership skills to manage Agile team to deliver product features\\nPreferences:\\nFinancial services industry experience is a plus\\nExperience with design and development of RESTful web services using languages such as Java, .NET, Golang or Python (ex: ASP.NET Core in .NET 5)\\nExperience with documenting RESTful web services with OpenAPI specification\\nKnowledgeable of API security best practices (e.g., TLS, CORS)\\nExperience with functional (e.g., Cucumber) or performance (e.g., JMeter, BlazeMeter) testing tools/frameworks\\nExperience with Agile software development methodologies\\n\\nPay Range:\\n$132,160-$198,240/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$132K - $198K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'ALTA IT Services\\n4.4',\n",
       "  'JobTitle': 'Data Engineer (Sr. and Mid)',\n",
       "  'JobLocation': 'Alexandria, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Title: Sr. & Mid Data Engineer ( w/ active Secret clearance) Location: hybrid in office / remote: 2-3 days a week on-site in DC metro area ( Arlington / DC) Security clearance needed: TS/SCI Compensation range: Open, based on extent of position-relevant experience *** FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly: TEXT: ( 240-601-8546) OR CALL: ( 301-212-7355) ALTA IT Services is seeking a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform. Responsibilities As a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members. Required Skills: Clearance: Secret 8+ years of experience with SQL 8+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets. 8+ years of experience with a modern programming language such as Python or Java 8 years of experience working in a big data and cloud environment Experience with distributed computer understanding and experience with SQL, Spark, ETL. Documented experience with AWS, EC2, S3, and/or RDS Preferred Skills: 4 years of experience working in an agile development environment Ability to quickly learn technical concepts and communicate with multiple functional groups Ability to display a positive, can-do attitude to solve the challenges of tomorrow Possession of excellent verbal and written communication skills Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly: TEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\\nTitle: Sr. & Mid Data Engineer ( w/ active Secret clearance)\\nLocation: hybrid in office / remote: 2-3 days a week on-site in DC metro area ( Arlington / DC)\\nSecurity clearance needed: TS/SCI\\nCompensation range: Open, based on extent of position-relevant experience\\n\\n*** FOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly:\\nTEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\\n\\nALTA IT Services is seeking a Data Engineer to join our team of experts to assist with building state of the art data platforms for the Department of Defense's premier data analytics platform.\\nResponsibilities\\nAs a Data Engineer, this role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. The best athlete candidate for this position will be able to apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems. This candidate is able to work without considerable direction and may mentor or supervise other team members.\\nRequired Skills:\\nClearance: Secret\\n8+ years of experience with SQL\\n8+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.\\n8+ years of experience with a modern programming language such as Python or Java\\n8 years of experience working in a big data and cloud environment\\nExperience with distributed computer understanding and experience with SQL, Spark, ETL.\\nDocumented experience with AWS, EC2, S3, and/or RDS\\nPreferred Skills:\\n4 years of experience working in an agile development environment\\nAbility to quickly learn technical concepts and communicate with multiple functional groups\\nAbility to display a positive, can-do attitude to solve the challenges of tomorrow\\nPossession of excellent verbal and written communication skills\\nPreferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes\\n\\nFOR IMMEDIATE CONSIDERATION, please call and/or text Adam directly:\\nTEXT: ( 240-601-8546) OR CALL: ( 301-212-7355)\",\n",
       "  'JobSalary': '$76K - $116K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$5 to $25 million (USD)'},\n",
       " {'CompanyName': 'Wells Fargo\\n3.7',\n",
       "  'JobTitle': 'Fraud Data Engineer',\n",
       "  'JobLocation': 'McLean, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"About this role:\\n\\nWe are looking for a Data Engineer to Join the Fraud AI Platform team. The Fraud AI Platform team is a newly created team at the bank that is building a new Data Science platform to solve real world fraud problems and help reduce our fraud loss year over year. Members of this team will be coming in at the ground floor, with the ability to help drive the vision, design, and implementation of the new platform. Team members will have the freedom to innovate and deliver real capabilities back to our bank, saving our customers time and money as we reduce fraudulent activity. Furthermore, this team is working on the cutting edge of data science, making it a very exciting environment to continue to build out a career with many opportunities for excellence and recognition.\\n\\nThe Data engineer will be responsible for working with our various data platforms, understanding supporting a team of data scientists and analysts. The Data engineer will work on data modeling in a graph database, support ETL on data ingestion, serve as a consultant to data scientists on data issues. The individual will be working across a data intake team in the fraud platform.\\n\\nIn this role, you will:\\nLead moderately complex initiatives within Technology and contribute to large scale data processing framework initiatives related to enterprise strategy deliverables\\nBuild and maintain optimized and highly available data pipelines that facilitate deeper analysis and reporting\\nReview and analyze moderately complex business, operational or technical challenges that require an in-depth evaluation of variable factors\\nOversee the data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis\\nResolve moderately complex issues and lead teams to meet data engineering deliverables while leveraging solid understanding of data information policies, procedures and compliance requirements\\nCollaborate and consult with colleagues and managers to resolve data engineering issues and achieve strategic goals\\nRequired Qualifications, US:\\n1+ Years of experience with Fraud data supporting FI Fraud, Cyber, or AML use cases\\n4+ Years using scripting language such as python, javascript, or shell scripting to process, clean, and prepare data\\n3+ years with ingesting and extracting data from large data store\\n2+ years with SQL\\n4+ years of Data Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\\nRequired Qualifications, International:\\nExperience in Data Engineering, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\\nDesired Qualifications:\\n1+ Year working with a graph database such as neo4J, ArangoDB, Tigergraph\\nExperience working with Apache Spark\\nExperience with Elasticsearch\\nWe Value Diversity\\n\\nAt Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.\\n\\nEmployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.\\n\\nCandidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\\n\\nCandidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process.\\n\\nDrug and Alcohol Policy\\n\\nWells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1852',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Locus Recruiting\\n4.0',\n",
       "  'JobTitle': 'Data Center Engineer (Structured Cabling)',\n",
       "  'JobLocation': 'United States',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Locus (Recruitment Firm) is recruiting for a Data Center (Structured Cabling) Engineers for our client.\\nPosition: Data Center (Structured Cabling) Engineer\\nDuration: 6 month contract to hire (Full-time work)\\nPay: $50-60/hr. ($100-$125,000 conversion)\\nLocation: United states\\nOvernight Shift (11pm-7am)\\nTravel 75% + each week (leave Sunday evening-return Thursday or Friday)\\nExpenses Reimbursed (Hotel, Meals, etc.)\\n40 hours a week\\nHave a need to hire 6 individuals-2 Western US, 2 Central US and 2 Eastern US\\nSkills:\\n· 3rd Shift to cover Datacenters and Critical facilities at airports while flights are not flying.\\n· Doing remediation work\\n· Following a run book\\n· Taking 12 largest airports, re-cabling and moving racks forward, putting in AC, separate power\\n· Worked in Data centers, rack and stack at airports\\n· Low voltage electrician backgrounds work is great, fibers/cabling experience\\nJob Types: Full-time, Contract\\nPay: $50.00 - $60.00 per hour\\nBenefits:\\nDental insurance\\nHealth insurance\\nVision insurance\\nSchedule:\\n8 hour shift\\nEvening shift\\nMonday to Friday\\nNight shift\\nWeekends as needed\\nApplication Question(s):\\nAre you open to a 6-month contract-to-hire position working 40 hours a week?\\nAre you comfortable traveling Sunday-Thursday each week?\\nAre you comfortable working 11pm-7am?\\nAre the pay requirements in line with what you are looking for?\\nDo you have any experience working within the Airline Industry?\\nExperience:\\nData center: 5 years (Preferred)\\nRack and Stack: 5 years (Preferred)\\nCabling: 5 years (Preferred)\\nLow Voltage: 5 years (Preferred)\\nShift availability:\\nOvernight Shift (Required)\\nWillingness to travel:\\n75% (Required)\\nWork Location: On the road',\n",
       "  'JobSalary': 'Employer Provided Salary:$50.00 - $60.00 Per Hour',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Enterprise Software & Network Solutions',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Honda Development and Manufacturing of America, LLC.\\n3.4',\n",
       "  'JobTitle': 'Mass Production Data Process Engineer',\n",
       "  'JobLocation': 'East Liberty, OH',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Mass Production Data Process Engineer\\nLocation: East Liberty, Ohio\\nWorkstyle: Onsite\\nWhat Makes a Honda, is Who makes a Honda\\nHonda has a clear vision for the future, and it’s a joyful one. We are looking for individuals with the skills, courage, persistence, and dreams that will help us reach our future-focused goals.\\nAt our core is innovation. Honda is constantly innovating and developing solutions to drive our business with record success. We strive to be a company which serves as a source of “power” that supports people around the world who are trying to do things based on their own initiative and that helps people expand their own potential. To this end, Honda strives to realize “the joy and freedom of mobility” by developing new technologies and an innovative approach to achieve a “zero environmental footprint.”\\nWe are looking for qualified individuals with diverse backgrounds, experiences, continuous improvement values, and a strong work ethic to join our team.\\nIf your goals and values align with Honda’s, we want you to join our team to Bring the Future!\\nAbout this Position:\\nProcess Data Engineer will utilize engineering methods to engage in continuous improvement activities, while improving SQDCM characteristics. Process Data Engineer will use analytical tools to optimize assembly line production functions.\\nResponsibilities include:\\nAudit big data to ensure accurate information across all operation standards. Utilize Excel and VBA to optimize tasks.\\nIdentify opportunities to reduce waste within the process and improve efficiency. Apply 5S work methods, workstation design, and set best practices for production.\\nUse M.O.S.T. To develop balanced processes across manufacturing areas. Work to achieve high process efficiency. Apply lean manufacturing techniques. Design processes that are simple and achieve SQCDM targets.\\nSupport New Model events and forecast design impact to department. Identify concerns and countermeasures before mass production.\\nRegularly lead meetings with other Honda functional groups to develop and launch improvement themes (quality, safety, delivery, production, purchasing, packaging, or logistics.\\nDevelop plans and schedules for projects. Set timelines and detailed activities. Hold meetings with cross functional engineering groups to ensure completeness.\\nComplete studies to understand project/plant characteristic impacts and perform time estimates for assembly line work, equipment loads, and develop operation standards.\\nWork with counterparts in other NA plants to understand system change points.\\nRoot cause identification and countermeasure for complex problems.\\nWho we are seeking:\\nRequired Experience:\\nPrefer intern or co-op experience (1-2 terms) or relevant work experience of two years.\\nRequired Education:\\nBachelor’s degree in Engineering(Mechanical, Manufacturing, Industrial or Systems) or equivalent relevant experience.\\nOther job-specific skills:\\nRoot cause analysis, strong communication skills, collaboration with teams, and self-driven, strong continuous improvement mindset; MS office; problem solving/decision making skills; project management capabilities\\nAdditional Position Factors:\\nWorkstyle: Onsite\\nTravel: 5%\\nAt Honda, you will play a key role in our journey to become a company that society wants to exist now, and in the future. Your endless curiosity will drive innovation and your courageous spirit will challenge the status quo. We believe having a workforce made up of diverse thinkers and innovators makes us a better Honda. Respect for each other and respect for diversity each day drives our associates to contribute at the highest level and work effectively in a team environment. We make the dream of mobility a reality with our innovative and high-quality products. Together, we Bring the Future to our customers, associates, and communities. We are Honda!\\nWhat differentiates Honda and make us an employer of choice?\\nTotal Rewards:\\nCompetitive Base Salary\\nAnnual Bonus\\nOvertime Pay\\nIndustry-leading Benefit Plans (Medical, Dental, Vision, Rx)\\nPaid time off, including vacation, paid holidays, sick time, and personal days\\n401K Plan with company match + additional contribution\\nRelocation assistance (if eligible)\\nCareer Growth:\\nAdvancement opportunities\\nCareer mobility\\nEducation reimbursement for continued learning\\nTraining and Development programs\\nAdditional Offerings:\\nWellbeing program\\nCommunity service and engagement programs\\nProduct programs\\nFree drinks onsite\\nHonda is an equal opportunity employer and considers qualified applicants for employment without regard to race, color, creed, religion, national origin, sex, sexual orientation, gender identity and expression, age, disability, veteran status, or any other protected factor.',\n",
       "  'JobSalary': '$56K - $80K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.4',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Vehicle Dealers',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Subsidiary or Business Segment',\n",
       "  'CompanyIndustry': 'Retail & Wholesale',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Deloitte\\n4.0',\n",
       "  'JobTitle': 'Cloud Data Engineer - Healthcare',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Are you an experienced, passionate pioneer in technology who wants to work in a collaborative environment? As an experienced Cloud Data Engineer - Healthcare you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. This position is working on a multi-year project for a major healthcare client. This is a remote role.\\n\\nWork you'll do/Responsibilities\\n\\nYou will determine processes and automation tools to reduce IT spend and increase efficiencies on multiple projects within the Healthcare domain.\\n\\nThis position includes collaborating with DevOps teams to implement CI/CD pipelines, automated deployments, and infrastructure as code (IaC) practices for AWS-based solutions. Document design, development, and deployment processes, as well as create technical specifications and user guides for developed solutions.\\n\\nYour role will be to design, develop, and deploy cloud-based solutions for data processing, analytics, and integration using cloud services and big data technologies. Collaborate with architects, data engineers, and business stakeholders to understand requirements and translate them into technical solutions.\\n\\nYou will implement data ingestion, transformation, and storage processes using cloud services like AWS's S3, Glue, Athena, Redshift, and EMR. Implement security, data governance, and compliance measures to ensure data integrity and protection in AWS-based solutions. Develop and optimize data pipelines using Snowpark, SnowSQL, Hadoop and PySpark to extract, transform, and load data efficiently.\\n\\nYou will conduct performance tuning and optimization of data processing and analytics workflows to maximize efficiency and scalability. Work with cross-functional teams to troubleshoot and resolve issues related to data processing, data integration, and analytics solutions.\\n\\nCommunicate regularly with Engagement Managers (Directors), project team members, and representatives from various functional and / or technical teams, including escalating any matters that require additional attention and consideration from engagement management\\n\\nThe Team\\n\\nAs a part of the US Strategy & Analytics Offering Portfolio, the AI & Data Operations offering provides managed AI, Intelligent Automation, and Data DevOps services across the advise-implement-operate spectrum.\\n\\nQualifications\\n\\nRequired\\n\\n5+ years' experience as a Cloud Data Engineer\\n\\n5+ years' hands on experience in Snowpark, SnowSQL, Hadoop and PySpark\\n\\n5+ years' experience in AWS services such as S3, Glue, Athena, Redshift, EMR, Lambda and Cloud Formation.\\n\\n5+ years' experience in Python with a focus on data processing and analytics\\n\\n5+ years in healthcare domain\\n\\n5+ years in consulting\\n\\nStrong knowledge and hands-on experience in designing, developing, and deploying scalable solutions on the cloud platforms\\n\\nExpertise in SQL and database technologies for data manipulation and querying\\n\\nBachelor's degree or equivalent experience\\n\\nLimited immigration sponsorship may be available\\n\\nPreferred\\n\\nFamiliarity with data modeling, data warehousing, and data integration concepts.\\n\\nExperience with DevOps practices, CI/CD pipelines, and infrastructure as code (IAAC) using tools like Jenkins, Git, and Terraform.\\n\\nStrong analytical and problem-solving skills, with the ability to troubleshoot and resolve complex technical issues.\\n\\nFamiliarity with agile development methodologies and experience working in Agile teams\\n\\nAbility to travel 10%, on average, based on the work you do and the clients and industries/sectors you serve\\n\\nBachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience\\n\\nAnalytical/ decision making responsibilities\\n\\nAnalytical ability to manage multiple projects and prioritize tasks into manageable work products\\n\\nCan operate independently or with minimum supervision\\n\\nExcellent communication skills\\n\\nAbility to deliver technical demonstrations\",\n",
       "  'JobSalary': '$88K - $118K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1850',\n",
       "  'CompanyIndustry': 'Accounting & Tax',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'AT&T\\n3.7',\n",
       "  'JobTitle': 'Senior-Big Data Software Engineer',\n",
       "  'JobLocation': 'Plano, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Join AT&T and reimagine the communications and technologies that connect the world. We’re committed to those who seek to discover the undiscoverable and dare to disrupt the norm. Bring your bold ideas and fearless risk-taking to redefine connectivity and transform how the world shares stories and experiences that matter. When you step into a career with AT&T, you won’t just imagine the future – you’ll create it.\\nThe Senior Big Data Software Engineer provides a critical role for the success of our Chief Data Office (CDO) Network Intelligence Data Collect & AI Platforms and the AT&T Network Tracking (ANT) Transformation program. This role is key in supporting our azure cloud technologies that enable RAN, Customer Care, Legal Demands and Mobility business partners enabling various use cases like RAN Augmentation, IP Analytics, Billing optimization, StreamSaver analytics and Network planning.\\nKey Roles and Responsibilities\\nAs a Sr. Big Data Software Engineer, you will:\\nProduce batch and real time data pipelines for our network and NLP based use cases for our data scientists and other developers\\nDevelop deep domain expertise and opportunities to expand your role into additional facets of data insights including modeling, extensive cloud-based environment architecting, and software development.\\nAnalyze, design, program, debug and modify software enhancements and/or new products used in distributed, large-scale analytics and visualization solutions.\\nWork in a highly agile environment.\\nResponsible for the development of high performance, distributed computing tasks using Big Data technologies such as Hadoop, NoSQL, text mining and other distributed environment technologies in azure and on-prem.\\nHadoop Administration, Apache NiFi support and Databricks workspace Administration, supporting the Network Intelligence Data Collect & AI Team and their business partners.\\nResponsible for Automation of infrastructure using CICD/ansible or comparable technologies in Azure cloud.\\nSetting up secure vents and core infrastructure components in Azure. UNIX Administration.\\nSupporting the Network Intelligence Flowlogic Hadoop clusters & Databricks workspaces that collects the mobile packet data via the network probes\\nManaging the core ingestion NiFi servers and supporting the platforms teams with all things azure And responsible for Automation of infrastructure using CICD/ansible.\\nExpert Azure Security Engineer responsible for setting up secure vents and core infrastructure components in Azure. Addressing & supporting azure admin and Databricks related issues for various in house and offshore teams.\\nQualifications\\nBachelor of Science in Computer Science, Computer Engineering, or Scientific Computing preferred.\\nRequires 5 - 8 years of related experience.\\nRequires hands on experience using and administrating Databricks workspaces.\\nRequires experience with Azure cloud administration and subscription management.\\nStrong communication skills – communicating with all levels of management, understanding business requirements, and defining project roadmaps.\\nRequires on site presence 3-5 days a week in Dallas/Plano.\\nOur Big Data Software Engineers earn between $128,400-$192,600. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training.\\nJoining our team comes with amazing perks and benefits:\\nMedical/Dental/Vision coverage\\n401(k) plan\\nTuition reimbursement program\\nPaid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)\\nPaid Parental Leave\\nPaid Caregiver Leave\\nAdditional sick leave beyond what state and local law require may be available but is unprotected.\\nAdoption Reimbursement\\nDisability Benefits (short term and long term)\\nLife and Accidental Death Insurance\\nSupplemental benefit programs: critical illness/accident hospital indemnity/group legal\\nEmployee Assistance Programs (EAP)\\nExtensive employee wellness programs\\nEmployee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone\\nA career with us, a global leader in communications and technology, comes with big rewards. As part of our team, you’ll lead transformation surrounded by trailblazing industry leaders like you. You’ll be empowered to go above and beyond – making a difference through company-sponsored initiatives or connecting and networking through one of our many employee groups. And regardless of where you’re at in your career trajectory, you’ll be rewarded by the impact that comes with making a difference in the lives of millions. With AT&T, you’ll be a part of something greater, do incredible things and be rewarded with a chance to change the world.\\nApply now!',\n",
       "  'JobSalary': 'Employer Provided Salary:$128K - $193K',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Telecommunications',\n",
       "  'CompanyYearFounded': '1876',\n",
       "  'CompanyIndustry': 'Telecommunications Services',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Cambay Consulting LLC\\n4.3',\n",
       "  'JobTitle': 'AWS Data Engineer',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'This position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services.\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\nRESPONSIBILITIES\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\nREQUIREMENTS\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\nJob Type: Contract\\nSchedule:\\n8 hour shift\\nExperience:\\nAWS data services and technologies: 5 years (Preferred)\\nETL processes using AWS Glue, AWS Lambda, Apache Spark: 5 years (Preferred)\\nprogramming languages such as Python, Scala, or Java: 5 years (Preferred)\\nWork Location: Remote',\n",
       "  'JobSalary': '$78K - $107K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2011',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Computer Enterprises, Inc. (CEI)\\n3.6',\n",
       "  'JobTitle': 'Data/Software Engineer III',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"CEI's Fortune 20 client in Banking is seeking a highly skilled and experienced Software Developer III to join our team.\\nJob at a Glance:\\nLocation: 100% Remote (EST preferred)\\nWorking Hours: Monday to Friday, 8-5\\nContract Length: 6 months, w/ potential for hire/extension based on performance\\n*Corp2Corp not accepted\\nKey Responsibilities:\\nAttend daily meetings, sprint planning, and sprint assignments\\nTake responsibility for assigned work and provide regular updates\\nSupport new application functionality related to data and reporting needs\\nCollaborate with a project team to add new data feeds into applications as part of Data Warehouse projects\\nModify existing databases and database management systems\\nWrite and code logical and physical database descriptions\\nCoordinate database development within the project team\\nReview project requests and estimate time and cost requirements\\nTroubleshoot issues and make system changes as needed\\nPerform other duties as assigned\\nRequired Skills:\\nHigh School Degree and Bachelor's degree in computer science, software engineering, or a relevant field; or equivalent work experience\\n6-8 years of experience in IT development, analysis, information management, or QA\\nMust haves: CA7, data warehouse, Hive/Hadoop, Spark, Kafka\\nAdditional Skills (Preferred):\\nExperience in Financial Services\\nProficiency in XML, Java, JSP, and other relevant software\\nExperience with database development and management systems\\nAbility to troubleshoot issues and make system changes\\nExcellent verbal and written communication skills\\nAbility to work independently and manage time effectively\\nAs a trusted technology partner, CEI delivers solutions that help our customers transform their business and achieve meaningful results. From strategy and custom application development through application management - our technology and digital experience services are tailored to meet each unique need of our customers. Our staffing solutions bring specialized skills to complement our customers' workforce and project requirements.\\nJob Types: Full-time, Contract\\nPay: $65.00 per hour\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nVision insurance\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nDallas, TX 75225: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nCA7: 4 years (Required)\\nHive/Hadoop: 4 years (Required)\\nData Warehouse: 4 years (Required)\\nSpark: 4 years (Required)\\nKafka: 4 years (Required)\\nWork Location: Hybrid remote in Dallas, TX 75225\",\n",
       "  'JobSalary': 'Employer Provided Salary:$65.00 Per Hour',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1992',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Genesis10\\n3.9',\n",
       "  'JobTitle': 'Data Center Technical Operations Engineer I - Sterling, VA',\n",
       "  'JobLocation': 'Sterling, VA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'New Journey AI/Genesis10 is actively seeking a resource for a 6 month Contract position with possible extension or conversion.\\n\\nResponsibilities:\\nResponsible for the on-site management of shift technicians, senior shift technicians, sub-contractors and vendors, ensuring that all work performed is in accordance with established practices and procedures.\\nEstablish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance.\\nWork with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency.\\nOperate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc.\\nMay assist in the design and build out of new facilities.\\nMay assist in projects to increase current facility efficiency.\\nResponsible for asset and inventory management.\\nAssist in recruiting efforts\\nDeliver quality service and ensure all customer demands are met\\n\\nBasic Qualifications:\\nBachelor’s Degree or Technical (Military/ Trade School) Degree and relevant experience.\\n2-4 years of relevant work experience.\\n2-4 years of management experience.\\nStrong verbal and written communication skills.\\nStrong leadership and organizational skills.\\nStrong attention to detail.\\nAbility to prioritize in complex, fast-paced environment.\\nPreferred Qualifications:\\n2-4 years of Data Center Engineering Experience\\n2-4 years of Data Center Management Experience\\nBachelor’s Degree in Electrical Engineering, Mechanical Engineering or relevant discipline.\\nFundamental knowledge of network design and layout as well as low voltage (copper/ fiber) cab\\nHVAC technician experience\\nElectrician experience\\nNuclear power experience\\nMaintenance experience in large corporation (ex. Hotel)\\nOnly candidates available and ready to work directly as NJAI/Genesis10 employees will be considered for this position.\\n\\nCompensation: $37.00 per hour\\n\\nIf you have the described qualifications and are interested in this exciting opportunity, apply today!\\n\\nAbout New Journey/Genesis10:\\nNew Journey, a Genesis10 company, is a leader in staffing, providing opportunities in light industrial, finance/accounting, financial services, human resources, data, administrative, autonomous vehicles, business operations, and legal, amongst others. These opportunities provide professional growth with direct-hire, contract, & contract-to-hire roles at Fortune 1000 and mid-market companies.\\n\\nBenefits of working with New Journey include:\\nWeekly pay\\nMedical, Dental, Vision\\nBehavioral Health Platform\\nHealth Savings Account\\nVoluntary Term Life Insurance\\nVoluntary Hospital Indemnity (Critical Illness & Accident)\\n401K\\nSick Pay (for applicable states/municipalities\\nCommuter Benefits (Dallas, NYC, SF)\\nOur team of experienced recruiters can help you find the ideal job to help you build your career. We care about people. We care about you. To learn more and to view all of our available career opportunities, please find us by searching \"New Journey AI.\\n\\nNew Journey is an Equal Opportunity Employer. Candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\\nGenesis10-74123698',\n",
       "  'JobSalary': 'Employer Provided Salary:$37.00 Per Hour',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Management & Consulting',\n",
       "  'CompanyYearFounded': '1999',\n",
       "  'CompanyIndustry': 'Business Consulting',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'METROHEALTH SOUTH CAMPUS\\n3.9',\n",
       "  'JobTitle': 'Engineer - Server - Information Services Data Center',\n",
       "  'JobLocation': 'Cleveland, OH',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Location: MetroHealth Old Brooklyn Campus\\nBiweekly Hours: 80.00\\nShift: Mon-Fri 8-4:30\\n\\nThe MetroHealth System is redefining health care by going beyond medical treatment to improve the foundations of community health and well-being: affordable housing, a cleaner environment, economic opportunity and access to fresh food, convenient transportation, legal help and other services. The system strives to become as good at preventing disease as it is at treating it. Founded in 1837, Cuyahoga County’s safety-net health system operates four hospitals, four emergency departments and more than 20 health centers.\\n\\nS ummary:\\nPlans, designs and builds networks and/or computer systems for the MetroHealth System. Researches the latest technology as it applies to the MetroHealth infrastructure and technological potential. Monitors all networks and systems for maximum usage and performance. Evaluates vendor proposals and vendor solutions. Insures enterprise-wide computer and network security. Upholds the mission, vision, values, and customer service standards of The MetroHealth System.\\n\\nQualifications:\\nRequired Bachelor’s Degree in Engineering, Computer Science or related discipline. May substitute work experience if deemed of adequate complexity and responsibility. Five years of experience with Microsoft Active Directory 2008/2012 with three or more years of experience in design and/or implementation and support. Three or more years of experience with Windows 2008/2012 Server and Linux servers in a highly complex, multi-site, client server environment. Three or more years of experience with Microsoft System Center tools in a highly complex, multisite, client server environment. Server Virtualization experience with three or more ears in VMWare/HypeV design and/or implementation (minimally utilizing EMC and Brocade Switches) and implementation with Microsoft and/or Linux servers. Experience with and ability to program in one or more of the following: C#, HTML, PowerShell (other languages may substitute per Director/Manager). Ability to lead technical projects involving staff from multiple disciplines. Preferred Experience with EPIC, Citrix, Xen App Server, Netscaler, SNMP network monitoring, thin clients, databases such as Oracle, Microsoft SQL Server, TCP/IP and other network protocols. MCSE, MCSA Certifications. Physical Demands: May need to move around intermittently during the day, including sitting, standing, stooping, bending, and ambulating. May need to remain still for extended periods, including sitting and standing. Ability to communicate in face-to-face, phone, email, and other communications. Ability to read job related documents. Ability to use computer.',\n",
       "  'JobSalary': '$54K - $80K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Hospital',\n",
       "  'CompanySector': 'Healthcare',\n",
       "  'CompanyYearFounded': '1837',\n",
       "  'CompanyIndustry': 'Health Care Services & Hospitals',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Computer Enterprises, Inc. (CEI)\\n3.6',\n",
       "  'JobTitle': 'Data Engineer (Product, SQL, Tableau, Kibana) - W2 Only',\n",
       "  'JobLocation': 'Philadelphia, PA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"CEIs Fortune 50 Client is adding a Data Engineer to their team in Philadelphia.\\nPosition at a glance:\\nLong term contract with potential for permanent conversion\\nHybrid position, onsite in Philadelphia Tuesday, Wednesday, & Thursday with Monday & Friday remote\\nPre-budgeted pay rate of $54.00-60.00 / HR\\nThis role will be performing Data Reporting with a focus on product as well as working to improve workflow processes and dashboard building. This team specializes in product operation realms between products and end users.\\nThis person will need to be able to explain why they used a certain process or data technique in the work they're doing and why they're drilling down into certain areas of the data.\\nWill work with SQL, Tableau, Kibana, and APIs\\nProject:\\nTeam specializes in product ops realms between product and end users\\nThis role is data centric and a new role for the team\\nData governance, data reporting, APIs, and any other data items in that realm\\nSeek out areas of the data to drill down into. Requires someone who is vocal and will stand up for what they're working on and give the reason why\\nHave conversations at a technical and non-technical level\\nWill work on the data side of kibana and tableau (Not looking for someone who has lightly touched on the tools from a user side)\\nBuild queries and pull large data sets in SQL, build dashboards in Tableau\\nSkills:\\nAbility to independently work and identify issues without a ton of upfront direction, comfortable in a large meeting setting and identifying problems, stand up for why they are doing the work they’re doing\\nMust have Skills: Kibana, Tableau, SQL\\nYears of experience: 5+ years\\nAs a trusted technology partner, CEI delivers solutions that help our customers transform their business and achieve meaningful results. From strategy and custom application development through application management - our technology and digital experience services are tailored to meet each unique need of our customers. Our staffing solutions bring specialized skills to complement our customers' workforce and project requirements\\nJob Types: Full-time, Contract\\nPay: $54.00 - $60.00 per hour\\nBenefits:\\n401(k)\\n401(k) matching\\nDental insurance\\nHealth insurance\\nVision insurance\\nCompensation package:\\nHourly pay\\nExperience level:\\n5 years\\nSchedule:\\n8 hour shift\\nDay shift\\nApplication Question(s):\\n(Yes/No) Are you able to work on W2?\\nExperience:\\nTableau: 3 years (Preferred)\\nKibana: 1 year (Preferred)\\nData Engineer: 5 years (Preferred)\\nWork Location: Hybrid remote in Philadelphia, PA 19103\",\n",
       "  'JobSalary': 'Employer Provided Salary:$54.00 - $60.00 Per Hour',\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1992',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'Sr. Software Engineer - API/Data Services',\n",
       "  'JobLocation': 'Fort Mill, SC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients\\nJob Overview:\\nAt LPL Financial we consider it our mission to take care of our advisors so they can take care of their clients. Joining as Senior Software Engineer – API/Data Services, you will be responsible for building and maintaining sustainable, scalable products and capabilities that support our advisors to deliver excellent services for their investors.\\nResponsibilities:\\nDesign, develop, test, tune, and implement n-tiered web-based applications while collaborating with Development, Enterprise Architecture and Product teams\\nDeveloping and maintaining web applications, microservices, and RESTful APIs using Python and AWS services such as EKS, S3, RDS, Lambda, and API Gateway.\\nDesigning and implementing microservices-based architecture using Python and AWS.\\nAssess opportunities for application and process improvements and prepare SDLC documentation\\nMaintain, troubleshoot, optimize and enhance existing systems\\nWork collaboratively with QA, DevOPS teams to adopt CI/CD tool chain and develop automation\\nCommunicate with technical and non-technical groups on a regular basis as part of product/project support\\nDesign and develop core services and components with expertise in service-oriented architecture\\nDesign patterns and coding best practices.\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nProven experience as a Senior Python or C# Developer\\nExperience in developing RESTful Microservice-based APIs using Python web development frameworks such as Django, AWS Powertools, or Flask.\\nProficiency in using AWS services such as EKS, AWS Fargate, EC2, S3, RDS, Lambda, EventBridge, Glue, and API Gateway.\\nStrong unit testing and debugging skills.\\n5+ years of experience in designing and implementing complex systems accompanied by a Bachelor’s degree or Master Degree in Computer Science, Information Systems, Business, or other related field.\\nExcellent understanding and writing of SQL server procedures, views, functions and designing skills, caching and exception handling.\\nCore Competencies:\\nExcellent verbal and written communication skills, both technical and non-technical\\nStrong analytical and problem-solving skills\\nMotivated and driven by achieving long-term business outcomes\\nCapable of effectively planning, prioritizing and executing tasks utilizing resources and tools\\nPreferences:\\nExperience with Agile using JIRA\\nC#, .NET 6, ELK Stack, TeamCity, TFS, GIT\\nMaster data management (MDM)\\nExperience in Message queue implementations (Kafka)\\n\\nPay Range:\\n$96,800-$145,200/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$97K - $145K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'LPL Financial\\n3.9',\n",
       "  'JobTitle': 'AVP - Data Engineer Lead',\n",
       "  'JobLocation': 'Fort Mill, SC',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!\\nLPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.\\nJob Description\\nWe are currently looking to hire a tech lead (Technical Lead, AVP) within the Enterprise Data and Information Services team, which is part of LPL’s Technology organization. This position is responsible for contributing to a team focused on delivering the next generation cloud-based data platform.\\nResponsibilities:\\nLead development user stories for integration of data into cloud-native storage\\nAdhere to industry standards and methodologies for software development\\nFollow and provide feedback to technical guidance on standards, design patterns, architectural patterns and frameworks for services and integrations\\nCollaborate with analysts, modelers and subject matter experts\\nCreate system and architecture design documents to review with senior management and Enterprise Architecture teams\\nImplement best practices for cloud designs, service selection, and coding best practices\\nWhat are we looking for?\\nWe want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.\\nRequirements:\\nBachelor’s degree and/or relevant experience in IT related software development or support\\n7-10 years’ experience with developing on a major cloud computing provider (AWS, API Gateway, .Net Core, Docker) PostgreSQL, Kafka, SQLServer,\\nKnowledgeable of middleware and API patterns (GRPC and REST)\\nExperience with integration with message frameworks (ex: TIBCO EMS, Kafka)\\nExperience with DevOps toolchain that enables CI/CD pipeline (e.g., Git, TFS, Jenkins, TeamCity, Octopus, Puppet)\\nCore Competencies:\\nStrong problem-solving skills (e.g., root cause analysis) to debug or improve existing systems\\nStrong project leadership skills to manage Agile team to deliver product features\\nPreferences:\\nFinancial services industry experience is a plus\\nExperience with design and development of RESTful web services using languages such as Java, .NET, Golang or Python (ex: ASP.NET Core in .NET 5)\\nExperience with documenting RESTful web services with OpenAPI specification\\nKnowledgeable of API security best practices (e.g., TLS, CORS)\\nExperience with functional (e.g., Cucumber) or performance (e.g., JMeter, BlazeMeter) testing tools/frameworks\\nExperience with Agile software development methodologies\\n\\nPay Range:\\n$132,160-$198,240/year\\nActual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\\n\\nWhy LPL?\\nAt LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.\\nWe are one team on one mission. We take care of our advisors, so they can take care of their clients.\\nBecause our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.\\nWant to hear from our employees on what it’s like to work at LPL? Watch this!\\nWe take social responsibility seriously. Learn more here\\nWant to see info on our benefits? Learn more here\\nJoin the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.\\nInformation on Interviews:\\nLPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.',\n",
       "  'JobSalary': 'Employer Provided Salary:$132K - $198K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1968',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Mayo Clinic\\n3.9',\n",
       "  'JobTitle': 'Senior Data Operations Engineer - Remote',\n",
       "  'JobLocation': 'Rochester, MN',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Why Mayo Clinic\\n\\nMayo Clinic has been ranked the #1 hospital in the nation by U.S. News & World Report, as well as #1 in more specialties than any other care provider. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans – to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You’ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.\\n\\n\\nResponsibilities\\nWorks with MCP Director of Data Asset Management on executing MCP data roadmap. Develops and deploys data pipelines and data warehouses to support MCP Data & Analytics operations. Uses various open-source programming languages, Google Cloud technologies, machine learning models, and vended software to meet data needs of MCP products. The position requires maintaining an understanding of the Mayo's data policies, enterprise data warehouse, and regularly requires the application of independent judgment. Demonstrated experience in designing, building, and installing data systems and how they are applied to support Mayo Clinic Platform products is required. Builds data pipelines to handle large volumes of clinical data including EHR data, DICOM data, and Genomic data.\\n\\nQualifications\\nA Bachelor's degree in a relevant field such as engineering, mathematics, computer science, information technology, health science, or other analytical/quantitative field and a minimum of five years of professional or research experience in data visualization, data engineering, analytical modeling techniques; OR an Associate's degree in a relevant field such as engineering, mathematics, computer science, information technology, health science, or other analytical/quantitative field and a minimum of seven years of professional or research experience in data visualization, data engineering, analytical modeling techniques. In-depth business or practice knowledge will also be considered.\\n\\nIncumbent must have the ability to manage a varied workload of projects with multiple priorities and stay current on healthcare trends and enterprise changes. Interpersonal skills, time management skills, and demonstrated experience working on cross functional teams are required. Requires strong analytical skills and the ability to identify and recommend solutions and a commitment to customer service. The position requires excellent verbal and written communication skills, attention to detail, and a high capacity for learning and problem resolution. Advanced experience in SQL is required. Strong Experience in programming languages such as Python, JavaScript, PHP, C++ or Java & API integration is required. Experience with data warehouse technologies such as BigQuery, DB2 and RDBMS is required. Experience in hybrid data processing methods (batch and streaming) such as Apache Spark, GC Dataflow and GC Data Fusion is required. Experience with big data, statistics, and machine learning is required. The ability to navigate linux and windows operating systems is required. Knowledge of workflow scheduling (Apache Airflow Google Composer), Infrastructure as code (Terraform, Kubernetes, Docker), CI/CD (Jenkins, Github Actions) is preferred. Experience agile methodologies is preferred. Working knowledge of Tableau, Power BI, SAS, and SSIS is preferred.\\n\\nExemption Status\\n\\nExempt\\n\\nCompensation Detail\\n\\n$105,268 - $147,388 / year\\n\\nBenefits Eligible\\n\\nYes\\n\\nSchedule\\n\\nFull Time\\n\\nHours/Pay Period\\n\\n80\\n\\nSchedule Details\\n\\nMonday - Friday, Normal Business Hours 25%+ travel may be required 100% Remote. This position may work remotely from any location within the US. This vacancy is not eligible for sponsorship/ we will not sponsor or transfer visas for this position\\n\\nWeekend Schedule\\n\\nNot Applicable\\n\\nInternational Assignment\\n\\nNo\\n\\nSite Description\\nJust as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.\\n\\nAffirmative Action and Equal Opportunity Employer\\n\\nAs an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.\\n\\n\\nRecruiter\\n\\nJulie Melton\\n\\nDepartment Title\\n\\nMayo Clinic Platform\",\n",
       "  'JobSalary': 'Employer Provided Salary:$105K - $147K',\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Health Care Services & Hospitals',\n",
       "  'CompanySector': '$10+ billion (USD)',\n",
       "  'CompanyYearFounded': 'Nonprofit Organization',\n",
       "  'CompanyIndustry': 'Healthcare',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'PubMatic\\n4.3',\n",
       "  'JobTitle': 'Senior Software Engineer (Data Analytics / Big Data Engineer )',\n",
       "  'JobLocation': 'Redwood City, CA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Company Description\\n\\nPubMatic (Nasdaq: PUBM) is an independent technology company maximizing customer value by delivering digital advertising’s supply chain of the future.\\nPubMatic’s sell-side platform empowers the world’s leading digital content creators across the open internet to control access to their inventory and increase monetization by enabling marketers to drive return on investment and reach addressable audiences across ad formats and devices.\\nSince 2006, our infrastructure-driven approach has allowed for the efficient processing and utilization of data in real time. By delivering scalable and flexible programmatic innovation, we improve outcomes for our customers while championing a vibrant and transparent digital advertising supply chain.\\n\\nJob Description\\n\\nOur Advertising technology team is building a holistic advertising platform which will be key to e-commerce company’s ad revenue growth strategy. We will build cutting edge machine learning and optimization algorithms to ingest, model and analyze online and in-store data from e-commerce companies. Importantly, we will build smart systems that deliver relevant retail ads and experiences that connect customers with the brands and products they love and enable advertisers to achieve their product sales goals via campaign activation.\\nThe Senior Software Engineer will be responsible for developing end-to-end product features for reporting system for advertising platform for e-commerce clients, that would have big data processing and will include the design and development of Analytics components.\\nWe are looking for self-motivated senior software engineers who enjoy working in dynamic, agile development environments as individual contributors to the team. The ideal candidate is a self-motivated problem solver with a strong background in big data tech stack, software design, development, and e-commerce AdTech domain.\\nJob Location: US (RWC/NYC)\\nResponsibilities:\\nBuild, design and implement our highly scalable, fault-tolerant, highly available big data platform to process terabytes of data and provide customers with in-depth analytics.\\nDeveloping Big Data pipelines using modern technology stack such as Spark, Hadoop, Kafka, HBase, Hive, Presto etc.\\nDeveloping analytics application ground up using modern technology stack such as Java, Spring, Tomcat, Jenkins, REST APIs, JDBC, Amazon Web Services, Hibernate.\\nBuilding data pipeline to automate high-volume data collection and processing to provide real-time data analytics.\\nCustomize PubMatic’s reporting and analytics platform based on customer’s requirements from customers and deliver scalable, production-ready solutions.\\nLead multiple projects to develop features for data processing and reporting platform, collaborate with product managers, cross-functional teams, other stakeholders and ensure successful delivery of projects.\\nUse various mechanisms established to fetch data from different external data sources and reconcile them with PubMatic’s processed data.\\nCollaborate with functional teams to build products to deliver end-to-end products and features and fix bugs for better performance.\\nDevelop robust & fault-tolerant systems and monitor implications of changes on data processing pipeline and performance.\\nLeveraging a broad range of PubMatic’s data architecture strategies and proposing both data flows and storage solutions.\\nManaging hadoop map reduce and spark jobs & solving any ongoing issues with operating the cluster.\\nWorking closely with cross functional teams on improving availability and scalability of large data platform and functionality of PubMatic software.\\nExpertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, performing code reviews, committing to Github, preparing documents in Confluence, continuous delivery using Jenkins, automated testing, and operations.\\nParticipate in Agile/Scrum processes such as sprint planning, sprint retrospective, backlog grooming, user story management, work item prioritization, etc.\\nFrequently discuss with product managers about the software features to include in PubMatic Data Analytics platform. Understand the technical aspects customer requirement from product managers.\\nKeep in regular touch with quality engineering team which ensure the quality of the platforms/products and performance SLAs of java based micro services and spark-based data pipeline.\\nSupport customer issues over email or JIRA (bug tracking system), provide updates, patches to customers to fix the issues.\\nDiscuss with technical writing team about the technical documents that are published on documentation portal.\\nPerform code and design reviews for code implemented by peers or as per the code review process.\\n\\nQualifications\\n\\n3+ years coding experience in Java.\\nExperience in implementing closed loop reporting for sponsored product listing campaigns for e-commerce companies.\\nBuilding analytics pipes for product search relevance modeling and click prediction\\nExperience in implementing reporting for display/video/native Ad campaigns for e-commerce companies.\\nSolid computer science fundamentals including data structure and algorithm design, and creation of architectural specifications.\\nExpertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, documentation, build processes, automated testing, and operations.\\nA passion for developing and maintaining a high-quality code and test base and enabling contributions from engineers across the team.\\nExpertise in big data technologies like Hadoop, Spark, Kafka, HBase etc would be an added advantage.\\nExperience in developing and delivering large-scale big-data pipelines, real-time systems & data warehouses would be preferred.\\nDemonstrated ability to achieve stretch goals in a very innovative and fast paced environment.\\nDemonstrated ability to learn new technologies quickly and independently.\\nExcellent verbal and written communication skills, especially in technical communications.\\nStrong interpersonal skills and a desire to work collaboratively.\\nBase Compensation Range: $120,000 - $160,000\\nIn accordance with applicable law, the above salary range provided is PubMatic’s reasonable estimate of the base salary for this role. The actual amount may vary, based on non-discriminatory factors such as location, experience, knowledge, skills and abilities. In addition to salary PubMatic also offers a bonus, restricted stock units and a competitive benefits package.\\n#LI-KS2\\n\\nAdditional Information\\n\\nReturn to Office: PubMatic employees throughout the global have returned to our offices via a hybrid work schedule (3 days “in office” and 2 days “working remotely”) that is intended to maximize collaboration, innovation, and productivity among teams and across functions. All PubMatic employees in the US and India are required to be fully vaccinated to return to our offices. Covid-19 boosters are not required at this point in time.\\nBenefits: Our benefits package includes the best of what leading organizations provide such as, paid leave programs, paid holidays, healthcare, dental and vision insurance, disability and life insurance, commuter benefits, physical and financial wellness programs, unlimited DTO in the US (that we actually require you to use!), reimbursement for mobile and internet expenses and fully stocked pantries plus in-office catered lunches 3 days per week.\\nDiversity and Inclusion: PubMatic is proud to be an equal opportunity employer; we don’t just value diversity, we promote and celebrate it. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.',\n",
       "  'JobSalary': 'Employer Provided Salary:$120K - $160K',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2006',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Deloitte\\n4.0',\n",
       "  'JobTitle': 'On Premises Teradata/Informatica Data Engineer - Healthcare',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Are you an experienced, passionate pioneer in technology who wants to work in a collaborative environment? As an experienced Healthcare On Premises Data Engineer will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. This position is working on a multi-year project for a major healthcare client. This is a remote role.\\n\\nWork you'll do/Responsibilities\\n\\nYou will determine processes and automation tools to reduce IT spend and increase efficiencies on multiple projects within the Healthcare domain.\\n\\nThis position requires designing, coding, testing, and maintaining Informatica/BTEQ (Basic Teradata Query) scripts needed to support applications.\\n\\nThis job requires expertise in building and managing Teradata/ Informatica/ Hadoop infrastructure On Premises and an understanding to migrate applications to Cloud.\\n\\nYou will be participating in Agile development methodologies, such as Scrum or Kanban, to ensure timely project delivery. Documenting technical specifications, architectural designs, and application configurations.\\n\\nYou will implement security measures and best practices to ensure data protection and compliance. Troubleshooting and debugging issues in applications and infrastructure, and implementing effective resolutions. Collaborating with cross-functional teams, including software engineers, architects, and stakeholders, to deliver high-quality solutions.\\n\\nThe Team\\n\\nAs a part of the US Strategy & Analytics Offering Portfolio, the AI & Data Engineering offering provides managed AI, Intelligent Automation, and Data DevOps services across the advise-implement-operate spectrum.\\n\\nQualifications\\n\\nRequired\\n\\n\\n4+ years' experience in data engineering, ETL development, and data integration using Informatica Power Center.\\n4+ years' experience in Teradata, including database design, SQL programming, and performance optimization.\\n4+ years' experience in writing BTEQ (Basic Teradata Query) scripts for data extraction, transformation, and loading within Teradata.\\n4+ years' experience in SQL programming skills, including writing efficient and optimized SQL queries for data retrieval and manipulation.\\nSolid understanding of data warehousing concepts, data modeling, and data quality principles.\\nExperience in working with large data sets and handling complex data integration scenarios.\\nBachelor's degree or equivalent experience\\nLimited immigration sponsorship may be available\\n\\n\\nPreferred\\n\\n\\nFamiliarity with data profiling tools and techniques to assess data quality and identify data issues.\\nKnowledge of scheduling tools like Control-M or similar for job scheduling and monitoring.\\nGood problem-solving and analytical skills, with the ability to troubleshoot and resolve data-related issues\\nAbility to travel 10%, on average, based on the work you do and the clients and industries/sectors you serve\\nBachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience\\nAnalytical/ decision making responsibilities\\nAnalytical ability to manage multiple projects and prioritize tasks into manageable work products\\nCan operate independently or with minimum supervision\\nExcellent communication skills\\nAbility to deliver technical demonstrations\",\n",
       "  'JobSalary': '$80K - $106K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1850',\n",
       "  'CompanyIndustry': 'Accounting & Tax',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'The Judge Group\\n3.7',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Cincinnati, OH',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Our client is currently seeking a Senior Data Engineer\\n\\n\\nJob Description:\\nAccountable for developing and delivering technological responses to targeted business outcomes.\\nAnalyze, design, and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration.\\n2+ years? experience with automation production systems (Ansible Tower, Jenkins, Puppet, or Selenium)\\nWorking knowledge of databases and SQL Experience with software development methodologies and SDLC Candidate possess a problem-solving attitude and can work independently\\nMust be very organized, able to balance multiple priorities, and self-motivated\\nKey Responsibilities:\\nExperience in administration and configuration of API Gateways (e.g. Apigee/Kong) Apply cloud computing skill to deploy upgrades and fixes\\nDesign, develop, and implement integrations based on use feedback.\\nTroubleshoot production issues and coordinate with the development team to streamline code deployment.\\nImplement automation tools and frameworks (Ci/CD pipelines).\\nAnalyze code and communicate detailed reviews to development teams to ensure a marked improvement in applications and the timely completion of products.\\nCollaborate with team members to improve the company?s engineering tools, systems and procedures, and data security.',\n",
       "  'JobSalary': '$102K - $133K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Human Resources & Staffing',\n",
       "  'CompanyYearFounded': '1970',\n",
       "  'CompanyIndustry': 'HR Consulting',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'Gulfstream Aerospace Corporation\\n3.7',\n",
       "  'JobTitle': 'Data Integrity Engineer Technical Specialist I',\n",
       "  'JobLocation': 'Savannah, GA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Data Integrity Engineer Technical Specialist I in GAC Savannah\\n\\nUnique Skills:\\nExperience interpretting industry and vendor specifications and standards\\nExperience with CatiaV5\\nSmarTeam experience preferred\\nDetail oriented\\n\\nEducation and Experience Requirements\\nBachelor's Degree required or equivalent combination of education and experience sufficient to successfully perform the essential functions of the job. 5 ys in specific technical discipline or 10 yrs broad eng experience in several technical disciplines with Bachelors. Experience credit considered for related adv degrees limited to 2 yrs for Masters, 4 yrs for PhD in fields applicable to this job.\\nPosition Purpose:\\nAssists with the planning, coordination, performance of design, analysis and liaison engineering supporting the development and production of Gulfstream products and services\\nJob Description\\nPrinciple Duties and Responsibilities:\\nEssential Functions:\\nContributes to planning, developing and coordinating of important engineering projects .\\nProvides specialized technical assistance to team members .\\nUses advanced techniques and modified extension of theories to provide technical solutions to a wide range of difficult problems .\\nDetermines and develops own approach to solutions within schedule and cost objectives .\\nWorks under limited supervision; work should only be required. to be reviewed for accuracy and consistency with meeting overall objectives .\\nActs as liaison with representatives outside of the assigned group/area .\\nAdditional Functions:\\nActively works to improve daily processes and ensures all work meets customer requirements .\\nMaintains a current knowledge of developments in the field of specialty and/or other related aircraft fields in order to recommend innovations to improve quality and effectiveness of company product .\\nPerform other duties as assigned.\\nOther Requirements:\\nAdvanced degree in engineering related field preferred.\\nThe level of technical skill and abilities appropriate for this grade will be established by each discipline.\\nAdditional Information\\nRequisition Number: 212981\\nCategory: Engineering\\nPercentage of Travel: Up to 25%\\nShift: First\\nEmployment Type: Full-time\\nPosting End Date: 07/15/2023\\n\\nEqual Opportunity Employer/Veterans/Disabled.\\n\\nGulfstream does not provide work visa sponsorship for this position, unless the applicant is a currently sponsored Gulfstream employee.\\n\\nLegal Information | Site Utilities | Contacts | Sitemap\\nCopyright © 2020 Gulfstream Aerospace Corporation. All Rights Reserved. A General Dynamics Company.\\n\\nGulfstream Aerospace Corporation, a wholly-owned subsidiary of General Dynamics (NYSE: GD), designs, develops, manufactures, markets, services and supports the world's most technologically-advanced business jet aircraft\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Subsidiary or Business Segment',\n",
       "  'CompanySector': 'Aerospace & Defense',\n",
       "  'CompanyYearFounded': '1958',\n",
       "  'CompanyIndustry': 'Aerospace & Defense',\n",
       "  'CompanyRevenue': '$1 to $5 billion (USD)'},\n",
       " {'CompanyName': 'Google\\n4.4',\n",
       "  'JobTitle': 'Manufacturing Engineer, Data Center Equipment',\n",
       "  'JobLocation': 'Atlanta, GA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Minimum qualifications:\\nBachelor's degree in Electrical Engineering, Process Engineering, Manufacturing Engineering, a related field, or equivalent practical experience.\\n5 years of experience in manufacturing and construction.\\nExperience in process or product development.\\nExperience in Product Data Management (PDM) systems (e.g., Agile) and Bill of Materials (BoM) construction.\\n\\nPreferred qualifications:\\nMaster's degree in Engineering or equivalent practical experience.\\nExperience with LEAN Manufacturing, 5S, or 8D practices.\\nKnowledge of overseeing yield improvements and cycle time reductions for high volume and high complexity parts.\\nAbility to influence the product design process by proposing improvements with associated data and produce a timely and accurate work product.\\nAbility to travel up to 40% of the time as needed.\\nExcellent problem-solving and organizational skills with attention to details.\\nAbout the job\\nGoogle has one of the largest and most powerful computing infrastructures in the world. Your team is responsible for providing the manufacturing capability to deliver this state-of-the-art physical infrastructure. As a Manufacturing Engineer, you evaluate the product designs and create the processes, tools and procedures behind Google's powerful search technology. When vendors build parts for our infrastructure, you're right there alongside ensuring manufacturing processes are repeatable and controlled. You collaborate with Commodity Managers and Design Engineers to determine Google's infrastructure needs and product specifications. Your work ensures the various pieces of Google's infrastructure fit together perfectly and keep our systems humming along smoothly for a seamless user experience.\\nBehind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.\\nThe US base salary range for this full-time position is $108,000-$158,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\\n\\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.\\nResponsibilities\\nProvide comprehensive manufacturing engineering support to Google's global manufacturing partners during development and production builds.\\nIdentify and evaluate Contract Manufacturers (CM)/vendors to fabricate data center related products and architecture.\\nConduct manufacturing and process audits and qualify CMs/vendors. Identify, initiate, and drive process improvements that reduce process cycle times and cost, while maintaining exceptional quality.\\nResolve manufacturing and field issues by applying knowledge and skills, escalating design, manufacturing, and product test issues to others within the organization when appropriate.\\nUse Product Data Management (PDM) systems to create and maintain Bills of Materials (BoM), Approved Vendor Lists (AVL), and Engineering Change Notifications/Orders (ECN/O) in support of Google products.\\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1998',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Lockheed Martin\\n4.1',\n",
       "  'JobTitle': 'Wind Tunnel Data Reduction Engineer / Dallas',\n",
       "  'JobLocation': 'Grand Prairie, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Job ID: 643226BR\\nDate posted: Jul. 12, 2023\\nProgram: High Speed Wind Tunnel\\n\\nDescription:Candidate will be working in the dynamic environment of supersonic wind tunnel testing for a company known for promoting a flexible work environment and excellent benefits. Position entails planning and conducting the data acquisition effort for multiple test programs per year with responsibility for data reduction programming for aerodynamic and mechanical response data obtained during wind tunnel tests to meet our customer's data requirements. Candidate will perform set-up and modification of computer programs for wind tunnel test systems control and data acquisition, processing and software maintenance as well as new systems development. Position entails remaining current in the latest wind tunnel testing techniques including support of instrumentation calibration software maintenance and upgrades. Candidate must possess the initiative to continuously improve tools and processes for ever increasing test productivity. Programming in a Windows OS environment with a working knowledge of Linux is required. Candidate must possess effective communication skills, be detail oriented and be able to work in a team environment with other wind tunnel personnel and customer representatives. While a 4/10 work schedule is our typical schedule, shift work with shift differential pay and/or paid overtime could be required during heavy periods of active testing in support of customer milestones. A cursory capability and knowledge in aerodynamics is a plus. Previous experience in a test or laboratory environment is desirable. Experience with a formal programming language is required. Must possess or be able to obtain a DoD SECRET level security clearance.\\nBasic Qualifications:\\nAbility to obtain a DoD Secret/SAR security clearance\\nFormal programming language experience\\nWindows Environment Skills; MS Office including Excel and Excel VBA\\nKnowledge of data acquisition and instrumentation hardware\\nAbility to work paid OT and night shift during high workload periods\\nDesired Skills:\\nFORTRAN programming experience\\nVisual Basic programming experience\\nMATLAB programming experience\\nLabVIEW programming experience\\nTest/Laboratory experience\\nWind tunnel testing experience\\nBasic knowledge of aerodynamics/fluids\\nMaster's degree from an accredited college in a related engineering discipline\\nSecurity Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.\\nClearance Level: Secret with an investigation within 5 years\\nOther Important Information You Should Know\\nExpression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.\\nAbility to Work Remotely: Part-time Remote Telework: The employee selected for this position will work part of their work schedule remotely and part of their work schedule at a designated Lockheed Martin facility. The specific weekly schedule will be discussed during the hiring process.\\nWork Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.\\nSchedule for this Position: 4x10 hour day, 3 days off per week\\nLockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.\\nAt Lockheed Martin, we use our passion for purposeful innovation to help keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.\\n\\nWith our employees as our priority, we provide diverse career opportunities designed to propel, develop, and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work. We place an emphasis on empowering our employees by fostering an inclusive environment built upon integrity and corporate responsibility.\\n\\nIf this sounds like a culture you connect with, you're invited to apply for this role. Or, if you are unsure whether your experience aligns with the requirements of this position, we encourage you to search on Lockheed Martin Jobs, and apply for roles that align with your qualifications.\\nExperience Level: Experienced Professional\\nBusiness Unit: MISSILES AND FIRE CONTROL\\nRelocation Available: Possible\\nCareer Area: Systems Engineering: Other\\nType: Full-Time\\nShift: Multiple shifts available\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Aerospace & Defense',\n",
       "  'CompanyYearFounded': '1995',\n",
       "  'CompanyIndustry': 'Aerospace & Defense',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Principal Financial Group\\n4.1',\n",
       "  'JobTitle': 'Sr Data Engineer Leader',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"What You'll Do:\\nWe’re looking for a Sr Data Engineering Leader to join our Enterprise Data team. In this role, you’ll lead and enable teams to help bring modern engineering capabilities to drive enterprise strategy!\\nProvide data engineering and analysis expertise to enhance effectiveness across multiple technologies and services.\\nShape broad architecture; make an obvious positive impact on the company's technical trajectory.\\nFacilitate strategic discussions and implement tactical plans.\\nMake recommendations on future technical trends/directions that encompass multiple systems and teams to meet business initiatives.\\nMultiply the effectiveness of others by facilitating cross-team work.\\nProvide coaching and mentoring to team members.\\nDemonstrate influence across BUs.\\nResponsible for staff recruitment, performance assessment, training and career development. Set clear goal expectations for members of a team.\\nGrow the team's ability to own and design all software development layers of a solution.\\nOperating at the intersection of financial services and technology, Principal builds financial tools that help our customers live better lives. We take pride in being a purpose-led firm, motivated by our mission to make financial security accessible to all. Our mission, integrity, and customer focus have made us a trusted leader for more than 140 years.\\nAs Principal continues to modernize its systems, this role will offer you an exciting opportunity to build solutions that will directly impact our long-term strategy and tech stack, all while ensuring that our products are robust, scalable, and secure!\\nWho You Are:\\nYou have a Bachelor's degree plus 8+years related work experience or a Master's in related field plus 4 + years related work experience\\nYou have solid leadership and presentation skills\\nYou are able to collaborate with others and able to effectively communicate strategies and designs to all levels of the company\\nSkills That Will Help You Stand Out\\nAbility to work directly with data and datastores through programming language such as SQL, Python, Java, etc\\nExperience with cloud based technologies such as AWS and Snowflake\\nExperience with a variety of data modeling techniques and data structures\\nExperience leading geographically dispersed teams\\n\\nSalary Range Information: Salary ranges below reflect targeted base salaries. Non-sales positions have the opportunity to participate in a bonus program. Sales positions are eligible for sales incentives, and in some instances a bonus plan, whereby total compensation may far exceed base salary depending on individual performance. Actual compensation for all roles will be based upon geographic location, work experience, education, licensure requirements and/or skill level and will be finalized at the time of offer. Salary Range: $95200 - $182400 / year Additional Information:\\nOur Engineering Culture\\nThrough our product-driven Agile/Lean DevOps environment, we’ve fostered a culture of innovation and experimentation across our development teams. As a customer-focused organization, we work closely with our end users and product owners to understand and rapidly respond to emerging business needs.\\nCollaboration is embedded into everything we do – from the products we develop to the quality service we provide. We’re driven by the belief that diversity of thought, background, and perspective is critical to creating the best products and experiences for our customers.\\nWork Environments\\nThis role offers the ability for in-office, hybrid (blending both office and remote work in a typical workweek), and remote work arrangements. You’ll work with your leader to figure out which option may align best based on several factors.\\nJob Level\\nWe’ll consider talent at the next level with the right experiences and skills.\\nWork Authorization/Sponsorship\\nAt this time, we're not considering applicants that need any type of immigration sponsorship (additional work authorization or permanent work authorization) now or in the future to work in the United States. This includes, but IS NOT LIMITED TO: F1-OPT, F1-CPT, H-1B, TN, L-1, J-1, etc. For additional information around work authorization needs please use the following links.\\nNonimmigrant Workers and Green Card for Employment-Based Immigrants\\nInvestment Code of Ethics\\nFor Principal Asset Management positions, you’ll need to follow an Investment Code of Ethics related to personal and business conduct as well as personal trading activities for you and members of your household. These same requirements may also apply to other positions across the organization.\\nExperience Principal\\nAt Principal, we value connecting on both a personal and professional level. Together, we’re imagining a more purpose-led future for financial services – and that starts with you. Our success depends on the unique experiences, backgrounds, and talents of our employees. And we support our employees the same way we support our customers: with comprehensive, competitive benefit offerings crafted to protect their physical, financial, and social well-being. Check out our careers site to learn more about our purpose, values and benefits.\\nPrincipal is an Equal Opportunity Employer\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.\\nLinkedIn Remote Hashtag\\n: #LI-Remote\",\n",
       "  'JobSalary': 'Employer Provided Salary:$95K - $182K',\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1879',\n",
       "  'CompanyIndustry': 'Investment & Asset Management',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'JPMorgan Chase Bank, N.A.\\n3.8',\n",
       "  'JobTitle': 'Lead Data Engineer',\n",
       "  'JobLocation': 'Wilmington, DE',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'As a Lead Data Engineer within Consumer and Community Banking, in Home Lending, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm\\'s business objectives.\\n\\nJob Responsibilities\\nDesign and implement end-to-end data pipelines supporting analytical and operational needs accounting for data management practices focused on data quality, metadata management etc.\\nArchitect, design, and implement cloud native solutions on AWS.\\nDefine and implement event driven architecture patterns leveraging messaging / streaming solutions like Kafka, Kinesis, Flink, and Spark\\nAbility to decompose large initiatives / designs into manageable smaller bodies of work to demonstrate continuous progress\\nCollaborate with business stakeholders, product owners, architects, data domain owners to understand current landscape and develop solutions in alignment with business & technology strategy. Assist in refining /evolving data strategy highlighting clear outcomes.\\nDeep understanding or desire to continue to learn new database technologies, cloud computing & storage services\\nUnderstanding of the pros / cons associated with various technology choices and ability to pick the right technology based on the use case\\n\\nRequired qualifications, capabilities, and skills\\nFormal training, or certification on data engineering concepts, and 5+ years of experience. In addition, demonstrated coaching and mentoring experience\\nProgramming experience in Java, Python, Scala etc.\\nExperience in using distributed frameworks like Spark, Hadoop etc.\\nExperience with AWS services like Lambda, EC2, EMR, Redshift, Glue, S3, IAM, RDS, Aurora, DynamoDB etc.\\nKnowledge of cloud networking, security, storage, and compute services\\nInfrastructure provisioning experience using Cloud Formation, Terraform etc.\\nExperience implementing solutions leveraging CI / CD etc.\\n\\nPreferred qualifications, capabilities, and skills\\nAWS Solutions Architect / Developer or any advanced level certification preferred\\nExperience and proficiency across the data lifecycle\\nExperience with database back-up, recovery, and archiving strategy\\nProficient knowledge of linear algebra, statistics, and geometrical algorithms\\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world\\'s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants\\' and employees\\' religious practices and beliefs, as well as any mental health or physical disability needs.\\nThe health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the \"WELL Health-Safety Rating\" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.\\nAs a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm\\'s current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm\\'s vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.\\n\\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\\nEqual Opportunity Employer/Disability/Veterans',\n",
       "  'JobSalary': '$114K - $149K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.8',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1799',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'NYC Health + Hospitals\\n3.6',\n",
       "  'JobTitle': 'Data Center Operations Engineer (Senior Consultant MIS Lvl A), Enterprise Infrastructure *Connecticut*',\n",
       "  'JobLocation': 'New York, NY',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'About NYC Health + Hospitals\\n\\nEmpower Every New Yorker — Without Exception — to Live the Healthiest Life Possible\\nNYC Health + Hospitals is the largest public health care system in the United States. We provide essential outpatient, inpatient and home-based services to more than one million New Yorkers every year across the city’s five boroughs. Our large health system consists of ambulatory centers, acute care centers, post-acute care/long-term care, rehabilitation programs, Home Care, and Correctional Health Services. Our diverse workforce is uniquely focused on empowering New Yorkers, without exception, to live the healthiest life possible.\\nAt NYC Health + Hospitals, our mission is to deliver high quality care health services, without exception. Every employee takes a person-centered approach that exemplifies the ICARE values (Integrity, Compassion, Accountability, Respect, and Excellence) through empathic communication and partnerships between all persons.\\nJob Description\\n\\nThe Data Center Operations Engineer will be a team oriented individual who will be responsible for installation and de-installation of network devices, monitor Z10 mainframe environment and peripherals at our Connecticut location. They will verify connectivity of all devices and also troubleshoot any reconnection issues. They will also be responsible for recording the assets that are assigned to a user.\\nDuties & Responsibilities\\nInstallation of network devices and cable management systems\\nProvide technical support to our network owner/user community\\nMaintain and update an accurate documentation of Service Desk trouble tickets\\nMonitor System interfaces and other communication utilities and take proper action\\nInstall/Rack Servers\\nAdd Memory, H/D, Network Interface Cards, CPU etc.\\nInstall KVM and In-Rack PDU\\nMonitor/Maintain all Servers in the Data Center\\nFamiliarization with a large-scale IBM Z10 mainframe environment\\nIPL, TSO, MVS, Jes2, CA7\\nIBM 4000, IBM 6500 Canon VS 4400\\nService Now Ticketing System\\nAsset Management\\nBack up Daily/Monthly tapes for offsite storage\\nInventory\\nPerform all other related duties\\nInteract with NYC-based Team Members\\nMinimum Qualifications\\n\\n1. A Baccalaureate Degree from an accredited college or university with a major in Computer Science, Systems Engineering, applied Mathematics, Business Administration, Economics/Statistics, Telecommunications, Data Communications, or a related field of study; and\\n2. Five (5) years of progressive, responsible experience in the field of data processing, computer systems and applications.\\nOperations Specialty requires supervisory experience (5 years).\\nNetwork Services requires a telecommunications background and experience.\\n3. Broad knowledge and expertise in the characteristics of computers, peripheral devices, communications systems and hardware capabilities, programming languages, E.D.P. applications, systems analysis methodology, data management and retrieval techniques; or\\n4. A satisfactory equivalent combination of training, education and experience\\nDepartment Preferences\\nFamiliarization with a large-scale IBM Z mainframe environment TSO, Jes2 Service Now Ticketing System\\n9 years of experience; Operations Specialty requires supervisory experience (5 years).\\nHow To Apply\\n\\nPlease be advised that proof of Covid-19 vaccination is required prior to hire.\\nIf you wish to apply for this position, please apply online by clicking the \"Apply Now\" button.\\nIf applying online, please include your cover letter in the same file attachment with your uploaded resume.\\nNYC Health and Hospitals offers a competitive benefits package that includes:\\nComprehensive Health Benefits for employees hired to work 20+ hrs. per week\\nRetirement Savings and Pension Plans\\nLoan Forgiveness Programs for eligible employees\\nPaid Holidays and Vacation in accordance with employees\\' Collectively bargained contracts\\nCollege tuition discounts and professional development opportunities\\nMultiple employee discounts programs\\nNote: Candidates selected for a position are required to come to NYC as part of their onboarding, training and as required by Human Resources.',\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.6',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Health Care Services & Hospitals',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Hospital',\n",
       "  'CompanyIndustry': 'Healthcare',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'PeopleTec, Inc.\\n4.9',\n",
       "  'JobTitle': 'Data Engineer - Mid/Senior',\n",
       "  'JobLocation': 'Huntsville, AL',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Responsibilities:\\nPeopleTec is currently seeking a Mid to Senior Data Engineer to support our Huntsville, AL location.\\n\\nPeopleTec is a company that hires and develops technology talent and puts them to work on client projects across the United States. We specialize in full stack development, Cloud, DevSecOps, Cybersecurity, Artificial Intelligence (AI), Business Intelligence (BI), Data Science, Machine Learning (ML) related initiatives, both in the federal and commercial space. We are currently seeking a Mid to Senior Data Engineer to support our Huntsville, AL location. The Data Engineer serves as the person responsible for developing, testing, deploying, and sustaining data pipelines in support of an end-to-end data analytics system using a combination of COTS (commercial off the shelf), GOTS (Government off the shelf), and open-source tools.\\n\\nResponsibilities:\\nLead data engineering and architecture efforts, to include - developing, testing, deploying and sustaining data pipelines in support of an end-to-end data analytics system using a combination of COTS (commercial off the shelf), GOTS (Government off the shelf), and open-source tools.\\nDesign and implement end-to-end data architecture solutions on a cloud platform.\\nCollaborate with business stakeholders, data engineers, and data scientists to understand data requirements and design scalable and flexible data models.\\nDevelop data ingestion strategies and frameworks to efficiently capture and integrate data from various sources into cloud data services.\\nDefine data storage and retrieval mechanisms, including data partitioning, indexing, and compression, to optimize performance and cost efficiency.\\nImplement data governance and data management practices to ensure data quality, consistency, and compliance with relevant regulations.\\nCreate and maintain data pipelines and ETL/ELT processes to transform, cleanse, and integrate data across different data sources.\\nWork with other team members to integrate data solutions into existing systems and processes.\\nEnsure all technical solutions adhere to DoD security and compliance regulations\\nQualifications:\\n\\nRequired Skills/Experience:\\n\\nExperience as Data Architect, Data Engineering, or any related role to Data solutions.\\nStrong proficiency in cloud data services and products.\\nDeep understanding of data modeling principles and experience with relational, dimensional, and NoSQL data modeling techniques.\\nSolid knowledge of ETL/ELT processes, data integration patterns, and data warehouse concepts.\\nFamiliarity with big data processing frameworks, such as Apache Spark or Hadoop.\\nExperience with Spark, SQL, Python, Scala, and/or R.\\nExperience with data governance, data security, and compliance practices in cloud environments.\\nStrong problem-solving and analytical skills with the ability to troubleshoot and resolve complex data-related issues.\\nExcellent communication and collaboration skills to work effectively with stakeholders at all levels.\\nAbility to manage multiple projects and priorities in a fast-paced and dynamic environment.\\nTravel: 5 %\\nMust be a U.S. Citizen\\nAn active DoD Secret clearance is required to perform this work. Candidates are required to have an active Secret clearance upon hire, and the ability to maintain this level of clearance during their employment.\\n\\nEducation Requirements:\\nBachelor’s degree in Computer Science, Data Science or other related field\\nDesired Skills:\\nMaster’s degree or Ph.D. with experience as Data Architect, Data Engineering, or any related role to Data solutions.\\nTS/SCI w/poly\\nExperience with Azure and/or AWS\\nIntelligence and threat analytics background\\nOverview:\\nPeople First. Technology Always.\\n\\nPeopleTec, Inc. is an employee-owned small business founded in Huntsville, AL that provides exceptional customer support by employing and retaining a highly skilled workforce.\\n\\nCulture: The name \"PeopleTec\" was deliberately chosen to remind us of our core value system - our people. Our company\\'s foundation was built on placing our employees and customers first. With an award-winning atmosphere, we have matured into a company that boasts the best and brightest across multiple technical fields.\\n\\nCareer: At PeopleTec, we value your long-term goals. Whether it\\'s through our continuing-education opportunities, our robust training programs, or our \"People First\" benefits package, PeopleTec truly believes that our best investments are our people.\\n\\nCome Experience It.\\n#cjpost #dpost\\n\\nEEO Statement\\n\\nPeopleTec, Inc. is an Equal Employment Opportunity employer and provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in its job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may use the following email address, applicationhelp@peopletec.com and/or phone number (256.319.3800) to contact us about your interest in employment with PeopleTec, Inc.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetic information, citizenship, ancestry, marital status, protected veteran status, disability status or any other status protected by federal, state, or local law. PeopleTec, Inc. participates in E-Verify.',\n",
       "  'JobSalary': '$80K - $118K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.9',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Aerospace & Defense',\n",
       "  'CompanyYearFounded': '2005',\n",
       "  'CompanyIndustry': 'Aerospace & Defense',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Regeneron\\n3.9',\n",
       "  'JobTitle': 'Process Development Engineer II - OT Data/ Automation',\n",
       "  'JobLocation': 'Tarrytown, NY',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Regeneron's Data Enablement and Analytics team within PMPD (Preclinical Manufacturing and Process Development), is seeking an OT Data Manager/Automation Engineer II. In this role, you will be responsible for the management of PMPD Operational Technology data systems, process automation systems and the delivery of OT/automation platforms and tools. You will empower PMPD with value-focused OT data management solutions to improve the value of data assets, meet immediate business needs and build data capabilities for the future, contributing to our mission to drive accelerated process development and bring new medicines to patients.\\nA Typical Day In the Role Might Look Like:\\nSupport the monitoring, maintenance and continued development of PMPD OT data systems, e.g. AVEVA-PI data historian, synTQ and automation systems (PLCs, SCADA, HMIs, instrumentation, process/equipment network etc.)\\nEnsure OT data availability, integrity, quality, security and accessibility\\nAdvise distributed data managers in effectively defining and managing programs/projects and enable teams to deliver on key objectives\\nAdvance automation/digital infrastructure to drive integration of process control systems with enterprise systems for IT/OT convergence\\nPartner with PMPD, IT and IOPS (Industrial Operations and Product Supply) to implement unified OT data management and infrastructure to advance data, analytics and digital maturity\\nContinuous Improvement: Identify use cases and develop feature enhancements for existing systems. Conduct new application evaluations and present proof-of-concept findings. Participate as a team member in the design, implementation and testing phases of agile and waterfall projects.\\nAdministration: Onboard new equipment. Review and manage system access requests.\\nMaintenance: Perform software updates and patching\\nSupport: Proactively monitor system health issues. Solve and resolve incident reports.\\nConsultancy: Provide OT domain expertise to PMPD partners and cross-functional teams.\\nThis Role May Be for You If:\\nYou are a champion for data driven decision making\\nYou thrive in a team-based, multi-functional, collaborative environment\\nYou possess a problem-solving mentality\\nTo be considered for this role, you must have a bachelor’s degree or higher in Engineering (computer science, chemical, electrical) or related field, with 5+ years of experience working with AVEVA/OSIsoft PI data historian and SQL database. In-depth understanding of OT data systems and communication standards preferred. Experience with the design, rollout and lifecycle management of automation and OT data systems deployed in a high-availability industrial architecture a plus. Good understanding of continuous and batch manufacturing processes, associated data sources, software integration, consumption and data analytics tools a plus. Technical knowledge of lab-based automation systems, software (e.g. Unicorn, MFCS, Ignition, FactoryTalk, PLC/HMIs/SCADA). Familiarity with the concepts of Industry 4.0, digitalization, IIoT and IT/OT convergence a plus. Python programming skills are a plus.\\n#pmpd\\nIntro to PMPD Data Enablement & Analytics:\\n\\nhttps://www.youtube.com/watch?v=aeJjCYkKVF0&list=PL_7lj13Lp4fH4OssUue8J5bby6muZsaR4&index=7\\nIntro to PMPD:\\nhttps://www.youtube.com/watch?v=pAboGKlI1zw&list=PL_7lj13Lp4fH4OssUue8J5bby6muZsaR4&index=5\\n\\nDoes this sound like you? Apply now to take your first steps toward living the Regeneron Way! We have an inclusive and diverse culture that provides comprehensive benefits including health and wellness programs, fitness centers and equity awards, annual bonuses, and paid time off for eligible employees at all levels!\\nRegeneron is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion or belief (or lack thereof), sex, nationality, national or ethnic origin, civil status, age, citizenship status, membership of the Traveler community, sexual orientation, disability, genetic information, familial status, marital or registered civil partnership status, pregnancy or parental status, gender identity, gender reassignment, military or veteran status, or any other protected characteristic in accordance with applicable laws and regulations. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application process. Please contact us to discuss any accommodations you think you may need.\\nThe salary ranges provided are shown in accordance with U.S. law and apply to U.S. based positions, where the hired candidate will be located in the U.S. If you are outside the U.S, please speak with your recruiter about salaries and benefits in your location.\\nSalary Range (annually)\\n$88,500.00 - $144,500.00\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Pharmaceutical & Biotechnology',\n",
       "  'CompanyYearFounded': '1988',\n",
       "  'CompanyIndustry': 'Biotech & Pharmaceuticals',\n",
       "  'CompanyRevenue': '$5 to $10 billion (USD)'},\n",
       " {'CompanyName': 'Siemens\\n4.2',\n",
       "  'JobTitle': 'Application Support Engineer, PCB/Data Management',\n",
       "  'JobLocation': 'Longmont, CO',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'Company: Siemens EDA\\nJob Title: Application Support Engineer, PCB/Data Management\\nJob Reference #: 378527\\nJob Location: Wilsonville, OR, Fremont, CA or Longmont, CO\\nSiemens EDA is a global technology leader in Electronic Design Automation software. Our software tools enable companies around the world to develop new and highly innovative electronic products faster and more cost-effectively. Our customers use our tools to push the boundaries of technology and physics in order to deliver better products in the increasingly complex world of chip, board and system design.\\nPosition Overview:\\nThe Siemens EDA Global Support team is seeking a dynamic, support engineer for the Xpedition Enterprise tools suite, with emphasis on EDM library and data management. Building on EDA experience and system administration skills, this individual will work directly with internal and external customers to resolve tool issues, partner with account teams to educate users and grow product usage and create knowledge content for the web-based support portal.\\nResponsibilities include, but are not limited to the following activities:\\nProactively help customers quickly become productive with Xpedition EDM Library and Design tools.\\nHelp customers achieve project deadlines by providing support by phone, electronic means or on-site.\\nEngage with existing customers to understand and provide guidance for their library development and PCB design methodology.\\nAssist account teams with struggling customers to overcome challenges.\\nInterface with customers and Electronic Board Systems engineering and provide feedback in both directions.\\nRegularly participate in meetings and conference calls with customers, both internal and external.\\nWork as part of the account team to develop solutions for customer issues and take an active role to communicate these solutions to the customer.\\nCapture knowledge for web-based support systems in the form of articles, white papers and videos.\\nProvide pro-active customer feedback into product marketing and engineering to drive improvement and future evolution of the product line.\\nActively participate in marketing and sales activities such as, workshops, seminars, etc.\\nProvide first line support for all tools in the Xpedition EDM tool suite.\\nSupport coverage focused on the western United States, collaborating with the worldwide team.\\nRequired Qualifications:\\nBroad knowledge in the ECAD Library, electrical system design, and data management domains.\\nDetailed understanding of ECAD Library creation and management using the Xpedition EDM tools, Altium 360, Cadence EDM or similar.\\nExperience with PCB design tools (including PCB data management) such as Xpedition Enterprise, Cadence Allegro, Altium or similar.\\nExperience working with Oracle or PostgreSQL based data systems.\\nSelf-motivated with the ability to work independently but collaboratively and to thrive in a fast-paced and evolving product and technical environment.\\nAbility to build strong rapport and credibility with customers’ organizations while maintaining an internal network of contacts.\\nManage multiple tasks and customer issues concurrently.\\nStrong technical communication skills and presentation skills.\\nBSEE or BSME, 5+ years of ECAD Library and electrical system design experience desired.\\nPreferred Qualifications\\nDetailed knowledge of the PCB Design process and design data management. Siemens Xpedition Enterprise EDM preferred.\\nInstallation and System administration experience of PostgreSQL and/or Oracle.\\nStrong project management and communication skills – both written and verbal. Prior project management experience a plus\\nPrior role in team leadership\\nWorking Conditions/Physical Requirements:\\nRole will be hybrid with some flexibility in location.\\nWill support discussions in various time zones, so virtual meetings outside of home time zone will sometimes be necessary.\\nThe salary range for this position is $95,800 to $172,400 and this role is eligible to earn incentive compensation. The actual compensation offered is based on the successful candidate’s work location as well as additional factors, including job-related skills, experience, and relevant education/training. Siemens offers a variety of health and wellness benefits to employees. Details regarding our benefits can be found here: www.benefitsquickstart.com. In addition, this position is eligible for time off in accordance with Company policies, including paid sick leave, paid parental leave, PTO (for non-exempt employees) or non-accrued flexible vacation (for exempt employees).\\n#LI-EDA\\n#LI-CF1\\n#DISW\\n#LI-HYBRID\\nAt Siemens we are always challenging ourselves to build a better future. We need the most innovative and diverse Digital Minds to develop tomorrow’s reality. Find out more about the Digital world of Siemens here: www.siemens.com/careers/digitalminds\\n\\n\\n\\nEqual Employment Opportunity Statement\\nSiemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law.\\n\\nEEO is the Law\\nApplicants and employees are protected under Federal law from discrimination. To learn more, Click here.\\n\\nPay Transparency Non-Discrimination Provision\\nSiemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here.\\n\\nCalifornia Privacy Notice\\nCalifornia residents have the right to receive additional notices about their personal information. To learn more, click here.',\n",
       "  'JobSalary': 'Employer Provided Salary:$96K - $172K',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Manufacturing',\n",
       "  'CompanyYearFounded': '1847',\n",
       "  'CompanyIndustry': 'Electronics Manufacturing',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'ARK Solutions\\n4.5',\n",
       "  'JobTitle': 'AWS DATA ENGINEER- Remote Role',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'AWS DATA ENGINEER\\n\\nREMOTE ROLE!!!\\n\\n\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services.\\n\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\n\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\n\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\n\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\n\\nRESPONSIBILITIES\\n\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\n\\nREQUIREMENTS\\n\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\n\\nSkills Matrix:\\nSkill\\nRequired / Desired\\nAmount\\nof Experience\\n\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nRequired\\n\\nProfessional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nRequired\\n5\\nYears\\n\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nRequired\\n5\\nYears\\n\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing a\\nRequired\\n5\\nYears\\n\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nRequired\\n5\\nYears\\n\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nRequired\\n5\\nYears\\n\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nDesired\\n\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nRequired\\n5\\nYears\\n\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nRequired\\n\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment\\nRequired',\n",
       "  'JobSalary': '$69K - $97K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.5',\n",
       "  'CompanySize': '201 to 500 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2003',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Walmart\\n3.3',\n",
       "  'JobTitle': 'Senior Software Engineer SDET - Data Ventures',\n",
       "  'JobLocation': 'Dallas, TX',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Position Summary...\\n\\nWhat you'll do...\\n\\nAs a Senior SDET (Software Development Engineer in Test), you will be responsible for test planning, test scenario design, test case development and test automation suite development, to assess and improve the overall quality of our products.\\n\\nAbout Team: Data Ventures\\nData Ventures exists to unlock the full value of Walmart's data by developing and productizing B2B data initiatives that empower merchants and suppliers to make better, faster decisions for the business. As part of this transformation, we're seeking entrepreneurial individuals to help drive data productization from concept to deployment.\\n\\nWhat you'll do:\\n\\nYou will be part of a global engineering team set up that caters to several businesses across the globe at Walmart. You will be responsible for designing and implementing functional and end-to-end tests, and will work at UI, API and database levels\\n\\nYour Responsibility shall include:\\nLeading Design, Planning and Implementation of manual & automation testing and documentation.\\nResponsible for products end to end testing methodology and strategy by following best practices.\\nCreate and execute test plans, test cases and scripts that will determine optimal application performance according to specifications.\\nContribute to planning and estimation activities, monitoring processes and reviewing QA deliverables and tasks.\\nSetup and execute manual & automation tests, derive KPIs and conduct periodic measurement analysis, deliver, and communicate test results.\\nPerform quality assurance measures and testing criteria for new applications, products, and/or enhancements to existing applications throughout their development/product lifecycles.\\nDefect reporting and retesting, performing regression testing, integration testing, end-to-end testing, API testing.\\nPerforms root cause analysis to prevent future occurrence of issues.\\nAnalyze documentation and technical specifications of any new application under development or consideration to determine its intended functionality.\\nEnsure that testing activities allow applications to meet business requirements and systems goals, fulfil end-user requirements, and identify existing or potential issues.\\nResponsible for assessing bugs priority & severity, effectively communicate to stakeholders.\\nEngage with Product Management and Business Teams across the globe to drive the agenda, understand priorities and lead towards delivering quality products.\\nTroubleshoots business and production issues by reviewing and analyzing information (for example, issue, impact, criticality, possible root cause)\\nEngage with 4iTB model across the globe to drive the agenda, understand priorities and lead towards delivering quality products.\\n\\n\\nWhat you'll bring:\\n\\nYou have a deep interest and passion for technology. You love writing and owning codes and enjoy working with people who will keep challenging you at every stage. You breadth and live large scale systems engineering. You have strong problem solving, analytic, decision-making and excellent communication with interpersonal skills. You are self-driven and motivated with the desire to work in a fast-paced, results-driven agile environment with varied responsibilities. You will also have to provide technical leadership and mentoring to a small team of highly talented and motivated engineers to deliver these solutions with highest quality.\\n\\nYour qualifications shall include:\\nBachelor's or master's degree in computer science or related field with 7+ years of experience in Computer Science or related field.\\nExperience in building UI and API Test Automation Frameworks and libraries\\nProficient in automating tests using Selenium, Cypress, WebdriverIO, Appium, or similar frameworks.\\nServer-side API testing, Integration testing and solid understanding of REST API's\\nProficient in using JIRA to log defects, develop test scenarios, execution and create status reports/dashboards.\\nExperience in writing complex SQL queries.\\nExperience in setting up CI/CD pipeline with Jenkins and GitHub\\nExperience in Agile / Scrum / Kanban model.\\nProficient in any of the programming Java, JavaScript, Python,\\nExperience in Postman, Eclipse, JUnit, Maven, Gradle, TestNG, POI and respective open source or similar tools.\\nExperience in troubleshooting test failures and make appropriate fixes.\\nExperience in performing feasibility, compatibility test & prototyping for various features.\\nIdentifying trends / Pattern Recognition skills\\nAccurately estimate timelines for test activities\\nExperience in Functional and Integration testing with cross browsers or products\\nExperience on Regression, Performance & Load Testing\\nDocumenting risks, issues, assumptions, and dependencies and proactively managing them.\\nClear communication skills to work closely with stakeholders on Sprint/Test Strategy\\nGood interpersonal skills - including negotiation, facilitation, and consensus building skills; ability to prioritize, influence and persuade without direct control.\\nAbility to work independently as well as a team player.\\n\\n\\nAbout Walmart Global Tech\\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\\n\\nFlexible, hybrid work:\\nWe use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.\\n\\nBenefits:\\nBenefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\\n\\nEqual Opportunity Employer:\\nWalmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\\n\\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\\n\\nMinimum Qualifications...\\n\\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\\n\\nOption 1: Bachelor's degree in computer science, computer engineering, computer information systems, software engineering, or related area and 3 years' experience in software engineering or related area.Option 2: 5 years' experience in software engineering or related area.\\n\\nPreferred Qualifications...\\n\\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\\n\\nMaster's degree in Computer Science, Computer Engineering, Computer Information Systems, Software Engineering, or related area and 1 year's experience in software engineering or related area.\\n\\nPrimary Location...\\n603 MUNGER AVE STE 400, DALLAS, TX 75202, United States of America\",\n",
       "  'JobSalary': '$92K - $128K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.3',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Retail & Wholesale',\n",
       "  'CompanyYearFounded': '1962',\n",
       "  'CompanyIndustry': 'General Merchandise & Superstores',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Humana\\n3.9',\n",
       "  'JobTitle': 'Data Lake Senior Software Engineer',\n",
       "  'JobLocation': 'Birmingham, AL',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': \"Do you want to be part of a team that is continuously learning and evolving to build modernized and innovative solutions? Do you want to transform an industry? Do you crave new challenges and the opportunity to solve hard customer problems using the latest in software technology? Are you motivated to transform today’s complex healthcare processes into by building analytical data lakes for data science and machine learning? We are looking for a team player to join our Data Lake Agile team as a Senior Software Engineer who is highly motivated and self-driven to make a difference by being a part of our claims modernization team by building and deploying cloud-native data lake applications and services using latest cloud technology stack to modernize our core claims system.\\nResponsibilities\\nThe Senior Software Engineer will complete software development for the Humana Data Lake team, as well as quality assurance, debugging, overseeing, and performing development testing. Researches and provides recommended technical solutions for complex data-driven business cases and software-related issues. Begins to influence the department’s strategy. Makes decisions on moderately complex to complex issues regarding technical approaches for project components, and work is performed without direction. Exercises considerable latitude in determining objectives and approaches to assignments. Follows established guidelines/procedures.\\nResponsibilities:\\nIn this role, you will lead the design, development, and support of our Humana Claim Data Lake\\nYou’ll apply your Data Lake competencies to development of new software capabilities.\\nParticipating in design and code reviews to maintain our high development standards\\nEnsure high quality software through adherence to Test Driven Development (TDD) as the foundation of development and enabling CI/CD and DevSecOps practices\\nA proven team player, you’ll have the opportunity to provide hands-on leadership while taking an active role in collaborative assessments and design\\nCollaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in data lake development tools and technologies\\nUtilize programming languages like Spark, Java, Python, Groovy, RDBMS and NoSQL databases, Container Orchestration services including Docker and Kubernetes, and a variety of cloud tools and services\\nTroubleshoots business and production issues by gathering information (for example, issue, impact, criticality, possible root cause); engaging support teams to assist in the resolution of issues; formulating an action plan; performing actions as designated in the plan; interpreting the results to determine further action; performs root cause analysis to prevent future occurrence of issues; and completing online documentation\\nRequired Qualifications\\nBachelor's Degree in Computer Science, Engineering, Management Information Systems, InformationTechnology, or 5+ years of equivalent work experience\\n5+ years of Data Lake software development on Cloud\\n5+ years of experience with Java, Python, Groovy, ETL technologies\\n4+ years of experience writing SQL/Functions/Procedures in transactional database systems such as Oracle, SQL Server, DB2\\nDeep understanding of developing cloud-based Data Lake/Delta Lake applications\\nUnderstanding of Containerization concepts.\\nPreferred Qualifications\\nMaster's Degree\\nExperience in JPA, Docker, Kubernetes or similar technologies is a significant plus\\nExperience designing and developing cloud solutions using AWS, Azure or Google is a significant plus\\nExperience in Apache Kafka\\nExperience in Docker and Kubernetes\\nAzure certifications\\nDataBricks certifications\\nExperience with Medical Insurance claim data\\nAdditional Information\\nWhy Humana?\\nAt Humana, we know your well-being is important to you, and it’s important to us too. That’s why we’re committed to making resources available to you that will enable you to become happier, healthier, and more productive in all areas of your life. Just to name a few:\\nWork-Life Balance\\nGenerous PTO package\\nHealth benefits effective day 1\\nAnnual Incentive Plan\\n401K - Company match\\nWell-being program\\nPaid Volunteer Time Off\\nStudent Loan Refinancing\\nIf you share our passion for helping people, we likely have the right place for you at Humana!\\nWork-At-Home Requirements\\nTo ensure Home or Hybrid Home/Office associates’ ability to work effectively, the self-provided internet service of Home or Hybrid Home/Office associates must meet the following criteria:\\nAt minimum, a download speed of 25 Mbps and an upload speed of 10 Mbps is recommended to support Humana applications, per associate.\\nWireless, Wired Cable or DSL connection is suggested.\\nSatellite, cellular and microwave connection can be used only if they provide an optimal connection for associates. The use of these methods must be approved by leadership. (See Wireless, Wired Cable or DSL Connection in Exceptions, Section 7.0 in this policy.)\\nHumana will not pay for or reimburse Home or Hybrid Home/Office associates for any portion of the cost of their self-provided internet service, with the exception of associates who live or work from Home in the state of California, Illinois, Montana, or South Dakota. Associates who live and work from Home in the state of California, Illinois, Montana, or South Dakota will be provided a bi-weekly payment for their internet expense.\\nHumana will provide Home or Hybrid Home/Office associates with telephone equipment appropriate to meet the business requirements for their position/job.\\nInterview Format:\\nAs part of our hiring process for this opportunity, we will be using an exciting interviewing technology called Modern Hire to enhance our hiring and decision-making ability. Modern Hire allows us to quickly connect and gain valuable information for you pertaining to your relevant skills and experience at a time that is best for your schedule.\\nIf you are selected for a first-round interview, you will receive an email/text correspondence inviting you to participate in a Modern Hire interview. In this interview, you will receive a set of interview questions over your phone and you will provide recorded or text message responses to each question. You should anticipate this interview to take about 15 minutes. Your recorded interview will be reviewed, and you will subsequently be informed if you will be moving forward to the next round of interviews.\\nSocial Security Task\\nAlert: Humana values personal identity protection. Please be aware that applicants selected for leader review may be asked to provide a social security number if it is not already on file. When required, an email will be sent from Humana@myworkday.com with instructions to add the information to the application at Humana’s secure website.\\nAt Humana, we know your well-being is important to you, and it’s important to us too. That’s why we’re committed to making resources available to you that will enable you to become happier, healthier, and more productive in all areas of your life. If you share our passion for helping people, we likely have the right place for you at Humana.\\nAfter applying, we encourage you to join our Talent Network as well, so you can stay informed and up to date on what’s happening around our organization in the changing world of healthcare.\\nThis is a remote position.\\n#LI-Remote\\n#LI-CB2\\nScheduled Weekly Hours\\n40\\n\\nNot Specified\\n0\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '3.9',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Healthcare',\n",
       "  'CompanyYearFounded': '1961',\n",
       "  'CompanyIndustry': 'Health Care Services & Hospitals',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'JPMorgan Chase Bank, N.A.\\n3.8',\n",
       "  'JobTitle': 'Big Data/ML Lead Software Engineer',\n",
       "  'JobLocation': 'Wilmington, DE',\n",
       "  'EasyApply': 'Easy Apply',\n",
       "  'JobDescription': 'We have an opportunity to impact your career and provide an adventure where you can push the limits of what\\'s possible.\\nAs a Big Data/ML Lead Software Engineer at JPMorgan Chase within the FINANCE RISK DATA AND CONTROLS in our Corporate Sector, you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. As a core technical contributor, you are responsible for conducting critical technology solutions across multiple technical areas within various business functions in support of the firm\\'s business objectives.\\nJob responsibilities\\n\\nDesign Enterprise Machine Learning platforms that are capable of running predictive models.\\nDocument Machine Learning processes and keep abreast of Developments in Machine Learning.\\nEstablish standards, guidance and best practices for ML Platform.\\nEstablish the appropriate monitoring and alerting of solution events related to performance, scalability, availability, and reliability.\\nProvides technical leadership, guidance and direction to other team members.\\nBuild prototypes for demonstration and illustration purposes for peer groups, Business partners, or senior leaders.\\nSolving complex problems with datasets and optimize existing ML libraries and frameworks.\\nNeed to have advance knowledge of application, data and infrastructure disciplines\\nPassionate about building an innovative culture\\nUnderstanding of software skills such as business analysis, development, maintenance, and software improvement\\nApply knowledge of machine learning model frameworks, algorithms and tools for building ML solutions\\nDeliver high-quality results under tight deadline\\nRequired qualifications, capabilities, and skills\\n\\nBackground with Machine Learning Frameworks and Big Data technologies such as Hadoop.\\nStrong experience in programming languages such as Java or Python\\nAbility to juggle multiple priorities and effectively deliver in a fast-paced, dynamic environment\\nStrong Delivery management capabilities\\nExperience with Cloud technologies such as AWS or Azure.\\nExperience working with databases such as Cassandra, MongoDB or Teradata\\nKnowledge of build tools like Maven and source control like Git/SVN.\\nExperience in performance tuning, code optimization\\nComfortable working in an Agile and collaborative environment.\\nStrong written and verbal skills.\\n\\nPreferred qualifications, capabilities, and skills\\nPython Machine Learning library and ecosystem experience ( Pandas and Numpy etc)\\nExperience in building ML Models, experience with model accuracy and tuning. Working with Data structured and NLP models\\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world\\'s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants\\' and employees\\' religious practices and beliefs, as well as any mental health or physical disability needs.\\nThe health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the \"WELL Health-Safety Rating\" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.\\nAs a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm\\'s current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm\\'s vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.\\n\\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\\nEqual Opportunity Employer/Disability/Veterans',\n",
       "  'JobSalary': '$111K - $152K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.8',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1799',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Locus Recruiting\\n4.0',\n",
       "  'JobTitle': 'Data Center Engineer (Structured Cabling)',\n",
       "  'JobLocation': 'United States',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Locus (Recruitment Firm) is recruiting for a Data Center (Structured Cabling) Engineers for our client.\\nPosition: Data Center (Structured Cabling) Engineer\\nDuration: 6 month contract to hire (Full-time work)\\nPay: $50-60/hr. ($100-$125,000 conversion)\\nLocation: United states\\nOvernight Shift (11pm-7am)\\nTravel 75% + each week (leave Sunday evening-return Thursday or Friday)\\nExpenses Reimbursed (Hotel, Meals, etc.)\\n40 hours a week\\nHave a need to hire 6 individuals-2 Western US, 2 Central US and 2 Eastern US\\nSkills:\\n· 3rd Shift to cover Datacenters and Critical facilities at airports while flights are not flying.\\n· Doing remediation work\\n· Following a run book\\n· Taking 12 largest airports, re-cabling and moving racks forward, putting in AC, separate power\\n· Worked in Data centers, rack and stack at airports\\n· Low voltage electrician backgrounds work is great, fibers/cabling experience\\nJob Types: Full-time, Contract\\nPay: $50.00 - $60.00 per hour\\nBenefits:\\nDental insurance\\nHealth insurance\\nVision insurance\\nSchedule:\\n8 hour shift\\nEvening shift\\nMonday to Friday\\nNight shift\\nWeekends as needed\\nApplication Question(s):\\nAre you open to a 6-month contract-to-hire position working 40 hours a week?\\nAre you comfortable traveling Sunday-Thursday each week?\\nAre you comfortable working 11pm-7am?\\nAre the pay requirements in line with what you are looking for?\\nDo you have any experience working within the Airline Industry?\\nExperience:\\nData center: 5 years (Preferred)\\nRack and Stack: 5 years (Preferred)\\nCabling: 5 years (Preferred)\\nLow Voltage: 5 years (Preferred)\\nShift availability:\\nOvernight Shift (Required)\\nWillingness to travel:\\n75% (Required)\\nWork Location: On the road',\n",
       "  'JobSalary': 'Employer Provided Salary:$50.00 - $60.00 Per Hour',\n",
       "  'CompanyRating': '4.0',\n",
       "  'CompanySize': 'Unknown',\n",
       "  'CompanyType': 'Enterprise Software & Network Solutions',\n",
       "  'CompanySector': 'Unknown / Non-Applicable',\n",
       "  'CompanyYearFounded': 'Company - Private',\n",
       "  'CompanyIndustry': 'Information Technology',\n",
       "  'CompanyRevenue': None},\n",
       " {'CompanyName': 'Cambay Consulting LLC\\n4.3',\n",
       "  'JobTitle': 'AWS Data Engineer',\n",
       "  'JobLocation': 'Des Moines, IA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'This position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services.\\nThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL. Please read through the skills section and entire description for more detail.\\nThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.\\nThis role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).\\nThe successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.\\nIndividual should work well in a team and independently as needed.\\nRESPONSIBILITIES\\nDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.\\nDevelop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.\\nCollaborate with stakeholders to understand business requirements and translate them into technical data solutions.\\nImplement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.\\nOptimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.\\nEnsure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.\\nMonitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.\\nCollaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.\\nStay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.\\nMentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.\\nREQUIREMENTS\\nBachelor’s or master’s degree in computer science, Data Engineering, or a related field.\\nMinimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.\\nStrong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.\\nProficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.\\nHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.\\nIn-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.\\nFamiliarity with big data technologies such as Hadoop, Hive, or Presto is a plus.\\nSolid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.\\nExcellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.\\nStrong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\\nJob Type: Contract\\nSchedule:\\n8 hour shift\\nExperience:\\nAWS data services and technologies: 5 years (Preferred)\\nETL processes using AWS Glue, AWS Lambda, Apache Spark: 5 years (Preferred)\\nprogramming languages such as Python, Scala, or Java: 5 years (Preferred)\\nWork Location: Remote',\n",
       "  'JobSalary': '$78K - $107K (Glassdoor est.)',\n",
       "  'CompanyRating': '4.3',\n",
       "  'CompanySize': '501 to 1000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2011',\n",
       "  'CompanyIndustry': 'Information Technology Support Services',\n",
       "  'CompanyRevenue': '$25 to $100 million (USD)'},\n",
       " {'CompanyName': 'Google\\n4.4',\n",
       "  'JobTitle': 'Manufacturing Engineer, Data Center Equipment',\n",
       "  'JobLocation': 'Atlanta, GA',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Minimum qualifications:\\nBachelor's degree in Electrical Engineering, Process Engineering, Manufacturing Engineering, a related field, or equivalent practical experience.\\n5 years of experience in manufacturing and construction.\\nExperience in process or product development.\\nExperience in Product Data Management (PDM) systems (e.g., Agile) and Bill of Materials (BoM) construction.\\n\\nPreferred qualifications:\\nMaster's degree in Engineering or equivalent practical experience.\\nExperience with LEAN Manufacturing, 5S, or 8D practices.\\nKnowledge of overseeing yield improvements and cycle time reductions for high volume and high complexity parts.\\nAbility to influence the product design process by proposing improvements with associated data and produce a timely and accurate work product.\\nAbility to travel up to 40% of the time as needed.\\nExcellent problem-solving and organizational skills with attention to details.\\nAbout the job\\nGoogle has one of the largest and most powerful computing infrastructures in the world. Your team is responsible for providing the manufacturing capability to deliver this state-of-the-art physical infrastructure. As a Manufacturing Engineer, you evaluate the product designs and create the processes, tools and procedures behind Google's powerful search technology. When vendors build parts for our infrastructure, you're right there alongside ensuring manufacturing processes are repeatable and controlled. You collaborate with Commodity Managers and Design Engineers to determine Google's infrastructure needs and product specifications. Your work ensures the various pieces of Google's infrastructure fit together perfectly and keep our systems humming along smoothly for a seamless user experience.\\nBehind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.\\nThe US base salary range for this full-time position is $108,000-$158,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\\n\\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.\\nResponsibilities\\nProvide comprehensive manufacturing engineering support to Google's global manufacturing partners during development and production builds.\\nIdentify and evaluate Contract Manufacturers (CM)/vendors to fabricate data center related products and architecture.\\nConduct manufacturing and process audits and qualify CMs/vendors. Identify, initiate, and drive process improvements that reduce process cycle times and cost, while maintaining exceptional quality.\\nResolve manufacturing and field issues by applying knowledge and skills, escalating design, manufacturing, and product test issues to others within the organization when appropriate.\\nUse Product Data Management (PDM) systems to create and maintain Bills of Materials (BoM), Approved Vendor Lists (AVL), and Engineering Change Notifications/Orders (ECN/O) in support of Google products.\\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form.\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.4',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '1998',\n",
       "  'CompanyIndustry': 'Internet & Web Services',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Siemens\\n4.2',\n",
       "  'JobTitle': 'Application Support Engineer, PCB/Data Management',\n",
       "  'JobLocation': 'Longmont, CO',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Company: Siemens EDA\\nJob Title: Application Support Engineer, PCB/Data Management\\nJob Reference #: 378527\\nJob Location: Wilsonville, OR, Fremont, CA or Longmont, CO\\nSiemens EDA is a global technology leader in Electronic Design Automation software. Our software tools enable companies around the world to develop new and highly innovative electronic products faster and more cost-effectively. Our customers use our tools to push the boundaries of technology and physics in order to deliver better products in the increasingly complex world of chip, board and system design.\\nPosition Overview:\\nThe Siemens EDA Global Support team is seeking a dynamic, support engineer for the Xpedition Enterprise tools suite, with emphasis on EDM library and data management. Building on EDA experience and system administration skills, this individual will work directly with internal and external customers to resolve tool issues, partner with account teams to educate users and grow product usage and create knowledge content for the web-based support portal.\\nResponsibilities include, but are not limited to the following activities:\\nProactively help customers quickly become productive with Xpedition EDM Library and Design tools.\\nHelp customers achieve project deadlines by providing support by phone, electronic means or on-site.\\nEngage with existing customers to understand and provide guidance for their library development and PCB design methodology.\\nAssist account teams with struggling customers to overcome challenges.\\nInterface with customers and Electronic Board Systems engineering and provide feedback in both directions.\\nRegularly participate in meetings and conference calls with customers, both internal and external.\\nWork as part of the account team to develop solutions for customer issues and take an active role to communicate these solutions to the customer.\\nCapture knowledge for web-based support systems in the form of articles, white papers and videos.\\nProvide pro-active customer feedback into product marketing and engineering to drive improvement and future evolution of the product line.\\nActively participate in marketing and sales activities such as, workshops, seminars, etc.\\nProvide first line support for all tools in the Xpedition EDM tool suite.\\nSupport coverage focused on the western United States, collaborating with the worldwide team.\\nRequired Qualifications:\\nBroad knowledge in the ECAD Library, electrical system design, and data management domains.\\nDetailed understanding of ECAD Library creation and management using the Xpedition EDM tools, Altium 360, Cadence EDM or similar.\\nExperience with PCB design tools (including PCB data management) such as Xpedition Enterprise, Cadence Allegro, Altium or similar.\\nExperience working with Oracle or PostgreSQL based data systems.\\nSelf-motivated with the ability to work independently but collaboratively and to thrive in a fast-paced and evolving product and technical environment.\\nAbility to build strong rapport and credibility with customers’ organizations while maintaining an internal network of contacts.\\nManage multiple tasks and customer issues concurrently.\\nStrong technical communication skills and presentation skills.\\nBSEE or BSME, 5+ years of ECAD Library and electrical system design experience desired.\\nPreferred Qualifications\\nDetailed knowledge of the PCB Design process and design data management. Siemens Xpedition Enterprise EDM preferred.\\nInstallation and System administration experience of PostgreSQL and/or Oracle.\\nStrong project management and communication skills – both written and verbal. Prior project management experience a plus\\nPrior role in team leadership\\nWorking Conditions/Physical Requirements:\\nRole will be hybrid with some flexibility in location.\\nWill support discussions in various time zones, so virtual meetings outside of home time zone will sometimes be necessary.\\nThe salary range for this position is $95,800 to $172,400 and this role is eligible to earn incentive compensation. The actual compensation offered is based on the successful candidate’s work location as well as additional factors, including job-related skills, experience, and relevant education/training. Siemens offers a variety of health and wellness benefits to employees. Details regarding our benefits can be found here: www.benefitsquickstart.com. In addition, this position is eligible for time off in accordance with Company policies, including paid sick leave, paid parental leave, PTO (for non-exempt employees) or non-accrued flexible vacation (for exempt employees).\\n#LI-EDA\\n#LI-CF1\\n#DISW\\n#LI-HYBRID\\nAt Siemens we are always challenging ourselves to build a better future. We need the most innovative and diverse Digital Minds to develop tomorrow’s reality. Find out more about the Digital world of Siemens here: www.siemens.com/careers/digitalminds\\n\\n\\n\\nEqual Employment Opportunity Statement\\nSiemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law.\\n\\nEEO is the Law\\nApplicants and employees are protected under Federal law from discrimination. To learn more, Click here.\\n\\nPay Transparency Non-Discrimination Provision\\nSiemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here.\\n\\nCalifornia Privacy Notice\\nCalifornia residents have the right to receive additional notices about their personal information. To learn more, click here.',\n",
       "  'JobSalary': 'Employer Provided Salary:$96K - $172K',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Manufacturing',\n",
       "  'CompanyYearFounded': '1847',\n",
       "  'CompanyIndustry': 'Electronics Manufacturing',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'The Judge Group\\n3.7',\n",
       "  'JobTitle': 'Senior Data Engineer',\n",
       "  'JobLocation': 'Cincinnati, OH',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Our client is currently seeking a Senior Data Engineer\\n\\n\\nJob Description:\\nAccountable for developing and delivering technological responses to targeted business outcomes.\\nAnalyze, design, and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration.\\n2+ years? experience with automation production systems (Ansible Tower, Jenkins, Puppet, or Selenium)\\nWorking knowledge of databases and SQL Experience with software development methodologies and SDLC Candidate possess a problem-solving attitude and can work independently\\nMust be very organized, able to balance multiple priorities, and self-motivated\\nKey Responsibilities:\\nExperience in administration and configuration of API Gateways (e.g. Apigee/Kong) Apply cloud computing skill to deploy upgrades and fixes\\nDesign, develop, and implement integrations based on use feedback.\\nTroubleshoot production issues and coordinate with the development team to streamline code deployment.\\nImplement automation tools and frameworks (Ci/CD pipelines).\\nAnalyze code and communicate detailed reviews to development teams to ensure a marked improvement in applications and the timely completion of products.\\nCollaborate with team members to improve the company?s engineering tools, systems and procedures, and data security.',\n",
       "  'JobSalary': '$102K - $133K (Glassdoor est.)',\n",
       "  'CompanyRating': '3.7',\n",
       "  'CompanySize': '1001 to 5000 Employees',\n",
       "  'CompanyType': 'Company - Private',\n",
       "  'CompanySector': 'Human Resources & Staffing',\n",
       "  'CompanyYearFounded': '1970',\n",
       "  'CompanyIndustry': 'HR Consulting',\n",
       "  'CompanyRevenue': '$100 to $500 million (USD)'},\n",
       " {'CompanyName': 'Siemens Digital Industries Software\\n4.2',\n",
       "  'JobTitle': 'Application Support Engineer, PCB/Data Management',\n",
       "  'JobLocation': 'Longmont, CO',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Company: Siemens EDA\\nJob Title: Application Support Engineer, PCB/Data Management\\nJob Reference #: 378527\\nJob Location: Wilsonville, OR, Fremont, CA or Longmont, CO\\nSiemens EDA is a global technology leader in Electronic Design Automation software. Our software tools enable companies around the world to develop new and highly innovative electronic products faster and more cost-effectively. Our customers use our tools to push the boundaries of technology and physics in order to deliver better products in the increasingly complex world of chip, board and system design.\\nPosition Overview:\\nThe Siemens EDA Global Support team is seeking a dynamic, support engineer for the Xpedition Enterprise tools suite, with emphasis on EDM library and data management. Building on EDA experience and system administration skills, this individual will work directly with internal and external customers to resolve tool issues, partner with account teams to educate users and grow product usage and create knowledge content for the web-based support portal.\\nResponsibilities include, but are not limited to the following activities:\\nProactively help customers quickly become productive with Xpedition EDM Library and Design tools.\\nHelp customers achieve project deadlines by providing support by phone, electronic means or on-site.\\nEngage with existing customers to understand and provide guidance for their library development and PCB design methodology.\\nAssist account teams with struggling customers to overcome challenges.\\nInterface with customers and Electronic Board Systems engineering and provide feedback in both directions.\\nRegularly participate in meetings and conference calls with customers, both internal and external.\\nWork as part of the account team to develop solutions for customer issues and take an active role to communicate these solutions to the customer.\\nCapture knowledge for web-based support systems in the form of articles, white papers and videos.\\nProvide pro-active customer feedback into product marketing and engineering to drive improvement and future evolution of the product line.\\nActively participate in marketing and sales activities such as, workshops, seminars, etc.\\nProvide first line support for all tools in the Xpedition EDM tool suite.\\nSupport coverage focused on the western United States, collaborating with the worldwide team.\\nRequired Qualifications:\\nBroad knowledge in the ECAD Library, electrical system design, and data management domains.\\nDetailed understanding of ECAD Library creation and management using the Xpedition EDM tools, Altium 360, Cadence EDM or similar.\\nExperience with PCB design tools (including PCB data management) such as Xpedition Enterprise, Cadence Allegro, Altium or similar.\\nExperience working with Oracle or PostgreSQL based data systems.\\nSelf-motivated with the ability to work independently but collaboratively and to thrive in a fast-paced and evolving product and technical environment.\\nAbility to build strong rapport and credibility with customers’ organizations while maintaining an internal network of contacts.\\nManage multiple tasks and customer issues concurrently.\\nStrong technical communication skills and presentation skills.\\nBSEE or BSME, 5+ years of ECAD Library and electrical system design experience desired.\\nPreferred Qualifications\\nDetailed knowledge of the PCB Design process and design data management. Siemens Xpedition Enterprise EDM preferred.\\nInstallation and System administration experience of PostgreSQL and/or Oracle.\\nStrong project management and communication skills – both written and verbal. Prior project management experience a plus\\nPrior role in team leadership\\nWorking Conditions/Physical Requirements:\\nRole will be hybrid with some flexibility in location.\\nWill support discussions in various time zones, so virtual meetings outside of home time zone will sometimes be necessary.\\nThe salary range for this position is $95,800 to $172,400 and this role is eligible to earn incentive compensation. The actual compensation offered is based on the successful candidate’s work location as well as additional factors, including job-related skills, experience, and relevant education/training. Siemens offers a variety of health and wellness benefits to employees. Details regarding our benefits can be found here: www.benefitsquickstart.com. In addition, this position is eligible for time off in accordance with Company policies, including paid sick leave, paid parental leave, PTO (for non-exempt employees) or non-accrued flexible vacation (for exempt employees).\\n\\\\#LI-EDA\\n\\\\#LI-CF1\\n\\\\#DISW\\n\\\\#LI-HYBRID\\nAt Siemens we are always challenging ourselves to build a better future. We need the most innovative and diverse Digital Minds to develop tomorrow’s reality. Find out more about the Digital world of Siemens here: www.siemens.com/careers/digitalminds\\nEqual Employment Opportunity Statement\\nSiemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law.\\nEEO is the Law\\nApplicants and employees are protected under Federal law from discrimination. To learn more, Click here.\\nPay Transparency Non-Discrimination Provision\\nSiemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here.\\nCalifornia Privacy Notice\\nCalifornia residents have the right to receive additional notices about their personal information. To learn more, click here.\\nJob Family: Customer Services\\nReq ID: 378527',\n",
       "  'JobSalary': 'Employer Provided Salary:$96K - $172K',\n",
       "  'CompanyRating': '4.2',\n",
       "  'CompanySize': '5001 to 10000 Employees',\n",
       "  'CompanyType': 'Subsidiary or Business Segment',\n",
       "  'CompanySector': 'Information Technology',\n",
       "  'CompanyYearFounded': '2004',\n",
       "  'CompanyIndustry': 'Software Development',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'JPMorgan Chase Bank, N.A.\\n3.8',\n",
       "  'JobTitle': 'Lead Software Engineer - Data Science & Machine Learning',\n",
       "  'JobLocation': 'Jersey City, NJ',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': 'Be an integral part of an agile team that\\'s constantly pushing the envelope to enhance, build, and deliver top-notch technology products.\\nAs a Lead Software Engineer at JPMorgan Chase, you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. Drive significant business impact through your capabilities and contributions, and apply deep technical expertise and problem-solving methodologies to tackle a diverse array of challenges that span multiple technologies and applications.\\nJob responsibilities :\\n\\nBe part of a high-performing team developing and implementing AI algorithms to help address solve complex problems and create unique solutions in Banking (Identity and Access management, Cyber Security space )\\nAssist with the evaluation, upkeep and improvement of the architecture and implementation of our MLOPs framework.\\nBuild MLOPs pipeline independently to support development, experimentation, continuous integration, continuous delivery, validation and monitoring AI/ML Models\\nContribute to architectural design discussions that meet Banking Compliance, performance stability, and operational requirements.\\nBuild Machine Learning Models from end to end and implement them in Production.\\nDevelop User Interface (I) for ML Models ( good to have but not mandatory).\\nIdentify valuable data sources and perform independent analysis.\\nAnalyze large amounts of data and find patterns in it.\\nCommunicate information effectively using data visualization and storytelling.\\nCollaborate with various teams for information collection, presentation, and liaison.\\nWork with CI/ CD pipeline, Jenkins, Docker,Kubernetes,Terraform Implementation, and Infrastructure as code.\\nUse Splunk, Data Dog, and CloudWatch monitoring tools.\\nContribute to architecture and structure discussions for production - grade platforms.\\nKnowledge of Python BDD (Behavior-Driven development)Testing, Pytest , Unit Testing and Jules pipeline building.\\n\\nRequired qualifications, capabilities, and skills:\\nBachelors Degree in Computer Science (preferably), Data Analytics, Software or Computer Engineering, Computational Statistics, Mathematics or a closely related discipline.\\nStrong understanding of basic concepts of ML and MLOPs\\n6+ years\\' experience in Data Science Model building\\nGood Experience in SQL\\nExperience in Python, Pyspark and Spark\\nExperience working with Structured and Unstructured data.\\nExcellent Team Player skills.\\nGood communication skills to present information using data visualization and story telling\\n\\nPreferred qualifications, capabilities, and skills :\\nImplemented 10 + models end to end in Production, including NLP, predictive models, time series forecasting, and deep learning.\\nPreferred background in Identity and access management in Banking\\nWorking experience in AWS Cloud\\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world\\'s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants\\' and employees\\' religious practices and beliefs, as well as any mental health or physical disability needs.\\nThe health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the \"WELL Health-Safety Rating\" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.\\nAs a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm\\'s current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm\\'s vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.\\n\\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\\nEqual Opportunity Employer/Disability/Veterans\\nBase Pay/Salary\\nJersey City,NJ $137,750.00 - $200,000.00 / year',\n",
       "  'JobSalary': 'Employer Provided Salary:$138K - $200K',\n",
       "  'CompanyRating': '3.8',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Financial Services',\n",
       "  'CompanyYearFounded': '1799',\n",
       "  'CompanyIndustry': 'Banking & Lending',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Lockheed Martin\\n4.1',\n",
       "  'JobTitle': 'Software Engineer Senior Staff - Data Architect',\n",
       "  'JobLocation': 'Fort Worth, TX',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Job ID: 642030BR\\nDate posted: Jul. 12, 2023\\n\\nDescription:- Collaborate with government counterparts to design and implement a Central Repository hosted in a Cloud environment used for Fleet Management and machine learning.\\n\\nCollaborate with government counterparts to define data archival, retrieval and retention policies.\\n\\nCollaborate with government counterparts, external contractors, and cyber security to design and implement an interface that will be used to provide data to external systems.\\n\\nAnalyze Tech Debt, existing PTRs, and program recommendations to ensure ODIN data design aligns to and supports program roadmap.\\n\\nProvide insight to ongoing ALIS/ODIN data efforts which include logical and physical data model design, maturation of centralizing data from edge nodes to a Central Repository, development of a user interface used to present and review fleet data.\\n\\nConduct domain driven design focused on data ownership at the domain, subdomain, and micro service levels.\\n\\nWork with domain teams to drive technical implementation based on the data design.\\n\\nWork with SMEs and existing analysis to assess as-is data design.\\n\\nIdentify opportunities to integrate data design technical debt burndown into planned work.\\n\\nDrive database schema management technologies.\\n\\nDrive cross-domain data access patterns and APIs.\\nBasic Qualifications:\\nA Bachelor's degree in Computer Science, Software Engineering, Systems Engineering, Information Technology, Mathematics, or other relevant area.\\n\\nExperience with software architecture or enterprise architecture methodologies.\\n\\nStrong analytical skills and understanding of data warehouses, data elements and application software solutions to optimize data gathering and analysis.\\n\\nExperience in developing data architectural diagrams, models, and database systems (SQL and NO SQL).\\nDesired Skills:\\nData APIs\\nDistributed data systems (e.g., Hadoop, HBase, Cassandra, Spark)\\nExtraction, Transformation and Load (ETL) tools\\nExperience in leading data architecture initiatives and develop transformation architecture for the project/product initiatives.\\nExperience with cloud-hosted data, analytics, and/or AI solutions implementation\\nExperience implementing, upgrading, migrating, and/or retiring data, analytics, or AI platforms\\nPrior experience with one or more industry-standard architectural frameworks, such as DoDAF, UAF, SysML, or MBSE\\nPossess strong problem-solving skills and excellent oral and written communication skills\\nAbility to document technical solutions in support of standards and best practices\\nUnderstands the data lifecycle and can interpret and communicate the state of data throughout the processing pipeline\\nSecurity Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.\\nClearance Level: Secret\\nOther Important Information You Should Know\\nExpression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.\\nAbility to Work Remotely: Part-time Remote Telework: The employee selected for this position will work part of their work schedule remotely and part of their work schedule at a designated Lockheed Martin facility. The specific weekly schedule will be discussed during the hiring process.\\nWork Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.\\nSchedule for this Position: 4x10 hour day, 3 days off per week\\nLockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.\\nAt Lockheed Martin, we use our passion for purposeful innovation to help keep people safe and solve the world's most complex challenges. Our people are some of the greatest minds in the industry and truly make Lockheed Martin a great place to work.\\n\\nWith our employees as our priority, we provide diverse career opportunities designed to propel, develop, and boost agility. Our flexible schedules, competitive pay, and comprehensive benefits enable our employees to live a healthy, fulfilling life at and outside of work. We place an emphasis on empowering our employees by fostering an inclusive environment built upon integrity and corporate responsibility.\\n\\nIf this sounds like a culture you connect with, you're invited to apply for this role. Or, if you are unsure whether your experience aligns with the requirements of this position, we encourage you to search on Lockheed Martin Jobs, and apply for roles that align with your qualifications.\\nExperience Level: Experienced Professional\\nBusiness Unit: AERONAUTICS COMPANY\\nRelocation Available: No\\nCareer Area: Software Engineering\\nType: Full-Time\\nShift: First\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Aerospace & Defense',\n",
       "  'CompanyYearFounded': '1995',\n",
       "  'CompanyIndustry': 'Aerospace & Defense',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'},\n",
       " {'CompanyName': 'Lockheed Martin\\n4.1',\n",
       "  'JobTitle': 'Test Instrumentation and Data Acquisition Engineer, Senior',\n",
       "  'JobLocation': 'Littleton, CO',\n",
       "  'EasyApply': None,\n",
       "  'JobDescription': \"Job ID: 635000BR\\nDate posted: Apr. 17, 2023\\nProgram: ATLO\\n\\nDescription:The coolest jobs on this planet... or any other... are with Lockheed Martin Space\\n\\nAt the dawn of a new space age, Lockheed Martin is a pioneer, partner, innovator and builder. Our amazing people are on a mission to make a difference in the world and every day we use our unique skills and experiences to create, design and build solutions to some of the worlds' hardest engineering problems. Our culture encourages employees to dream big, perform with excellence and create incredible products. We provide the resources, inspiration and focus and if you have the passion and courage to dream big, we want to build a better tomorrow with you.\\n\\nGoing to space is just the beginning. It's what you do when you get there that matters. Lockheed Martin builds the satellites and spacecraft that do amazing things in space for government and commercial customers. Connecting people. Advancing discovery. And protecting what matters most. Lockheed Martin-built satellites give earlier warning of severe weather, connect troops on the battlefield, and deliver GPS directions to a billion people worldwide.\\n\\nAs we look to the future, we're driving innovations to help our customers do even more in orbit. That's why we're crafting smarter satellites that operate like smartphones in the sky, with apps that can be updated in orbit so they can adapt as mission needs on the ground change. Your mission is ours. And as that mission evolves, we'll be ready. Will you?\\n\\nEvery day, our 115,000 employees come to work with one focus - our customers' missions. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They're dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.\\n\\nLockheed Martin. Your Mission is Ours.\\n\\nSeeking a Test Instrumentation and Data Acquisition Engineer, Senior to plan and execute instrumentation installation and test campaigns for component, subsystem and vehicle-level structural, dynamic and thermal vacuum testing. This role actively supports testing on multiple programs in all LM Space lines of business, in civil/commercial space, human space exploration, and classified programs. While the position requires a high-level clearance, work in a SCIF would comprise ~35% of the role's scope.\\n\\nSummary of Key Duties for this position:\\nWork closely with technical experts and test support staff meeting needs of the programs.\\nPrepare work instructions for instrumentation installation and checkout.\\nLead technicians efforts to execute instrumentation tasks.\\nOperate and troubleshoot control systems, data acquisition systems and automated test equipment.\\nDevelop and review test procedures.\\nAssist with preparing and executing Technical Interchange Meetings and pre-test meetings.\\nSupport test discrepancy resolution and other troubleshooting activities.\\n\\n#SpaceVets #ATLO\\nBasic Qualifications:\\nBachelors or Masters degree from an accredited college/university (Mechanical Engineering, Aerospace Engineering, Electrical Engineering or a related discipline preferred) or equivalent experience/combined education.\\nExperience with electrical test equipment such as bench top power supplies, oscilloscopes, digital multimeters, data acquisition and control systems, and video systems.\\nFamiliarity with principles of operation of transducers.\\nExperience with environmental and structural testing.\\nStrong communication skills.\\n\\nCurrent Top Secret clearance. Must be a US citizen, will be subject to a government security investigation, and must meet eligibility requirements for an elevated clearance for access to classified information.\\nDesired Skills:\\nCurrent SCI w/ Poly\\nExtensive experience working in the test lab environment; experience with large-scale structural testing, system-level vibration and acoustic testing, and thermal/vacuum testing with aerospace applications.\\nExperience with mechanical test instrumentation.\\nFamiliarity with bridge sensors, bridge circuitry, and transducer fabrication techniques.\\nFamiliarity with sensors including accelerometers, strain gages, thermocouples, RTDs, load cells, microphones.\\nExperience with data manipulation in Matlab and Excel very valuable; structured programming experience desired.\\nFamiliarity with automated test equipment; experience with Bruel&Kjaer, Dewetron, Dewesoft and m+p equipment and software.\\nSecurity Clearance Statement: This position requires a government security clearance, you must be a US Citizen for consideration.\\nClearance Level: TS/SCI w/Poly\\nOther Important Information You Should Know\\nExpression of Interest: By applying to this job, you are expressing interest in this position and could be considered for other career opportunities where similar skills and requirements have been identified as a match. Should this match be identified you may be contacted for this and future openings.\\nAbility to Work Remotely: Onsite Full-time: The work associated with this position will be performed onsite at a designated Lockheed Martin facility.\\nWork Schedules: Lockheed Martin supports a variety of alternate work schedules that provide additional flexibility to our employees. Schedules range from standard 40 hours over a five day work week while others may be condensed. These condensed schedules provide employees with additional time away from the office and are in addition to our Paid Time off benefits.\\nSchedule for this Position: 9x80 every other Friday off\\nPay Rate:\\nThe annual base salary range for this position in Colorado or Washington is $82,200 - $157,500. Please note that the salary information is a general guideline only. Lockheed Martin considers factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience, education/ training, key skills as well as market and business considerations when extending an offer.\\n\\nBenefits offered: Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Flexible Spending Accounts, EAP, Education Assistance, Parental Leave, Paid time off, and Holidays.\\n(Washington state applicants only) Non-represented full time employees: accrue 10 hours per month of Paid Time Off (PTO); receive 40 hours of Granted PTO annually for incidental absences; receive at least 90 hours for holidays. Represented full time employees accrue 6.67 hours of PTO per month; accrue up to 52 hours of sick leave annually; receive at least 96 hours for holidays. PTO is prorated based on hours worked and start date during the calendar year.\\n\\nThis position is incentive plan eligible.\\nLockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.\\nJoin us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They're dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.\\n\\nAs a leading technology innovation company, Lockheed Martin's vast team works with partners around the world to bring proven performance to our customers' toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.\\nExperience Level: Experienced Professional\\nBusiness Unit: SPACE\\nRelocation Available: Possible\\nCareer Area: Test Engineering\\nType: Full-Time\\nShift: First\",\n",
       "  'JobSalary': None,\n",
       "  'CompanyRating': '4.1',\n",
       "  'CompanySize': '10000+ Employees',\n",
       "  'CompanyType': 'Company - Public',\n",
       "  'CompanySector': 'Aerospace & Defense',\n",
       "  'CompanyYearFounded': '1995',\n",
       "  'CompanyIndustry': 'Aerospace & Defense',\n",
       "  'CompanyRevenue': '$10+ billion (USD)'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test single element, use for debugging\n",
    "\n",
    "# # Set selenium driver path\n",
    "# with webdriver.Edge('../SeleniumDrivers/msedgedriver.exe') as driver:\n",
    "#     # Open Glasdoor page: Last 24 hours of job postings for data engineers in USA\n",
    "#     url = 'https://www.glassdoor.com/Job/seattle-data-engineer-jobs-SRCH_IL.0,7_IC1150505_KO8,21.htm?fromAge=30'\n",
    "#     driver.get(url)\n",
    "#     driver.maximize_window()\n",
    "#     # Wait for page to load\n",
    "#     time.sleep(2)\n",
    "#     next_page = True\n",
    "#     # print(page_count(driver, True))\n",
    "#     for p in range(int(page_count(driver, True))):\n",
    "#         exit_prompt(driver)\n",
    "#         print(f'Page {p + 1} URL: {driver.current_url}')\n",
    "    \n",
    "#     # # print(driver.find_element_by_xpath('//*[@class=\"job-title mt-xsm\"]').text)\n",
    "#         job_posts = driver.find_elements_by_class_name('react-job-listing')\n",
    "#         print(job_posts)\n",
    "#         print(f'Job Posts: {len(job_posts)}')\n",
    "#         page_element = f'//*[@id=\"MainCol\"]/div[2]/div/div[1]/button[{p + 3}]'\n",
    "#         try:\n",
    "#             driver.find_element_by_xpath(page_element).click()\n",
    "#             exit_prompt(driver)\n",
    "#             print('Success!')\n",
    "#             time.sleep(10)\n",
    "#     #     # Catch exception, boolean False.\n",
    "#         except NoSuchElementException:\n",
    "#             print('No Page Element Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glassdoor_aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
