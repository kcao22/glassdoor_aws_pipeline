import boto3
import awswrangler as wr
from urllib.parse import unquote_plus

# Execute lambda_handler function when triggered
def lambda_handler(event, context):
    key = None
    bucket = None
    # From trigger event data
    # Get the source bucket and object name as passed to the Lambda function
    # Sample of records generated by trigger event: https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = unquote_plus(record['s3']['object']['key'])
    
    # Set DB name
    # Set table name. 
    # Raw files uploaded as:'raw/Raw_%Y%m%d.json'. Split raw file on '/' and use 'Raw_%Y%m%d' as table name.
    key_list = key.split('/')
    print(f'key_list: {key_list}')
    landing_db_name = 'glassdoorlanding'
    clean_db_name = 'glassdoorcleaned'
    raw_table_name = key_list[-1].split('.')[0]
    clean_table_name = f'Clean_{raw_table_name[4:]}'
    
    # Print statements for debugging
    print(f'Bucket: {bucket}')
    print(f'Key: {key}')
    print(f'Landing DB Name: {landing_db_name}')
    print(f'Clean DB Name: {clean_db_name}')
    print(f'Table Name: {raw_table_name}')
    
    # Specify landing bucket and target bucket (clean)
    input_path = f's3://{bucket}/{key}'
    print(f'Input_Path: {input_path}')
    output_path = f's3://kc-glassdoor-data-clean/clean/{clean_table_name}'
    print(f'Output_Path: {output_path}')
    
    # Read JSON file from S3 bucket.
    # Filter out null rows. Typically jobs without identifiable company names, job titles, and locations will have nulls for almost all other fields.
    input_df = wr.s3.read_json([input_path])
    input_df = input_df.dropna(subset=['CompanyName', 'JobTitle', 'JobLocation'], thresh=2)
    
    current_databases = wr.catalog.databases()
    wr.catalog.databases()
    for name in [landing_db_name, clean_db_name]:
        if name not in current_databases.values:
            print(f'- Database {name} does not exist ... creating')
            wr.catalog.create_database(name)
        else:
            print(f'- Database {name} already exists')
    
    result = wr.s3.to_parquet(
        df=input_df,
        path=output_path,
        dataset=True,
        database=clean_db_name,
        table=clean_table_name,
        mode="append")
        
    print("RESULT: ")
    print(f'{result}')
    
    return result
