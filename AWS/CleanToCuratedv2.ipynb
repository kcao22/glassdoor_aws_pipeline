{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 0.37.3 \n"
					]
				},
				{
					"data": {
						"text/markdown": [
							"\n",
							"# Available Magic Commands\n",
							"\n",
							"## Sessions Magic\n",
							"\n",
							"----\n",
							"    %help                             Return a list of descriptions and input types for all magic commands. \n",
							"    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n",
							"    %region             String        Specify the AWS region in which to initialize a session. \n",
							"                                      Default from ~/.aws/config on Linux or macOS, \n",
							"                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n",
							"    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n",
							"                                      Default: 2880 minutes (48 hours).\n",
							"    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n",
							"                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n",
							"                                      a random UUID will be generated.\n",
							"    %status                           Returns the status of the current Glue session including its duration, \n",
							"                                      configuration and executing user / role.\n",
							"    %session_id                       Returns the session ID for the running session. \n",
							"    %list_sessions                    Lists all currently running sessions by ID.\n",
							"    %stop_session                     Stops the current session.\n",
							"    %glue_version       String        The version of Glue to be used by this session. \n",
							"                                      Currently, the only valid options are 2.0 and 3.0. \n",
							"                                      Default: 2.0.\n",
							"----\n",
							"\n",
							"## Selecting Job Types\n",
							"\n",
							"----\n",
							"    %streaming          String        Sets the session type to Glue Streaming.\n",
							"    %etl                String        Sets the session type to Glue ETL.\n",
							"    %glue_ray           String        Sets the session type to Glue Ray.\n",
							"----\n",
							"\n",
							"## Glue Config Magic \n",
							"*(common across all job types)*\n",
							"\n",
							"----\n",
							"\n",
							"    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n",
							"                                      a session. Each parameter can be specified here or through individual magics.\n",
							"    %iam_role           String        Specify an IAM role ARN to execute your session with.\n",
							"                                      Default from ~/.aws/config on Linux or macOS, \n",
							"                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n",
							"    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n",
							"                                      when a session runs.\n",
							"                                      Default: 5.\n",
							"    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n",
							"                                      (can be from Pypi or S3).\n",
							"----\n",
							"\n",
							"                                      \n",
							"## Magic for Spark Jobs (ETL & Streaming)\n",
							"\n",
							"----\n",
							"    %worker_type        String        Set the type of instances the session will use as workers. \n",
							"                                      ETL and Streaming support G.1X, G.2X, G.4X and G.8X. \n",
							"                                      Default: G.1X.\n",
							"    %connections        List          Specify a comma separated list of connections to use in the session.\n",
							"    %extra_py_files     List          Comma separated list of additional Python files From S3.\n",
							"    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n",
							"    %spark_conf         String        Specify custom spark configurations for your session. \n",
							"                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
							"----\n",
							"                                      \n",
							"## Magic for Ray Job\n",
							"\n",
							"----\n",
							"    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n",
							"                                      Default: 1.\n",
							"    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n",
							"                                      Minimum: 0. Maximum: 100.\n",
							"    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n",
							"                                      Minimum: 0. Maximum: 100.\n",
							"----\n",
							"\n",
							"## Action Magic\n",
							"\n",
							"----\n",
							"\n",
							"    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n",
							"                                      as part of the SQL code.  \n",
							"----\n",
							"\n"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				}
			],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 3.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"import datetime\n",
				"import pyspark.sql.functions as F\n",
				"import pytz\n",
				"from awsglue.dynamicframe import DynamicFrame"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# Read dimensional tables in.\n",
				"dimCities = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimcities'\n",
				").toDF()\n",
				"\n",
				"dimStates = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimstates'\n",
				").toDF()\n",
				"\n",
				"dimSize = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimsize'\n",
				").toDF()\n",
				"\n",
				"dimSector = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimsector'\n",
				").toDF()\n",
				"\n",
				"dimIndustry = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimindustry'\n",
				").toDF()\n",
				"\n",
				"dimRevenue = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimrevenue'\n",
				").toDF()\n",
				"\n",
				"dimType = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimtype'\n",
				").toDF()\n",
				"\n",
				"dimEducation = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimeducation'\n",
				").toDF()\n",
				"\n",
				"dimExperience = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcurated',\n",
				"    table_name='curated_dimexperience'\n",
				").toDF()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def transform_data(df):\n",
				"    '''\n",
				"    Performs PySpark transformations on clean data into useable data for analysis.\n",
				"    \n",
				"    ARGUMENTS:\n",
				"        - df: Spark DataFrame\n",
				"    RETURNS:\n",
				"        - Spark DataFrame\n",
				"    '''\n",
				"    # Job Location\n",
				"    df = df.withColumn('jobcity', \n",
				"                       F.when(\n",
				"                            F.col('joblocation') == 'Remote', 'Remote'\n",
				"                            )\n",
				"                            .otherwise(\n",
				"                                F.split(F.col('joblocation'), pattern=',')[0]\n",
				"                            )\n",
				"                        ) \\\n",
				"            .withColumn('jobcity',\n",
				"                        F.trim(F.col('jobcity'))\n",
				"                        ) \\\n",
				"            .withColumn('jobstate', \n",
				"                        F.when(\n",
				"                            F.col('joblocation') == 'Remote', 'Remote'\n",
				"                            ).\n",
				"                            otherwise(\n",
				"                                F.element_at(F.split(F.col('joblocation'), pattern=','), -1)\n",
				"                                )\n",
				"                        ) \\\n",
				"            .withColumn('jobstate',\n",
				"                        F.trim(F.col('jobstate'))\n",
				"                        )\n",
				"    # Company Type and Company Name\n",
				"    df = df.withColumn('companytype', \n",
				"                        F.when(\n",
				"                            F.col('companytype')=='Other', F.col('companytype')\n",
				"                            )\n",
				"                            .otherwise(\n",
				"                                F.element_at(F.split(F.col('companytype'), pattern=' - '), -1)\n",
				"                                )\n",
				"                        ) \\\n",
				"                        .withColumn('companytype',\n",
				"                            F.trim(F.col('companytype'))\n",
				"                        ) \\\n",
				"            .withColumn('companyname', \n",
				"                        F.split(\n",
				"                            F.col('companyname'), pattern='\\n'\n",
				"                            )[0]\n",
				"                        ) \\\n",
				"            .withColumn('companyname',\n",
				"                        F.regexp_replace(\n",
				"                            F.col('companyname'), ',', '')\n",
				"                       ) \\\n",
				"            .withColumn('companyname',\n",
				"                        F.trim(F.col('companyname'))\n",
				"                        )\n",
				"    # Salary\n",
				"    df = df.withColumn('jobsalary', \n",
				"                        F.regexp_replace(\n",
				"                            F.col('jobsalary'), 'Employer Provided Salary:', '')\n",
				"                        ) \\\n",
				"            .withColumn('jobsalary', \n",
				"                        F.regexp_replace(\n",
				"                            F.col('jobsalary'), ' \\(Glassdoor est.\\)', '')\n",
				"                        ) \\\n",
				"            .withColumn('jobsalary',\n",
				"                        F.when(\n",
				"                            F.col('jobsalary').contains('Hour'), \n",
				"                            F.split(F.col('jobsalary'), pattern=' Per Hour')[0]\n",
				"                            ).otherwise(\n",
				"                                F.col('jobsalary')    \n",
				"                            )\n",
				"                        ) \\\n",
				"            .withColumn('jobsalary',\n",
				"                        F.regexp_replace(F.col('jobsalary'), '\\$', '')\n",
				"                    ) \\\n",
				"            .withColumn('minsalary',\n",
				"                        F.split(\n",
				"                            F.col('jobsalary'), pattern=' - '\n",
				"                            )[0]\n",
				"                        ) \\\n",
				"            .withColumn('minsalary',\n",
				"                        F.trim(F.col('minsalary'))\n",
				"                        ) \\\n",
				"            .withColumn('maxsalary',\n",
				"                        F.element_at(F.split(\n",
				"                            F.col('jobsalary'), pattern=' - '\n",
				"                            ), -1)\n",
				"                        ) \\\n",
				"            .withColumn('maxsalary',\n",
				"                        F.trim(F.col('maxsalary'))\n",
				"                        ) \\\n",
				"            .withColumn('minsalary',\n",
				"                F.when(\n",
				"                    F.col('minsalary').endswith('K'), \n",
				"                    F.regexp_replace(F.col('minsalary'), 'K', '000')\n",
				"                    ).otherwise(\n",
				"                        F.col('minsalary') * 2080\n",
				"                    )\n",
				"                ) \\\n",
				"            .withColumn('maxsalary',\n",
				"                        F.when(\n",
				"                            F.col('maxsalary').endswith('K'), \n",
				"                            F.regexp_replace(F.col('maxsalary'), 'K', '000')\n",
				"                            ).otherwise(\n",
				"                                F.col('maxsalary') * 2080\n",
				"                            )\n",
				"                        ) \\\n",
				"            .withColumn('averagesalary',\n",
				"                (F.col('minsalary') + F.col('maxsalary')) / 2\n",
				"            )\n",
				"    # Company Age\n",
				"    df = df.withColumn('companyage', \n",
				"                       datetime.datetime.now().year - F.col('companyyearfounded')\n",
				"                       ) \\\n",
				"            .withColumn('companyage',\n",
				"                       F.trim(F.col('companyage'))\n",
				"                       )\n",
				"    \n",
				"    # Education levels\n",
				"    phds = {'phd','doctorate', 'postdoc'}\n",
				"    masters  = {'msc', 'master'}\n",
				"    undergraduate = {'bachelors', 'undergraduate', 'associates'}\n",
				"\n",
				"    df = df.withColumn('educationlevel',\n",
				"                        F.when(\n",
				"                                F.lower(F.col('jobdescription')).rlike('|'.join(phds)),\n",
				"                                'PhD or higher'\n",
				"                            ).otherwise(\n",
				"                                F.when(\n",
				"                                    F.lower(F.col('jobdescription')).rlike('|'.join(masters)),\n",
				"                                    'Masters'\n",
				"                                ).otherwise(\n",
				"                                    F.when(\n",
				"                                        F.lower(F.col('jobdescription')).rlike('|'.join(undergraduate)),\n",
				"                                        'Undergraduate'\n",
				"                                    ).otherwise(\n",
				"                                        'Other / Unknown'\n",
				"                                    )\n",
				"                                )\n",
				"                            )\n",
				"                        )\n",
				"    # Create columns for skills demanded by job\n",
				"    masterlist = {'python', 'sql', 'scala', 'aws', 'gcp', 'azure', 'stream', 'batch', 'java', 'spark','dbt', 'airflow'}\n",
				"\n",
				"    for skill in masterlist:\n",
				"        df = df.withColumn(skill,\n",
				"                      F.when(\n",
				"                          F.lower(F.col('jobdescription')).rlike(skill),\n",
				"                          1\n",
				"                        ).otherwise(\n",
				"                              0\n",
				"                        )\n",
				"                    )\n",
				"    # Job Seniority\n",
				"    experienced = {'lead', 'principal', 'senior', 'sr', 'iv', 'manage'}\n",
				"    mid = {'mid', 'ii', 'iii'}\n",
				"    entry = {'entry', 'associate', 'assc', 'i', 'junior', 'jr'}\n",
				"    df = df.withColumn('experiencelevel',\n",
				"                       F.when(\n",
				"                           F.lower(F.col('jobtitle')).rlike('|'.join(experienced)),\n",
				"                           'Senior'\n",
				"                       ).otherwise(\n",
				"                           F.when(\n",
				"                               F.lower(F.col('jobtitle')).rlike('|'.join(mid)),\n",
				"                               'Mid'\n",
				"                       ).otherwise(\n",
				"                           F.when(\n",
				"                               F.lower(F.col('jobtitle')).rlike('|'.join(entry)),\n",
				"                               'Entry'\n",
				"                            ).otherwise(\n",
				"                                'Other / Unknown'\n",
				"                            )\n",
				"                        )\n",
				"                    )\n",
				"                )\n",
				"    # Select cols\n",
				"    df_transformed = df.select(\n",
				"        'date',\n",
				"        'companyname',\n",
				"        'companyrating',\n",
				"        'companyrevenue',\n",
				"        'companysector',\n",
				"        'companyindustry',\n",
				"        'companysize',\n",
				"        'companytype',\n",
				"        'companyyearfounded',\n",
				"        'easyapply',\n",
				"        'jobcity',\n",
				"        'jobstate',\n",
				"        'minsalary',\n",
				"        'maxsalary',\n",
				"        'averagesalary',\n",
				"        'companyage',\n",
				"        'educationlevel',\n",
				"        'stream',\n",
				"        'sql',\n",
				"        'gcp',\n",
				"        'scala',\n",
				"        'dbt',\n",
				"        'java',\n",
				"        'azure',\n",
				"        'aws',\n",
				"        'batch',\n",
				"        'spark',\n",
				"        'python',\n",
				"        'airflow',\n",
				"        'experiencelevel'\n",
				"    )\n",
				"    return df"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def map_star_schema(df):\n",
				"    '''\n",
				"    Converts DataFrame to STAR Schema data model.\n",
				"    \n",
				"    ARGUMENTS:\n",
				"        - df: PySpark DataFrame\n",
				"    RETURNS:\n",
				"        - PySpark DataFrame\n",
				"    '''\n",
				"    # To ensure fact table and dimensional tables join correctly, need to validate categorical null values and check all categorical values are valid values seen in dimensional tables.\n",
				"    # Get list of valid values for categorical cols containing nulls\n",
				"    cities = {val[0] for val in dimCities.select('cityname').distinct().collect()}\n",
				"    states = {val[0] for val in dimStates.select('stateabbrev').distinct().collect()}\n",
				"    sizes = {val[0] for val in dimSize.select('size').distinct().collect()}\n",
				"    sectors = {val[0] for val in dimSector.select('sector').distinct().collect()}\n",
				"    industries = {val[0] for val in dimIndustry.select('industry').distinct().collect()} \n",
				"    revenues = {val[0] for val in dimRevenue.select('revenue').distinct().collect()}\n",
				"    types = {val[0] for val in dimType.select('type').distinct().collect()}\n",
				"\n",
				"    # Apply valid categorical values check, fill nulls with Other / Unknown if necessary \n",
				"    validvalueshashmap = {\n",
				"        'jobcity': cities,\n",
				"        'jobstate': states,\n",
				"        'companytype': types,\n",
				"        'companyrevenue': revenues,\n",
				"        'companysector': sectors,\n",
				"        'companysize': sizes,\n",
				"        'companytype': types,\n",
				"        'companyindustry': industries\n",
				"    }\n",
				"    for col in validvalueshashmap:\n",
				"        df = df.withColumn(col,\n",
				"                        F.when(\n",
				"                            F.col(col).isin(validvalueshashmap[col]),\n",
				"                            F.col(col)\n",
				"                            ).otherwise(\n",
				"                                    'Other / Unknown'\n",
				"                                )\n",
				"                        )\n",
				"\n",
				"    # Fill company name nulls\n",
				"    df = df.fillna('Other / Unknown', subset=['companyname'])\n",
				"    \n",
				"    # Convert model to STAR schema\n",
				"    cols = [\n",
				"            'date',\n",
				"            'companyname', \n",
				"            'companyrating', \n",
				"            'revenuekey', \n",
				"            'sectorkey',\n",
				"            'industrykey', \n",
				"            'sizekey', \n",
				"            'typekey', \n",
				"            'educationkey', \n",
				"            'experiencekey',\n",
				"            'companyyearfounded', \n",
				"            'companyage', \n",
				"            'easyapply', \n",
				"            'citykey', \n",
				"            'statekey', \n",
				"            'minsalary', \n",
				"            'maxsalary', \n",
				"            'averagesalary',\n",
				"            'stream', \n",
				"            'sql', \n",
				"            'gcp', \n",
				"            'scala', \n",
				"            'dbt', \n",
				"            'java', \n",
				"            'azure', \n",
				"            'aws', \n",
				"            'batch', \n",
				"            'spark', \n",
				"            'python', \n",
				"            'airflow',\n",
				"        ]\n",
				"    df = df \\\n",
				"        .join(F.broadcast(dimCities), df.jobcity==dimCities.cityname, 'left') \\\n",
				"        .join(F.broadcast(dimStates), df.jobstate==dimStates.stateabbrev, 'left') \\\n",
				"        .join(F.broadcast(dimSize), df.companysize==dimSize.size, 'left') \\\n",
				"        .join(F.broadcast(dimSector), df.companysector==dimSector.sector, 'left') \\\n",
				"        .join(F.broadcast(dimIndustry), df.companyindustry==dimIndustry.industry, 'left') \\\n",
				"        .join(F.broadcast(dimRevenue), df.companyrevenue==dimRevenue.revenue, 'left') \\\n",
				"        .join(F.broadcast(dimType), df.companytype==dimType.type, 'left') \\\n",
				"        .join(F.broadcast(dimEducation), df.educationlevel==dimEducation.education, 'left') \\\n",
				"        .join(F.broadcast(dimExperience), df.experiencelevel==dimExperience.experience, 'left') \\\n",
				"        .select(cols)\n",
				"\n",
				"    return df"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def spark_to_dynamicframe(df):\n",
				"    '''\n",
				"    Converts Spark DataFrame to Glue DynamicFrame. Applies appropriate data type mappings to fields.\n",
				"    \n",
				"    ARGUMENTS:\n",
				"        - df: PySpark DataFrame\n",
				"    RETURNS:\n",
				"        - Glue DynamicFrame\n",
				"    '''\n",
				"    # Convert spark dataframe to glue dynamicframe\n",
				"    dyf_fact = DynamicFrame.fromDF(\n",
				"        df,\n",
				"        glueContext,\n",
				"        'convert'\n",
				"    )\n",
				"    # Change types, apply mapping\n",
				"    mapping = [\n",
				"        ('date', 'string', 'date', 'string'),\n",
				"        ('companyname', 'string', 'companyname', 'string'),\n",
				"        ('companyrating', 'double', 'companyrating', 'double'),\n",
				"        ('revenuekey', 'long', 'revenuekey', 'int'),\n",
				"        ('sectorkey', 'long', 'sectorkey', 'int'),\n",
				"        ('industrykey', 'long', 'industrykey', 'int'),\n",
				"        ('sizekey', 'long', 'sizekey', 'int'),\n",
				"        ('typekey', 'long', 'typekey', 'int'),\n",
				"        ('educationkey', 'long', 'educationkey', 'int'),\n",
				"        ('experiencekey', 'long', 'experiencekey', 'int'),\n",
				"        ('companyyearfounded', 'string', 'companyyearfounded', 'int'),\n",
				"        ('companyage', 'string', 'companyage', 'int'),\n",
				"        ('easyapply', 'string', 'easyapply', 'string'),\n",
				"        ('citykey', 'long', 'citykey', 'int'),\n",
				"        ('statekey', 'long', 'statekey', 'int'),\n",
				"        ('minsalary', 'string', 'minsalary', 'double'),\n",
				"        ('maxsalary', 'string', 'maxsalary', 'double'),\n",
				"        ('averagesalary', 'double', 'averagesalary', 'double'),\n",
				"        ('stream', 'int', 'stream', 'int'),\n",
				"        ('sql', 'int', 'sql', 'int'),\n",
				"        ('gcp', 'int', 'gcp', 'int'),\n",
				"        ('scala', 'int', 'scala', 'int'),\n",
				"        ('dbt', 'int', 'dbt', 'int'),\n",
				"        ('java', 'int', 'java', 'int'),\n",
				"        ('azure', 'int', 'azure', 'int'),\n",
				"        ('aws', 'int', 'aws', 'int'),\n",
				"        ('batch', 'int', 'batch', 'int'),\n",
				"        ('spark', 'int', 'spark', 'int'),\n",
				"        ('python', 'int', 'python', 'int'),\n",
				"        ('airflow', 'int', 'airflow', 'int')\n",
				"    ]\n",
				"    dyf_fact = ApplyMapping.apply(\n",
				"        frame=dyf_fact,\n",
				"        mappings=mapping,\n",
				"        transformation_ctx='dyfapplymapping'\n",
				"    )\n",
				"    \n",
				"    return dyf_fact"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def save_to_lake_catalog(dyf_fact, date):\n",
				"    '''\n",
				"    Saves Glue DynamicFrame to Lake Formation Curated bucket and reads data into Glue Data Catalog.\n",
				"    \n",
				"    ARGUMENTS:\n",
				"        - dyf_fact: Glue DynamicFrame\n",
				"        - date: String representation of desired date for naming purposes. Formatted as YYYYMMDD\n",
				"    RETURNS:\n",
				"        None\n",
				"    '''\n",
				"    # Add curated data to S3 curated bucket and Glue Data Catalog\n",
				"\n",
				"    s3output = glueContext.getSink(\n",
				"      path=f's3://kc-glassdoor-data-curated/curated/curated_{date}',\n",
				"      connection_type=\"s3\",\n",
				"      updateBehavior=\"UPDATE_IN_DATABASE\",\n",
				"      partitionKeys=[],\n",
				"        format='csv',\n",
				"      enableUpdateCatalog=True,\n",
				"      transformation_ctx=\"s3output\",\n",
				"    )\n",
				"\n",
				"    s3output.setCatalogInfo(\n",
				"      catalogDatabase=\"glassdoorcurated\", catalogTableName=f\"curated_{date}\"\n",
				"    )\n",
				"    s3output.setFormat(\"csv\")\n",
				"    s3output.writeFrame(dyf_fact.coalesce(1))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"date_format='%m/%d/%Y'\n",
				"date = datetime.datetime.now(tz=pytz.utc)\n",
				"date = date.astimezone(pytz.timezone('US/Pacific'))\n",
				"today = date.strftime('%Y%m%d')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# Read daily data into dynamic frame\n",
				"\n",
				"df = glueContext.create_dynamic_frame.from_catalog(\n",
				"    database='glassdoorcleaned',\n",
				"    table_name=f'clean_{today}'\n",
				").toDF()\n",
				"\n",
				"# df = glueContext.create_dynamic_frame.from_catalog(\n",
				"#     database='glassdoorcleaned',\n",
				"#     table_name=f'clean_20230719'\n",
				"# ).toDF()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"df = map_star_schema(transform_data(df))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"dyf = spark_to_dynamicframe(df)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"save_to_lake_catalog(dyf, today)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
